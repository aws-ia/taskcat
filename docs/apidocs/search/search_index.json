{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is TaskCat? TaskCat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. TaskCat is implemented as a Python class that you import, instantiate, and run. TaskCat was developed by the aws-ia team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions See TaskCat documentation . Support GitHub PyPi","title":"Home"},{"location":"#what-is-taskcat","text":"TaskCat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. TaskCat is implemented as a Python class that you import, instantiate, and run. TaskCat was developed by the aws-ia team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions See TaskCat documentation .","title":"What is TaskCat?"},{"location":"#support","text":"","title":"Support"},{"location":"#github","text":"","title":"GitHub"},{"location":"#pypi","text":"","title":"PyPi"},{"location":"docs/","text":"What is taskcat? taskcat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. taskcat is implemented as a Python class that you import, instantiate, and run. taskcat was developed by the aws-ia team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions Support GitHub PyPi","title":"Index"},{"location":"docs/#what-is-taskcat","text":"taskcat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. taskcat is implemented as a Python class that you import, instantiate, and run. taskcat was developed by the aws-ia team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions","title":"What is taskcat?"},{"location":"docs/#support","text":"","title":"Support"},{"location":"docs/#github","text":"","title":"GitHub"},{"location":"docs/#pypi","text":"","title":"PyPi"},{"location":"docs/INSTALLATION/","text":"Installation Currently only installation via pip is supported. Requirements The host taskcat is run on requires access to an AWS account, this can be done by any of the following mechanisms: Environment variables Shared credential file (~/.aws/credentials) AWS config file (~/.aws/config) Assume Role provider Boto2 config file (/etc/boto.cfg and ~/.boto) Instance metadata service on an Amazon EC2 instance that has an IAM role configured. for more info see the boto3 credential configuration documentation . Note docker is only required if building lambda functions using a Dockerfile Installing via pip3 pip3 install taskcat Installing via pip3 --user will install taskcat into homedir, useful if you get permissions errors with the regular method pip3 install taskcat -- user Note The user install dir is platform specific On Mac: ~/Library/Python/3.x/bin/taskcat On Linux: ~/.local/bin Warning Be sure to add the python bin dir to your $PATH Windows Taskcat on Windows is not supported . If you are running Windows 10 we recommend that you install Windows Subsystem for Linux (WSL) and then install taskcat inside the WSL environment. For details, see Install and configure TaskCat on Microsoft Windows 10 .","title":"Installation"},{"location":"docs/INSTALLATION/#installation","text":"Currently only installation via pip is supported.","title":"Installation"},{"location":"docs/INSTALLATION/#requirements","text":"The host taskcat is run on requires access to an AWS account, this can be done by any of the following mechanisms: Environment variables Shared credential file (~/.aws/credentials) AWS config file (~/.aws/config) Assume Role provider Boto2 config file (/etc/boto.cfg and ~/.boto) Instance metadata service on an Amazon EC2 instance that has an IAM role configured. for more info see the boto3 credential configuration documentation . Note docker is only required if building lambda functions using a Dockerfile","title":"Requirements"},{"location":"docs/INSTALLATION/#installing-via-pip3","text":"pip3 install taskcat","title":"Installing via pip3"},{"location":"docs/INSTALLATION/#installing-via-pip3-user","text":"will install taskcat into homedir, useful if you get permissions errors with the regular method pip3 install taskcat -- user Note The user install dir is platform specific On Mac: ~/Library/Python/3.x/bin/taskcat On Linux: ~/.local/bin Warning Be sure to add the python bin dir to your $PATH","title":"Installing via pip3 --user"},{"location":"docs/INSTALLATION/#windows","text":"Taskcat on Windows is not supported . If you are running Windows 10 we recommend that you install Windows Subsystem for Linux (WSL) and then install taskcat inside the WSL environment. For details, see Install and configure TaskCat on Microsoft Windows 10 .","title":"Windows"},{"location":"docs/readme.amiupdater/","text":"AMIUpdater README General Usage. amiupdater <flags> </path/to/template_directory|template_file> For a current list of options, see.. amiupdater -h Leveraging the Upstream Config File Upstream Mappings By default, AMIUpdater uses a config file bundled with taskcat . This config file is populated with common AMI Mappings, such as Amazon Linux AMI and Ubuntu Server 18.04 . To see all of the mappings available, check out the config file To utilize these upstream mappings, simply leverage them in your templates. Note: The AMI IDs are here for example purposes. When first configuring the Mapping, you can filll them with arbitrary data. JSON { (...) \"Mappings\" : { \"AWSAMIRegionMap\" : { \"ap-northeast-1\" : { \"AMZNLINUXHVM\" : \"ami-00a5245b4816c38e6\" , \"CENTOS7HVM\" : \"ami-8e8847f1\" , \"US1404HVM\" : \"ami-0be9269b44d4b26c1\" , \"US1604HVM\" : \"ami-0d5e82481c5fd4ad5\" , \"SLES15HVM\" : \"ami-09161bc9964f46a98\" }, \"ap-northeast-2\" : { \"AMZNLINUXHVM\" : \"ami-00dc207f8ba6dc919\" , \"CENTOS7HVM\" : \"ami-bf9c36d1\" , \"US1404HVM\" : \"ami-017332df4b882edd2\" , \"US1604HVM\" : \"ami-0507b772e2c9b8c15\" , \"SLES15HVM\" : \"ami-04ecb44b7d8e8d354\" }, \"ap-south-1\" : { \"AMZNLINUXHVM\" : \"ami-0ad42f4f66f6c1cc9\" , \"CENTOS7HVM\" : \"ami-1780a878\" , \"US1404HVM\" : \"ami-09dcf5653a185f5df\" , \"US1604HVM\" : \"ami-0c8810f694cbe10ba\" , \"SLES15HVM\" : \"ami-025d8258d76079367\" } (...) } } } } YAML Mappings : AWSAMIRegionMap : ap-northeast-1 : AMZNLINUXHVM : ami-00a5245b4816c38e6, CENTOS7HVM : ami-8e8847f1, US1404HVM : ami-0be9269b44d4b26c1, US1604HVM : ami-0d5e82481c5fd4ad5, SLES15HVM : ami-09161bc9964f46a98 ap-northeast-2 : AMZNLINUXHVM : ami-00dc207f8ba6dc919, CENTOS7HVM : ami-bf9c36d1, US1404HVM : ami-017332df4b882edd2, US1604HVM : ami-0507b772e2c9b8c15, SLES15HVM : ami-04ecb44b7d8e8d354 ap-south-1 : AMZNLINUXHVM : ami-0ad42f4f66f6c1cc9, CENTOS7HVM : ami-1780a878, US1404HVM : ami-09dcf5653a185f5df, US1604HVM : ami-0c8810f694cbe10ba, SLES15HVM : ami-025d8258d76079367 Defining your own AMI Mappings Custom Config File Functionally the same as the upstream config file, a local config file can be created and used in deployment pipelines. For a full list of filters, available, please see the AWS EC2 API Documentation . # Owner-id must be in quotes # Whereas, all other filters do not need quotes, # because they are not in a number format global : AMIs : CUSTOM_MAPPING_1 : name : my_super_awesome_name-* owner-id : 1234567890 CUSTOM_MAPPING_2 : name : my_super_other_awesome_name ???? * owner-id : 1234567890 architecture : arm64 Template Inline Config JSON \"Metadata\" : { \"AWSAMIRegionMap\" :{ \"Filters\" :{ \"<MAPPING_NAME>\" :{ \"name\" : \"my awesome AMI NAME\" , \"owner-id\" : \"01234567890\" } } } YAML Metadata : AWSAMIRegionMap : Filters : <MAPPING_NAME> : name : my awesome AMI NAME owner-id : 01234567890","title":"*AMIUpdater README*"},{"location":"docs/readme.amiupdater/#amiupdater-readme","text":"","title":"AMIUpdater README"},{"location":"docs/readme.amiupdater/#general-usage","text":"amiupdater <flags> </path/to/template_directory|template_file> For a current list of options, see.. amiupdater -h","title":"General Usage."},{"location":"docs/readme.amiupdater/#leveraging-the-upstream-config-file","text":"","title":"Leveraging the Upstream Config File"},{"location":"docs/readme.amiupdater/#upstream-mappings","text":"By default, AMIUpdater uses a config file bundled with taskcat . This config file is populated with common AMI Mappings, such as Amazon Linux AMI and Ubuntu Server 18.04 . To see all of the mappings available, check out the config file To utilize these upstream mappings, simply leverage them in your templates. Note: The AMI IDs are here for example purposes. When first configuring the Mapping, you can filll them with arbitrary data. JSON { (...) \"Mappings\" : { \"AWSAMIRegionMap\" : { \"ap-northeast-1\" : { \"AMZNLINUXHVM\" : \"ami-00a5245b4816c38e6\" , \"CENTOS7HVM\" : \"ami-8e8847f1\" , \"US1404HVM\" : \"ami-0be9269b44d4b26c1\" , \"US1604HVM\" : \"ami-0d5e82481c5fd4ad5\" , \"SLES15HVM\" : \"ami-09161bc9964f46a98\" }, \"ap-northeast-2\" : { \"AMZNLINUXHVM\" : \"ami-00dc207f8ba6dc919\" , \"CENTOS7HVM\" : \"ami-bf9c36d1\" , \"US1404HVM\" : \"ami-017332df4b882edd2\" , \"US1604HVM\" : \"ami-0507b772e2c9b8c15\" , \"SLES15HVM\" : \"ami-04ecb44b7d8e8d354\" }, \"ap-south-1\" : { \"AMZNLINUXHVM\" : \"ami-0ad42f4f66f6c1cc9\" , \"CENTOS7HVM\" : \"ami-1780a878\" , \"US1404HVM\" : \"ami-09dcf5653a185f5df\" , \"US1604HVM\" : \"ami-0c8810f694cbe10ba\" , \"SLES15HVM\" : \"ami-025d8258d76079367\" } (...) } } } } YAML Mappings : AWSAMIRegionMap : ap-northeast-1 : AMZNLINUXHVM : ami-00a5245b4816c38e6, CENTOS7HVM : ami-8e8847f1, US1404HVM : ami-0be9269b44d4b26c1, US1604HVM : ami-0d5e82481c5fd4ad5, SLES15HVM : ami-09161bc9964f46a98 ap-northeast-2 : AMZNLINUXHVM : ami-00dc207f8ba6dc919, CENTOS7HVM : ami-bf9c36d1, US1404HVM : ami-017332df4b882edd2, US1604HVM : ami-0507b772e2c9b8c15, SLES15HVM : ami-04ecb44b7d8e8d354 ap-south-1 : AMZNLINUXHVM : ami-0ad42f4f66f6c1cc9, CENTOS7HVM : ami-1780a878, US1404HVM : ami-09dcf5653a185f5df, US1604HVM : ami-0c8810f694cbe10ba, SLES15HVM : ami-025d8258d76079367","title":"Upstream Mappings"},{"location":"docs/readme.amiupdater/#defining-your-own-ami-mappings","text":"","title":"Defining your own AMI Mappings"},{"location":"docs/readme.amiupdater/#custom-config-file","text":"Functionally the same as the upstream config file, a local config file can be created and used in deployment pipelines. For a full list of filters, available, please see the AWS EC2 API Documentation . # Owner-id must be in quotes # Whereas, all other filters do not need quotes, # because they are not in a number format global : AMIs : CUSTOM_MAPPING_1 : name : my_super_awesome_name-* owner-id : 1234567890 CUSTOM_MAPPING_2 : name : my_super_other_awesome_name ???? * owner-id : 1234567890 architecture : arm64","title":"Custom Config File"},{"location":"docs/readme.amiupdater/#template-inline-config","text":"JSON \"Metadata\" : { \"AWSAMIRegionMap\" :{ \"Filters\" :{ \"<MAPPING_NAME>\" :{ \"name\" : \"my awesome AMI NAME\" , \"owner-id\" : \"01234567890\" } } } YAML Metadata : AWSAMIRegionMap : Filters : <MAPPING_NAME> : name : my awesome AMI NAME owner-id : 01234567890","title":"Template Inline Config"},{"location":"docs/administrative/CODE_OF_CONDUCT/","text":"Code of Conduct 1. Purpose A primary goal of TaskCat is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof). This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in TaskCat to help us create safe and positive experiences for everyone. 2. Open Source Citizenship A supplemental goal of this Code of Conduct is to increase open source citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community. Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know. 3. Expected Behavior The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential. Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations. 4. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people\u2019s personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Inappropriate physical contact. You should have someone\u2019s consent before touching them. Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. Sustained disruption of community events, including talks and presentations. 5. Consequences of Unacceptable Behavior Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event). 6. Reporting Guidelines If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. quickstart@amazon.com. Link to reporting guidelines: codeofconduct@amazon.com Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress. 7. Addressing Grievances If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify AWS Quickstart with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Policy 8. Scope We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business. This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members. 9. Contact info quickstart@amazon.com 10. License and attribution This Code of Conduct is distributed under a Creative Commons Attribution-ShareAlike license . Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy . Retrieved on November 22, 2016 from http://citizencodeofconduct.org/","title":"Code of Conduct"},{"location":"docs/administrative/CODE_OF_CONDUCT/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"docs/administrative/CODE_OF_CONDUCT/#1-purpose","text":"A primary goal of TaskCat is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof). This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in TaskCat to help us create safe and positive experiences for everyone.","title":"1. Purpose"},{"location":"docs/administrative/CODE_OF_CONDUCT/#2-open-source-citizenship","text":"A supplemental goal of this Code of Conduct is to increase open source citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community. Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know.","title":"2. Open Source Citizenship"},{"location":"docs/administrative/CODE_OF_CONDUCT/#3-expected-behavior","text":"The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential. Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations.","title":"3. Expected Behavior"},{"location":"docs/administrative/CODE_OF_CONDUCT/#4-unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people\u2019s personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Inappropriate physical contact. You should have someone\u2019s consent before touching them. Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. Sustained disruption of community events, including talks and presentations.","title":"4. Unacceptable Behavior"},{"location":"docs/administrative/CODE_OF_CONDUCT/#5-consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event).","title":"5. Consequences of Unacceptable Behavior"},{"location":"docs/administrative/CODE_OF_CONDUCT/#6-reporting-guidelines","text":"If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. quickstart@amazon.com. Link to reporting guidelines: codeofconduct@amazon.com Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress.","title":"6. Reporting Guidelines"},{"location":"docs/administrative/CODE_OF_CONDUCT/#7-addressing-grievances","text":"If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify AWS Quickstart with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Policy","title":"7. Addressing Grievances"},{"location":"docs/administrative/CODE_OF_CONDUCT/#8-scope","text":"We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business. This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members.","title":"8. Scope"},{"location":"docs/administrative/CODE_OF_CONDUCT/#9-contact-info","text":"quickstart@amazon.com","title":"9. Contact info"},{"location":"docs/administrative/CODE_OF_CONDUCT/#10-license-and-attribution","text":"This Code of Conduct is distributed under a Creative Commons Attribution-ShareAlike license . Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy . Retrieved on November 22, 2016 from http://citizencodeofconduct.org/","title":"10. License and attribution"},{"location":"docs/administrative/CONTRIBUTING/","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Documentation Links: Module Documentation User Guide Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests (Pull request template provided) Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: All changes are staged into the develop branch (Send PR to the develop branch) You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Licensing We may ask you to affirm the Apache 2.0 agreement for larger changes.","title":"Contributing"},{"location":"docs/administrative/CONTRIBUTING/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Documentation Links: Module Documentation User Guide","title":"Contributing Guidelines"},{"location":"docs/administrative/CONTRIBUTING/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"docs/administrative/CONTRIBUTING/#contributing-via-pull-requests-pull-request-template-provided","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: All changes are staged into the develop branch (Send PR to the develop branch) You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests (Pull request template provided)"},{"location":"docs/administrative/CONTRIBUTING/#licensing","text":"We may ask you to affirm the Apache 2.0 agreement for larger changes.","title":"Licensing"},{"location":"docs/schema/taskcat_schema/","text":"Schema Docs Expand all Collapse all Type: object Taskcat configuration file No Additional Properties general root general Type: object Default: {\"artifact_regions\": null, \"auth\": null, \"parameters\": null, \"posthooks\": null, \"prehooks\": null, \"regions\": null, \"s3_bucket\": null, \"s3_regional_buckets\": null, \"tags\": null} General configuration settings. No Additional Properties artifact_regions root general artifact_regions Type: array of string List of AWS regions where artifacts need to be copied. This helps same region artifact bucket access to resources Each item of this array must be: root general artifact_regions artifact_regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" auth root general auth Type: object AWS authentication section Example: { \"cn-northwest-1\" : \"china-profile\" , \"default\" : \"my-default-profile\" , \"us-east-2\" : \"specific-profile-for-us-east-2\" } Additional Properties Each additional property must conform to the following schema root general auth additionalProperties Type: string parameters root general parameters Type: object Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence Additional Properties Each additional property must conform to the following schema root general parameters additionalProperties One of Option 1 Option 2 Option 3 Option 4 root general parameters additionalProperties oneOf item 0 Type: string root general parameters additionalProperties oneOf item 1 Type: integer root general parameters additionalProperties oneOf item 2 Type: boolean root general parameters additionalProperties oneOf item 3 Type: array Each item of this array must be: root general parameters additionalProperties oneOf item 3 item 3 items One of Option 1 Option 2 root general parameters additionalProperties oneOf item 3 item 3 items oneOf item 0 Type: integer root general parameters additionalProperties oneOf item 3 item 3 items oneOf item 1 Type: string posthooks root general posthooks Type: array hooks to execute after executing tests Each item of this array must be: root general posthooks HookData Type: object Hook definition No Additional Properties config root general posthooks posthooks items config Type: object hook configuration type root general posthooks posthooks items type Type: string hook type prehooks root general prehooks Type: array hooks to execute prior to executing tests Each item of this array must be: root general prehooks HookData Type: object Hook definition Same definition as general_posthooks_items regions root general regions Type: array of string List of AWS regions Each item of this array must be: root general regions regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" s3_bucket root general s3_bucket Type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated Example: \"my-s3-bucket-name\" s3_regional_buckets root general s3_regional_buckets Type: boolean Enable regional auto-buckets. Examples: true false tags root general tags Type: object Tags to apply to CloudFormation template Example: { \"CostCenter\" : \"1001\" } Additional Properties Each additional property must conform to the following schema root general tags additionalProperties Type: string project root project Type: object Default: {\"artifact_regions\": null, \"auth\": null, \"az_blacklist\": null, \"build_submodules\": null, \"lambda_source_path\": null, \"lambda_zip_path\": null, \"name\": null, \"org_id\": null, \"owner\": null, \"package_lambda\": null, \"parameters\": null, \"posthooks\": null, \"prehooks\": null, \"regions\": null, \"role_name\": null, \"s3_bucket\": null, \"s3_enable_sig_v2\": null, \"s3_object_acl\": null, \"s3_regional_buckets\": null, \"shorten_stack_name\": null, \"tags\": null, \"template\": null} Project specific configuration section No Additional Properties artifact_regions root project artifact_regions Type: array of string List of AWS regions where artifacts need to be copied. This helps same region artifact bucket access to resources Each item of this array must be: root project artifact_regions artifact_regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" auth root project auth Type: object AWS authentication section Example: { \"cn-northwest-1\" : \"china-profile\" , \"default\" : \"my-default-profile\" , \"us-east-2\" : \"specific-profile-for-us-east-2\" } Additional Properties Each additional property must conform to the following schema root project auth additionalProperties Type: string az_blacklist root project az_blacklist Type: array of string List of Availablilty Zones ID's to exclude when generating availability zones Each item of this array must be: root project az_blacklist az_blacklist items Type: string Availability Zone ID, eg.: 'use1-az1' Must match regular expression: (afs1-az1|afs1-az2|afs1-az3|ape1-az1|ape1-az2|ape1-az3|apne1-az1|apne1-az2|apne1-az4|apne2-az1|apne2-az2|apne2-az3|apne2-az4|apne3-az1|apne3-az2|apne3-az3|aps1-az1|aps1-az2|aps1-az3|apse1-az1|apse1-az2|apse1-az3|apse2-az1|apse2-az2|apse2-az3|apse3-az1|apse3-az2|apse3-az3|cac1-az1|cac1-az2|cac1-az4|euc1-az1|euc1-az2|euc1-az3|eun1-az1|eun1-az2|eun1-az3|eus1-az1|eus1-az2|eus1-az3|euw1-az1|euw1-az2|euw1-az3|euw2-az1|euw2-az2|euw2-az3|euw3-az1|euw3-az2|euw3-az3|mes1-az1|mes1-az2|mes1-az3|sae1-az1|sae1-az2|sae1-az3|use1-atl1-az1|use1-az1|use1-az2|use1-az3|use1-az4|use1-az5|use1-az6|use1-bos1-az1|use1-chi1-az1|use1-dfw1-az1|use1-iah1-az1|use1-mci1-az1|use1-mia1-az1|use1-msp1-az1|use1-nyc1-az1|use1-phl1-az1|use2-az1|use2-az2|use2-az3|usw1-az1|usw1-az3|usw2-az1|usw2-az2|usw2-az3|usw2-az4|usw2-den1-az1|usw2-las1-az1|usw2-lax1-az1|usw2-lax1-az2|usw2-pdx1-az1|usw2-phx1-az1|usw2-sea1-az1) Examples: \"usw2-laz1-az1\" \"use2-az2\" build_submodules root project build_submodules Type: boolean Build Lambda zips recursively for submodules, set to false to disable Examples: true false lambda_source_path root project lambda_source_path Type: string Path relative to the project root containing Lambda zip files, default is 'lambda_functions/source' Example: \"functions/source\" lambda_zip_path root project lambda_zip_path Type: string Path relative to the project root to place Lambda zip files Example: \"functions/packages\" name root project name Type: string Project name, used as s3 key prefix when uploading objects Must match regular expression: ^[a-z0-9-]*$ Example: \"my-project-name\" org_id root project org_id Type: string Organization ID to use when launching CFN Stacks. starts with o-. It is found on Organization Settings page owner root project owner Type: string email address for project owner (not used at present) Example: \"Bob.Slydell@example.com\" package_lambda root project package_lambda Type: boolean Package Lambda functions into zips before uploading to s3, set to false to disable Examples: true false parameters root project parameters Type: object Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence Additional Properties Each additional property must conform to the following schema root project parameters additionalProperties One of Option 1 Option 2 Option 3 Option 4 root project parameters additionalProperties oneOf item 0 Type: string root project parameters additionalProperties oneOf item 1 Type: integer root project parameters additionalProperties oneOf item 2 Type: boolean root project parameters additionalProperties oneOf item 3 Type: array Each item of this array must be: root project parameters additionalProperties oneOf item 3 item 3 items One of Option 1 Option 2 root project parameters additionalProperties oneOf item 3 item 3 items oneOf item 0 Type: integer root project parameters additionalProperties oneOf item 3 item 3 items oneOf item 1 Type: string posthooks root project posthooks Type: array hooks to execute after executing tests Each item of this array must be: root project posthooks HookData Type: object Hook definition Same definition as general_posthooks_items prehooks root project prehooks Type: array hooks to execute prior to executing tests Each item of this array must be: root project prehooks HookData Type: object Hook definition Same definition as general_posthooks_items regions root project regions Type: array of string List of AWS regions Each item of this array must be: root project regions regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" role_name root project role_name Type: string Role name to use when launching CFN Stacks. s3_bucket root project s3_bucket Type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated Must match regular expression: ^[a-z0-9-]*$ Example: \"my-s3-bucket-name\" s3_enable_sig_v2 root project s3_enable_sig_v2 Type: boolean Enable (deprecated) sigv2 access to auto-generated buckets Examples: true false s3_object_acl root project s3_object_acl Type: string Default: \"private\" ACL for uploaded s3 objects Must match regular expression: ^(bucket-owner-full-control|bucket-owner-read|authenticated-read|aws-exec-read|public-read-write|public-read|private)$ Examples: \"bucket-owner-read\" \"private\" s3_regional_buckets root project s3_regional_buckets Type: boolean Enable regional auto-buckets. Examples: true false shorten_stack_name root project shorten_stack_name Type: boolean Shorten stack names generated for tests, set to true to enable Examples: true false tags root project tags Type: object Tags to apply to CloudFormation template Example: { \"CostCenter\" : \"1001\" } Additional Properties Each additional property must conform to the following schema root project tags additionalProperties Type: string template root project template Type: string path to template file relative to the project config file path Example: \"cloudformation_templates/\" tests root tests Type: object Default: {} Additional Properties Each additional property must conform to the following schema root tests TestConfig Type: object Test specific configuration section. No Additional Properties artifact_regions root tests additionalProperties artifact_regions Type: array of string List of AWS regions where artifacts need to be copied. This helps same region artifact bucket access to resources Each item of this array must be: root tests additionalProperties artifact_regions artifact_regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" auth root tests additionalProperties auth Type: object AWS authentication section Example: { \"cn-northwest-1\" : \"china-profile\" , \"default\" : \"my-default-profile\" , \"us-east-2\" : \"specific-profile-for-us-east-2\" } Additional Properties Each additional property must conform to the following schema root tests additionalProperties auth additionalProperties Type: string az_blacklist root tests additionalProperties az_blacklist Type: array of string List of Availablilty Zones ID's to exclude when generating availability zones Each item of this array must be: root tests additionalProperties az_blacklist az_blacklist items Type: string Availability Zone ID, eg.: 'use1-az1' Must match regular expression: (afs1-az1|afs1-az2|afs1-az3|ape1-az1|ape1-az2|ape1-az3|apne1-az1|apne1-az2|apne1-az4|apne2-az1|apne2-az2|apne2-az3|apne2-az4|apne3-az1|apne3-az2|apne3-az3|aps1-az1|aps1-az2|aps1-az3|apse1-az1|apse1-az2|apse1-az3|apse2-az1|apse2-az2|apse2-az3|apse3-az1|apse3-az2|apse3-az3|cac1-az1|cac1-az2|cac1-az4|euc1-az1|euc1-az2|euc1-az3|eun1-az1|eun1-az2|eun1-az3|eus1-az1|eus1-az2|eus1-az3|euw1-az1|euw1-az2|euw1-az3|euw2-az1|euw2-az2|euw2-az3|euw3-az1|euw3-az2|euw3-az3|mes1-az1|mes1-az2|mes1-az3|sae1-az1|sae1-az2|sae1-az3|use1-atl1-az1|use1-az1|use1-az2|use1-az3|use1-az4|use1-az5|use1-az6|use1-bos1-az1|use1-chi1-az1|use1-dfw1-az1|use1-iah1-az1|use1-mci1-az1|use1-mia1-az1|use1-msp1-az1|use1-nyc1-az1|use1-phl1-az1|use2-az1|use2-az2|use2-az3|usw1-az1|usw1-az3|usw2-az1|usw2-az2|usw2-az3|usw2-az4|usw2-den1-az1|usw2-las1-az1|usw2-lax1-az1|usw2-lax1-az2|usw2-pdx1-az1|usw2-phx1-az1|usw2-sea1-az1) Examples: \"usw2-laz1-az1\" \"use2-az2\" parameters root tests additionalProperties parameters Type: object Default: {} Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence Additional Properties Each additional property must conform to the following schema root tests additionalProperties parameters additionalProperties One of Option 1 Option 2 Option 3 Option 4 root tests additionalProperties parameters additionalProperties oneOf item 0 Type: string root tests additionalProperties parameters additionalProperties oneOf item 1 Type: integer root tests additionalProperties parameters additionalProperties oneOf item 2 Type: boolean root tests additionalProperties parameters additionalProperties oneOf item 3 Type: array Each item of this array must be: root tests additionalProperties parameters additionalProperties oneOf item 3 item 3 items One of Option 1 Option 2 root tests additionalProperties parameters additionalProperties oneOf item 3 item 3 items oneOf item 0 Type: integer root tests additionalProperties parameters additionalProperties oneOf item 3 item 3 items oneOf item 1 Type: string posthooks root tests additionalProperties posthooks Type: array hooks to execute after executing tests Each item of this array must be: root tests additionalProperties posthooks HookData Type: object Hook definition Same definition as general_posthooks_items prehooks root tests additionalProperties prehooks Type: array hooks to execute prior to executing tests Each item of this array must be: root tests additionalProperties prehooks HookData Type: object Hook definition Same definition as general_posthooks_items regions root tests additionalProperties regions Type: array of string List of AWS regions Each item of this array must be: root tests additionalProperties regions regions items Type: string AWS Region name Must match regular expression: ^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|west|southeast|southwest|northeast|northwest)-[0-9]$ Example: \"us-east-1\" role_name root tests additionalProperties role_name Type: string Role name to use when launching CFN Stacks. s3_bucket root tests additionalProperties s3_bucket Type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated Must match regular expression: ^[a-z0-9-]*$ Example: \"my-s3-bucket-name\" s3_regional_buckets root tests additionalProperties s3_regional_buckets Type: boolean Enable regional auto-buckets. Examples: true false stack_name root tests additionalProperties stack_name Type: string Cloudformation Stack Name stack_name_prefix root tests additionalProperties stack_name_prefix Type: string Prefix to apply to generated CFN Stack Name stack_name_suffix root tests additionalProperties stack_name_suffix Type: string Suffix to apply to generated CFN Stack Name tags root tests additionalProperties tags Type: object Tags to apply to CloudFormation template Example: { \"CostCenter\" : \"1001\" } Additional Properties Each additional property must conform to the following schema root tests additionalProperties tags additionalProperties Type: string template root tests additionalProperties template Type: string path to template file relative to the project config file path Example: \"cloudformation_templates/\" Generated using json-schema-for-humans on 2022-03-01 at 20:01:33 -0600","title":"Configuration Schema"},{"location":"docs/usage/GENERAL_USAGE/","text":"Usage CLI The cli is self documenting by using --help . The most common use of taskcat is for executing function tests of CloudFormation templates. The command for this is: taskcat test run add --help to see the supported flags and arguments Python Taskcat can be imported into Python and used in the testing framework of your choice. from taskcat.testing import CFNTest test = CFNTest . from_file ( project_root = './template_dir' ) with test as stacks : # Calling 'with' or 'test.run()' will deploy the stacks. for stack in stacks : print ( f \"Testing { stack . name } \" ) bucket_name = \"\" for output in stack . outputs : if output . key == \"LogsBucketName\" : bucket_name = output . value break assert \"logs\" in bucket_name assert stack . region . name in bucket_name print ( f \"Created bucket: { bucket_name } \" ) The example used here is very simple, you would most likely leverage other python modules like boto3 to do more advanced testing. The CFNTest object can be passed the same arguments as taskcat test run . See the docs for more details. Config files taskcat has several configuration files which can be used to set behaviors in a flexible way. Global config ~/.taskcat.yml provides global settings that become defaults for all projects. Please see our schema reference for specific configuration options that are available. Project config <PROJECT_ROOT>/.taskcat.yml provides project specific configuration. Please see our schema reference for specific configuration options that are available. Precedence With the exception of the parameters section, more specific config with the same key takes precedence. The rationale behind having parameters function this way is so that values can be overridden at a system level outside of a project, that is likely committed to source control. parameters that define account specific things like VPC details, Key Pairs, or secrets like API keys can be defined per host outside of source control. For example, consider this global config: ~/.taskcat.yml general : s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair Given a simple project config: project : name : my-project regions : - us-east-2 tests : default : template : ./template.yaml The effective test configuration would become: tests : default : template : ./template.yaml s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair If any item is re-defined in a project it takes precedence over the global value. Anything defined in a test takes precedence over what is defined in the project or global configuration. with the exception of the parameters section which works in reverse. For example, using the same global config as above, given this project config: project : name : my-project regions : - us-east-2 s3_bucket : my-project-s3-bucket tests : default : template : ./template.yaml parameters : KeyPair : my-test-ec2-keypair Would result in this effective test configuration: tests : default : template : ./template.yaml s3_bucket : my-project-s3-bucket parameters : KeyPair : my-global-ec2-keypair Notice that s3_bucket took the most specific value and KeyPair the most general. CLI interface taskcat adopts a similar cli command structure to git with a taskcat command subcommand --flag style. The cli is also designed to be simplest if run from the root of a project. Let's have a look at equivalent command to run a test: cd into the project root and type test run : cd ./quickstart-aws-vpc taskcat test run or run it from anywhere by providing the path to the project root taskcat test run -p ./quickstart-aws-vpc Configuration files The configuration files required for taskcat have changed, to ease migration, if taskcat is run and legacy config files are found, they are converted and written to new file locations. For more information on the new format, see the config file docs .","title":"General Usage"},{"location":"docs/usage/GENERAL_USAGE/#usage","text":"","title":"Usage"},{"location":"docs/usage/GENERAL_USAGE/#cli","text":"The cli is self documenting by using --help . The most common use of taskcat is for executing function tests of CloudFormation templates. The command for this is: taskcat test run add --help to see the supported flags and arguments","title":"CLI"},{"location":"docs/usage/GENERAL_USAGE/#python","text":"Taskcat can be imported into Python and used in the testing framework of your choice. from taskcat.testing import CFNTest test = CFNTest . from_file ( project_root = './template_dir' ) with test as stacks : # Calling 'with' or 'test.run()' will deploy the stacks. for stack in stacks : print ( f \"Testing { stack . name } \" ) bucket_name = \"\" for output in stack . outputs : if output . key == \"LogsBucketName\" : bucket_name = output . value break assert \"logs\" in bucket_name assert stack . region . name in bucket_name print ( f \"Created bucket: { bucket_name } \" ) The example used here is very simple, you would most likely leverage other python modules like boto3 to do more advanced testing. The CFNTest object can be passed the same arguments as taskcat test run . See the docs for more details.","title":"Python"},{"location":"docs/usage/GENERAL_USAGE/#config-files","text":"taskcat has several configuration files which can be used to set behaviors in a flexible way.","title":"Config files"},{"location":"docs/usage/GENERAL_USAGE/#global-config","text":"~/.taskcat.yml provides global settings that become defaults for all projects. Please see our schema reference for specific configuration options that are available.","title":"Global config"},{"location":"docs/usage/GENERAL_USAGE/#project-config","text":"<PROJECT_ROOT>/.taskcat.yml provides project specific configuration. Please see our schema reference for specific configuration options that are available.","title":"Project config"},{"location":"docs/usage/GENERAL_USAGE/#precedence","text":"With the exception of the parameters section, more specific config with the same key takes precedence. The rationale behind having parameters function this way is so that values can be overridden at a system level outside of a project, that is likely committed to source control. parameters that define account specific things like VPC details, Key Pairs, or secrets like API keys can be defined per host outside of source control. For example, consider this global config: ~/.taskcat.yml general : s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair Given a simple project config: project : name : my-project regions : - us-east-2 tests : default : template : ./template.yaml The effective test configuration would become: tests : default : template : ./template.yaml s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair If any item is re-defined in a project it takes precedence over the global value. Anything defined in a test takes precedence over what is defined in the project or global configuration. with the exception of the parameters section which works in reverse. For example, using the same global config as above, given this project config: project : name : my-project regions : - us-east-2 s3_bucket : my-project-s3-bucket tests : default : template : ./template.yaml parameters : KeyPair : my-test-ec2-keypair Would result in this effective test configuration: tests : default : template : ./template.yaml s3_bucket : my-project-s3-bucket parameters : KeyPair : my-global-ec2-keypair Notice that s3_bucket took the most specific value and KeyPair the most general.","title":"Precedence"},{"location":"docs/usage/GENERAL_USAGE/#cli-interface","text":"taskcat adopts a similar cli command structure to git with a taskcat command subcommand --flag style. The cli is also designed to be simplest if run from the root of a project. Let's have a look at equivalent command to run a test: cd into the project root and type test run : cd ./quickstart-aws-vpc taskcat test run or run it from anywhere by providing the path to the project root taskcat test run -p ./quickstart-aws-vpc","title":"CLI interface"},{"location":"docs/usage/GENERAL_USAGE/#configuration-files","text":"The configuration files required for taskcat have changed, to ease migration, if taskcat is run and legacy config files are found, they are converted and written to new file locations. For more information on the new format, see the config file docs .","title":"Configuration files"},{"location":"docs/usage/PARAMETER_OVERRIDES/","text":"Parameter overrides Parameter Overrides were added to the taskcat to solve a couple of common problems. First, many templates share common parameters that are unique to an AWS account, like a KeyPair name or an S3 Bucket, overrides provided a way to store those centrally for all your projects. Second, we didn't want to add sensitive data (usernames, passwords, tokens) to a git repository. The idea was to store sensitive/unique data outside of a git repository, but still execute a test using this data. To that end, any parameter defined in the global config will take precedence over the same parameter in a project-level config .","title":"Parameter Overrides"},{"location":"docs/usage/PARAMETER_OVERRIDES/#parameter-overrides","text":"Parameter Overrides were added to the taskcat to solve a couple of common problems. First, many templates share common parameters that are unique to an AWS account, like a KeyPair name or an S3 Bucket, overrides provided a way to store those centrally for all your projects. Second, we didn't want to add sensitive data (usernames, passwords, tokens) to a git repository. The idea was to store sensitive/unique data outside of a git repository, but still execute a test using this data. To that end, any parameter defined in the global config will take precedence over the same parameter in a project-level config .","title":"Parameter overrides"},{"location":"docs/usage/PSUEDO_PARAMETERS/","text":"To increase the flexibility of taskcat, we've built-in support for psuedo-parameters that are transposed at runtime for actual values. Following table describes the supported psuedo-parameters . Psuedo-Parameter Example Value passed to the CloudFormation stack Details $[taskcat_autobucket] taskcat-tag-sample-taskcat-project-5fba6597 Note: The S3 Bucket is created $[taskcat_genaz_1] \"us-east-1a\" Fetches a single Availability Zone within the region being launched in $[taskcat_genaz_2] \"us-east-1a,us-east-1b\" Fetches two AvailabilityZones within the region being launched in $[taskcat_genaz_3] \"us-east-1a,us-east-1b,us-east-1c\" Fetches three AvailabilityZones within the region being launched in $[taskcat_genpass_8A] tI8zN3iX8 An alphanumeric 8-charater random password. The length is customizable. $[taskcat_genpass_8S] mA5@cB5! An alphanumeric 8-charater random password. The length is customizable. $[taskcat_random-string] yysuawpwubvotiqgwjcu Generates a random string $[taskcat_random-numbers] 56188163597280820763 Generates random numbers. $[taskcat_genuuid] 1c2e3483-2c99-45bb-801d-8af68a3b907b Generates a UUID $[taskcat_getval_MyAppPassword] Dynamically generated password for the MyAppPassword parameter Retreives another parameter value. $[taskcat_current_region] \"us-east-2\" Region the test is being prepared for $[taskcat_project_name] \"my-example-project\" Name of the project being tested $[taskcat_test_name] \"cluster-with-windows-ad\" Name of the test being tested $[taskcat_ssm_/path/to/ssm/parameter] SSM Parameter Value Retreives values from SSM $[taskcat_secretsmanager_SecretNameOrARN] Value from SecretsManager Retreives a secret value from SecretsManager given an name or ARN From: (defined in taskcat.yaml') InstanceType : t2.small AvailablityZones : $[taskcat_genaz_2] RandomString : $[taskcat_random-string] RandomNumbers : $[taskcat_random-numbers] GenerateUUID : $[taskcat_genuuid] Password : $[taskcat_genpass_8A] PasswordConfirm : $[taskcat_getval_Password] To: (At runtime passed to cloudformation API) InstanceType : t2.small AvailablityZones : us-east-1a : us-east1b RandomString : yysuawpwubvotiqgwjcu RandomNumbers : 56188163597280820763 GenerateUUID : 1c2e3483-2c99-45bb-801d-8af68a3b907b Password : tI8zN3iX8 PasswordConfirm : tI8zN3iX8","title":"Psuedo Parameters"},{"location":"docs/usage/PSUEDO_PARAMETERS/#from-defined-in-taskcatyaml","text":"InstanceType : t2.small AvailablityZones : $[taskcat_genaz_2] RandomString : $[taskcat_random-string] RandomNumbers : $[taskcat_random-numbers] GenerateUUID : $[taskcat_genuuid] Password : $[taskcat_genpass_8A] PasswordConfirm : $[taskcat_getval_Password]","title":"From: (defined in taskcat.yaml')"},{"location":"docs/usage/PSUEDO_PARAMETERS/#to-at-runtime-passed-to-cloudformation-api","text":"InstanceType : t2.small AvailablityZones : us-east-1a : us-east1b RandomString : yysuawpwubvotiqgwjcu RandomNumbers : 56188163597280820763 GenerateUUID : 1c2e3483-2c99-45bb-801d-8af68a3b907b Password : tI8zN3iX8 PasswordConfirm : tI8zN3iX8","title":"To: (At runtime passed to cloudformation API)"},{"location":"reference/taskcat_plugin_testhook/","text":"Module taskcat_plugin_testhook None None View Source from typing import Mapping , Optional from taskcat._config import Config from taskcat._dataclasses import TestObj from taskcat.exceptions import TaskCatException from taskcat.testing._hooks import BaseTaskcatHook class Hook ( BaseTaskcatHook ): def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str , TestObj ], parameters : dict , outputs : Optional [ dict ], ): super () . __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ): raise TaskCatException ( \"generated failure from hook\" ) Classes Hook class Hook ( hook_config : dict , config : taskcat . _config . Config , tests : Mapping [ str , taskcat . _dataclasses . TestObj ], parameters : dict , outputs : Union [ dict , NoneType ] ) View Source class Hook ( BaseTaskcatHook ) : def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str, TestObj ] , parameters : dict , outputs : Optional [ dict ] , ) : super (). __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ) : raise TaskCatException ( \"generated failure from hook\" ) Ancestors (in MRO) taskcat.testing._hooks.BaseTaskcatHook","title":"Taskcat Plugin Testhook"},{"location":"reference/taskcat_plugin_testhook/#module-taskcat_plugin_testhook","text":"None None View Source from typing import Mapping , Optional from taskcat._config import Config from taskcat._dataclasses import TestObj from taskcat.exceptions import TaskCatException from taskcat.testing._hooks import BaseTaskcatHook class Hook ( BaseTaskcatHook ): def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str , TestObj ], parameters : dict , outputs : Optional [ dict ], ): super () . __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ): raise TaskCatException ( \"generated failure from hook\" )","title":"Module taskcat_plugin_testhook"},{"location":"reference/taskcat_plugin_testhook/#classes","text":"","title":"Classes"},{"location":"reference/taskcat_plugin_testhook/#hook","text":"class Hook ( hook_config : dict , config : taskcat . _config . Config , tests : Mapping [ str , taskcat . _dataclasses . TestObj ], parameters : dict , outputs : Union [ dict , NoneType ] ) View Source class Hook ( BaseTaskcatHook ) : def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str, TestObj ] , parameters : dict , outputs : Optional [ dict ] , ) : super (). __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ) : raise TaskCatException ( \"generated failure from hook\" )","title":"Hook"},{"location":"reference/taskcat_plugin_testhook/#ancestors-in-mro","text":"taskcat.testing._hooks.BaseTaskcatHook","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/","text":"Module taskcat taskcat python module None View Source \"\"\" taskcat python module \"\"\" from ._cfn.stack import Stack # noqa: F401 from ._cfn.template import Template # noqa: F401 from ._cli import main # noqa: F401 from ._config import Config # noqa: F401 __all__ = [ \"Stack\" , \"Template\" , \"Config\" , \"main\" ] Sub-modules taskcat.exceptions taskcat.iam_policy taskcat.local_zones taskcat.regions_to_partitions taskcat.testing Functions main def main ( cli_core_class =< class ' taskcat . _cli_core . CliCore '>, exit_func =< function exit_with_code at 0x7fea6e4d0ca0 > ) View Source def main ( cli_core_class = CliCore , exit_func = exit_with_code ): signal . signal ( signal . SIGINT , _sigint_handler ) log_level = _setup_logging ( sys . argv ) args = sys . argv [ 1 :] if not args : args . append ( \"-h\" ) try : _welcome () version = get_installed_version () cli = cli_core_class ( NAME , _cli_modules , DESCRIPTION , version , GLOBAL_ARGS . ARGS ) cli . parse ( args ) _default_profile = cli . parsed_args . __dict__ . get ( \"_profile\" ) if _default_profile : GLOBAL_ARGS . profile = _default_profile cli . run () except TaskCatException as e : LOG . error ( str ( e ), exc_info = _print_tracebacks ( log_level )) exit_func ( 1 ) except Exception as e : # pylint: disable=broad-except LOG . error ( \" %s %s \" , e . __class__ . __name__ , str ( e ), exc_info = _print_tracebacks ( log_level ) ) exit_func ( 1 ) Classes Config class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path ) View Source class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @ staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @ staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @ staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path {file_path}\" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @ staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def _get_regions ( self , region_parameter_name , test , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_object = {} for region in getattr ( test , region_parameter_name , []): # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_object [ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_object def get_regions ( self , boto3_cache : Boto3Cache = None ): region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects def get_artifact_regions ( self , boto3_cache : Boto3Cache = None ): region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): if test . artifact_regions is not None : region_objects [ test_name ] = self . _get_regions ( \"artifact_regions\" , test , boto3_cache ) else : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_artifact_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 org_id = self . config . project . org_id if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , org_id = org_id , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \"{region.account_id}{region.name}\" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 org_id = self . config . project . org_id if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \"{test.s3_bucket}-{region.name}\" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , org_id = org_id , ) if new : bucket_obj . create () return bucket_obj @ staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @ staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] artifact_region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , artifact_regions = artifact_region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests Static methods create def create ( template_file : Union [ pathlib . Path , NoneType ] = None , args : Union [ dict , NoneType ] = None , global_config_path : pathlib . Path = PosixPath ( '/home/trlindsa/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat/.taskcat_overrides.yml' ), env_vars : Union [ dict , NoneType ] = None , project_root : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) get_params_from_templates def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters Methods get_artifact_regions def get_artifact_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_artifact_regions ( self , boto3_cache : Boto3Cache = None ) : region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : if test . artifact_regions is not None : region_objects [ test_name ] = self . _get_regions ( \"artifact_regions\" , test , boto3_cache ) else : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects get_buckets def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_artifact_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings get_regions def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects get_rendered_parameters def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters get_templates def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates get_tests def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] artifact_region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , artifact_regions = artifact_region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests Stack class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Class variables REMOTE_TEMPLATE_PATH Static methods create def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack delete def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) import_existing def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack Instance variables launch_succeeded status Methods children def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children descendants def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self ) error_events def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors events def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events refresh def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () resources def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources set_stack_properties def set_stack_properties ( self , stack_properties : Union [ dict , NoneType ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () update def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\") Template class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x7fea8881f280 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Instance variables descendents linesplit s3_key s3_key_prefix Methods parameters def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ str , int ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters url_prefix def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix write def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding = \"utf-8\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"Index"},{"location":"reference/taskcat/#module-taskcat","text":"taskcat python module None View Source \"\"\" taskcat python module \"\"\" from ._cfn.stack import Stack # noqa: F401 from ._cfn.template import Template # noqa: F401 from ._cli import main # noqa: F401 from ._config import Config # noqa: F401 __all__ = [ \"Stack\" , \"Template\" , \"Config\" , \"main\" ]","title":"Module taskcat"},{"location":"reference/taskcat/#sub-modules","text":"taskcat.exceptions taskcat.iam_policy taskcat.local_zones taskcat.regions_to_partitions taskcat.testing","title":"Sub-modules"},{"location":"reference/taskcat/#functions","text":"","title":"Functions"},{"location":"reference/taskcat/#main","text":"def main ( cli_core_class =< class ' taskcat . _cli_core . CliCore '>, exit_func =< function exit_with_code at 0x7fea6e4d0ca0 > ) View Source def main ( cli_core_class = CliCore , exit_func = exit_with_code ): signal . signal ( signal . SIGINT , _sigint_handler ) log_level = _setup_logging ( sys . argv ) args = sys . argv [ 1 :] if not args : args . append ( \"-h\" ) try : _welcome () version = get_installed_version () cli = cli_core_class ( NAME , _cli_modules , DESCRIPTION , version , GLOBAL_ARGS . ARGS ) cli . parse ( args ) _default_profile = cli . parsed_args . __dict__ . get ( \"_profile\" ) if _default_profile : GLOBAL_ARGS . profile = _default_profile cli . run () except TaskCatException as e : LOG . error ( str ( e ), exc_info = _print_tracebacks ( log_level )) exit_func ( 1 ) except Exception as e : # pylint: disable=broad-except LOG . error ( \" %s %s \" , e . __class__ . __name__ , str ( e ), exc_info = _print_tracebacks ( log_level ) ) exit_func ( 1 )","title":"main"},{"location":"reference/taskcat/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/#config","text":"class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path ) View Source class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @ staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @ staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @ staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path {file_path}\" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @ staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def _get_regions ( self , region_parameter_name , test , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_object = {} for region in getattr ( test , region_parameter_name , []): # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_object [ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_object def get_regions ( self , boto3_cache : Boto3Cache = None ): region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects def get_artifact_regions ( self , boto3_cache : Boto3Cache = None ): region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): if test . artifact_regions is not None : region_objects [ test_name ] = self . _get_regions ( \"artifact_regions\" , test , boto3_cache ) else : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_artifact_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 org_id = self . config . project . org_id if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , org_id = org_id , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \"{region.account_id}{region.name}\" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 org_id = self . config . project . org_id if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \"{test.s3_bucket}-{region.name}\" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , org_id = org_id , ) if new : bucket_obj . create () return bucket_obj @ staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @ staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] artifact_region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , artifact_regions = artifact_region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"Config"},{"location":"reference/taskcat/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/#create","text":"def create ( template_file : Union [ pathlib . Path , NoneType ] = None , args : Union [ dict , NoneType ] = None , global_config_path : pathlib . Path = PosixPath ( '/home/trlindsa/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat/.taskcat_overrides.yml' ), env_vars : Union [ dict , NoneType ] = None , project_root : pathlib . Path = PosixPath ( '/home/trlindsa/git/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" , encoding = \"utf-8\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root )","title":"create"},{"location":"reference/taskcat/#get_params_from_templates","text":"def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters","title":"get_params_from_templates"},{"location":"reference/taskcat/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/#get_artifact_regions","text":"def get_artifact_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_artifact_regions ( self , boto3_cache : Boto3Cache = None ) : region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : if test . artifact_regions is not None : region_objects [ test_name ] = self . _get_regions ( \"artifact_regions\" , test , boto3_cache ) else : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects","title":"get_artifact_regions"},{"location":"reference/taskcat/#get_buckets","text":"def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_artifact_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings","title":"get_buckets"},{"location":"reference/taskcat/#get_regions","text":"def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = self . _get_regions ( \"regions\" , test , boto3_cache ) return region_objects","title":"get_regions"},{"location":"reference/taskcat/#get_rendered_parameters","text":"def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters","title":"get_rendered_parameters"},{"location":"reference/taskcat/#get_templates","text":"def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates","title":"get_templates"},{"location":"reference/taskcat/#get_tests","text":"def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] artifact_region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , artifact_regions = artifact_region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"get_tests"},{"location":"reference/taskcat/#stack","text":"class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Stack"},{"location":"reference/taskcat/#class-variables","text":"REMOTE_TEMPLATE_PATH","title":"Class variables"},{"location":"reference/taskcat/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/taskcat/#create_1","text":"def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack","title":"create"},{"location":"reference/taskcat/#delete","text":"def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" )","title":"delete"},{"location":"reference/taskcat/#import_existing","text":"def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack","title":"import_existing"},{"location":"reference/taskcat/#instance-variables","text":"launch_succeeded status","title":"Instance variables"},{"location":"reference/taskcat/#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/#children","text":"def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children","title":"children"},{"location":"reference/taskcat/#descendants","text":"def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self )","title":"descendants"},{"location":"reference/taskcat/#error_events","text":"def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors","title":"error_events"},{"location":"reference/taskcat/#events","text":"def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events","title":"events"},{"location":"reference/taskcat/#refresh","text":"def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now ()","title":"refresh"},{"location":"reference/taskcat/#resources","text":"def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources","title":"resources"},{"location":"reference/taskcat/#set_stack_properties","text":"def set_stack_properties ( self , stack_properties : Union [ dict , NoneType ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start ()","title":"set_stack_properties"},{"location":"reference/taskcat/#update","text":"def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\")","title":"update"},{"location":"reference/taskcat/#template","text":"class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x7fea8881f280 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Template"},{"location":"reference/taskcat/#instance-variables_1","text":"descendents linesplit s3_key s3_key_prefix","title":"Instance variables"},{"location":"reference/taskcat/#methods_2","text":"","title":"Methods"},{"location":"reference/taskcat/#parameters","text":"def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ str , int ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters","title":"parameters"},{"location":"reference/taskcat/#url_prefix","text":"def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix","title":"url_prefix"},{"location":"reference/taskcat/#write","text":"def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding = \"utf-8\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"write"},{"location":"reference/taskcat/exceptions/","text":"Module taskcat.exceptions None None View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ () Classes InvalidActionError class InvalidActionError ( expression ) Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred View Source class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ () Ancestors (in MRO) taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. TaskCatException class TaskCatException ( / , * args , ** kwargs ) View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" Ancestors (in MRO) builtins.Exception builtins.BaseException Descendants taskcat.exceptions.InvalidActionError taskcat._s3_stage.S3BucketCreatorException taskcat._amiupdater.AMIUpdaterFatalException taskcat._amiupdater.AMIUpdaterCommitNeededException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Exceptions"},{"location":"reference/taskcat/exceptions/#module-taskcatexceptions","text":"None None View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ ()","title":"Module taskcat.exceptions"},{"location":"reference/taskcat/exceptions/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/exceptions/#invalidactionerror","text":"class InvalidActionError ( expression ) Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred View Source class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ ()","title":"InvalidActionError"},{"location":"reference/taskcat/exceptions/#ancestors-in-mro","text":"taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/exceptions/#class-variables","text":"args","title":"Class variables"},{"location":"reference/taskcat/exceptions/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/exceptions/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/taskcat/exceptions/#taskcatexception","text":"class TaskCatException ( / , * args , ** kwargs ) View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\"","title":"TaskCatException"},{"location":"reference/taskcat/exceptions/#ancestors-in-mro_1","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/exceptions/#descendants","text":"taskcat.exceptions.InvalidActionError taskcat._s3_stage.S3BucketCreatorException taskcat._amiupdater.AMIUpdaterFatalException taskcat._amiupdater.AMIUpdaterCommitNeededException","title":"Descendants"},{"location":"reference/taskcat/exceptions/#class-variables_1","text":"args","title":"Class variables"},{"location":"reference/taskcat/exceptions/#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/exceptions/#with_traceback_1","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/taskcat/local_zones/","text":"Module taskcat.local_zones None None View Source ZONES = [ \"afs1-az1\", \"afs1-az2\", \"afs1-az3\", \"ape1-az1\", \"ape1-az2\", \"ape1-az3\", \"apne1-az1\", \"apne1-az2\", \"apne1-az4\", \"apne2-az1\", \"apne2-az2\", \"apne2-az3\", \"apne2-az4\", \"apne3-az1\", \"apne3-az2\", \"apne3-az3\", \"aps1-az1\", \"aps1-az2\", \"aps1-az3\", \"apse1-az1\", \"apse1-az2\", \"apse1-az3\", \"apse2-az1\", \"apse2-az2\", \"apse2-az3\", \"apse3-az1\", \"apse3-az2\", \"apse3-az3\", \"cac1-az1\", \"cac1-az2\", \"cac1-az4\", \"euc1-az1\", \"euc1-az2\", \"euc1-az3\", \"eun1-az1\", \"eun1-az2\", \"eun1-az3\", \"eus1-az1\", \"eus1-az2\", \"eus1-az3\", \"euw1-az1\", \"euw1-az2\", \"euw1-az3\", \"euw2-az1\", \"euw2-az2\", \"euw2-az3\", \"euw3-az1\", \"euw3-az2\", \"euw3-az3\", \"mes1-az1\", \"mes1-az2\", \"mes1-az3\", \"sae1-az1\", \"sae1-az2\", \"sae1-az3\", \"use1-atl1-az1\", \"use1-az1\", \"use1-az2\", \"use1-az3\", \"use1-az4\", \"use1-az5\", \"use1-az6\", \"use1-bos1-az1\", \"use1-chi1-az1\", \"use1-dfw1-az1\", \"use1-iah1-az1\", \"use1-mci1-az1\", \"use1-mia1-az1\", \"use1-msp1-az1\", \"use1-nyc1-az1\", \"use1-phl1-az1\", \"use2-az1\", \"use2-az2\", \"use2-az3\", \"usw1-az1\", \"usw1-az3\", \"usw2-az1\", \"usw2-az2\", \"usw2-az3\", \"usw2-az4\", \"usw2-den1-az1\", \"usw2-las1-az1\", \"usw2-lax1-az1\", \"usw2-lax1-az2\", \"usw2-pdx1-az1\", \"usw2-phx1-az1\", \"usw2-sea1-az1\", ] Variables ZONES","title":"Local Zones"},{"location":"reference/taskcat/local_zones/#module-taskcatlocal_zones","text":"None None View Source ZONES = [ \"afs1-az1\", \"afs1-az2\", \"afs1-az3\", \"ape1-az1\", \"ape1-az2\", \"ape1-az3\", \"apne1-az1\", \"apne1-az2\", \"apne1-az4\", \"apne2-az1\", \"apne2-az2\", \"apne2-az3\", \"apne2-az4\", \"apne3-az1\", \"apne3-az2\", \"apne3-az3\", \"aps1-az1\", \"aps1-az2\", \"aps1-az3\", \"apse1-az1\", \"apse1-az2\", \"apse1-az3\", \"apse2-az1\", \"apse2-az2\", \"apse2-az3\", \"apse3-az1\", \"apse3-az2\", \"apse3-az3\", \"cac1-az1\", \"cac1-az2\", \"cac1-az4\", \"euc1-az1\", \"euc1-az2\", \"euc1-az3\", \"eun1-az1\", \"eun1-az2\", \"eun1-az3\", \"eus1-az1\", \"eus1-az2\", \"eus1-az3\", \"euw1-az1\", \"euw1-az2\", \"euw1-az3\", \"euw2-az1\", \"euw2-az2\", \"euw2-az3\", \"euw3-az1\", \"euw3-az2\", \"euw3-az3\", \"mes1-az1\", \"mes1-az2\", \"mes1-az3\", \"sae1-az1\", \"sae1-az2\", \"sae1-az3\", \"use1-atl1-az1\", \"use1-az1\", \"use1-az2\", \"use1-az3\", \"use1-az4\", \"use1-az5\", \"use1-az6\", \"use1-bos1-az1\", \"use1-chi1-az1\", \"use1-dfw1-az1\", \"use1-iah1-az1\", \"use1-mci1-az1\", \"use1-mia1-az1\", \"use1-msp1-az1\", \"use1-nyc1-az1\", \"use1-phl1-az1\", \"use2-az1\", \"use2-az2\", \"use2-az3\", \"usw1-az1\", \"usw1-az3\", \"usw2-az1\", \"usw2-az2\", \"usw2-az3\", \"usw2-az4\", \"usw2-den1-az1\", \"usw2-las1-az1\", \"usw2-lax1-az1\", \"usw2-lax1-az2\", \"usw2-pdx1-az1\", \"usw2-phx1-az1\", \"usw2-sea1-az1\", ]","title":"Module taskcat.local_zones"},{"location":"reference/taskcat/local_zones/#variables","text":"ZONES","title":"Variables"},{"location":"reference/taskcat/regions_to_partitions/","text":"Module taskcat.regions_to_partitions None None View Source REGIONS = { \"af-south-1\": \"aws\", \"ap-east-1\": \"aws\", \"ap-northeast-1\": \"aws\", \"ap-northeast-2\": \"aws\", \"ap-northeast-3\": \"aws\", \"ap-south-1\": \"aws\", \"ap-southeast-1\": \"aws\", \"ap-southeast-2\": \"aws\", \"ap-southeast-3\": \"aws\", \"ca-central-1\": \"aws\", \"eu-central-1\": \"aws\", \"eu-north-1\": \"aws\", \"eu-south-1\": \"aws\", \"eu-west-1\": \"aws\", \"eu-west-2\": \"aws\", \"eu-west-3\": \"aws\", \"me-south-1\": \"aws\", \"sa-east-1\": \"aws\", \"us-east-1\": \"aws\", \"us-east-2\": \"aws\", \"us-west-1\": \"aws\", \"us-west-2\": \"aws\", \"cn-north-1\": \"aws-cn\", \"cn-northwest-1\": \"aws-cn\", \"us-gov-east-1\": \"aws-us-gov\", \"us-gov-west-1\": \"aws-us-gov\", \"us-iso-east-1\": \"aws-iso\", \"us-iso-west-1\": \"aws-iso\", \"us-isob-east-1\": \"aws-iso-b\" } PARTITIONS = { \"aws\": [ \"af-south-1\", \"ap-east-1\", \"ap-northeast-1\", \"ap-northeast-2\", \"ap-northeast-3\", \"ap-south-1\", \"ap-southeast-1\", \"ap-southeast-2\", \"ap-southeast-3\", \"ca-central-1\", \"eu-central-1\", \"eu-north-1\", \"eu-south-1\", \"eu-west-1\", \"eu-west-2\", \"eu-west-3\", \"me-south-1\", \"sa-east-1\", \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ], \"aws-cn\": [ \"cn-north-1\", \"cn-northwest-1\" ], \"aws-us-gov\": [ \"us-gov-east-1\", \"us-gov-west-1\" ], \"aws-iso\": [ \"us-iso-east-1\", \"us-iso-west-1\" ], \"aws-iso-b\": [ \"us-isob-east-1\" ] } Variables PARTITIONS REGIONS","title":"Regions To Partitions"},{"location":"reference/taskcat/regions_to_partitions/#module-taskcatregions_to_partitions","text":"None None View Source REGIONS = { \"af-south-1\": \"aws\", \"ap-east-1\": \"aws\", \"ap-northeast-1\": \"aws\", \"ap-northeast-2\": \"aws\", \"ap-northeast-3\": \"aws\", \"ap-south-1\": \"aws\", \"ap-southeast-1\": \"aws\", \"ap-southeast-2\": \"aws\", \"ap-southeast-3\": \"aws\", \"ca-central-1\": \"aws\", \"eu-central-1\": \"aws\", \"eu-north-1\": \"aws\", \"eu-south-1\": \"aws\", \"eu-west-1\": \"aws\", \"eu-west-2\": \"aws\", \"eu-west-3\": \"aws\", \"me-south-1\": \"aws\", \"sa-east-1\": \"aws\", \"us-east-1\": \"aws\", \"us-east-2\": \"aws\", \"us-west-1\": \"aws\", \"us-west-2\": \"aws\", \"cn-north-1\": \"aws-cn\", \"cn-northwest-1\": \"aws-cn\", \"us-gov-east-1\": \"aws-us-gov\", \"us-gov-west-1\": \"aws-us-gov\", \"us-iso-east-1\": \"aws-iso\", \"us-iso-west-1\": \"aws-iso\", \"us-isob-east-1\": \"aws-iso-b\" } PARTITIONS = { \"aws\": [ \"af-south-1\", \"ap-east-1\", \"ap-northeast-1\", \"ap-northeast-2\", \"ap-northeast-3\", \"ap-south-1\", \"ap-southeast-1\", \"ap-southeast-2\", \"ap-southeast-3\", \"ca-central-1\", \"eu-central-1\", \"eu-north-1\", \"eu-south-1\", \"eu-west-1\", \"eu-west-2\", \"eu-west-3\", \"me-south-1\", \"sa-east-1\", \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ], \"aws-cn\": [ \"cn-north-1\", \"cn-northwest-1\" ], \"aws-us-gov\": [ \"us-gov-east-1\", \"us-gov-west-1\" ], \"aws-iso\": [ \"us-iso-east-1\", \"us-iso-west-1\" ], \"aws-iso-b\": [ \"us-isob-east-1\" ] }","title":"Module taskcat.regions_to_partitions"},{"location":"reference/taskcat/regions_to_partitions/#variables","text":"PARTITIONS REGIONS","title":"Variables"},{"location":"reference/taskcat/_cfn/","text":"Module taskcat._cfn None None Sub-modules taskcat._cfn.stack taskcat._cfn.stack_url_helper taskcat._cfn.template taskcat._cfn.threaded","title":"Index"},{"location":"reference/taskcat/_cfn/#module-taskcat_cfn","text":"None None","title":"Module taskcat._cfn"},{"location":"reference/taskcat/_cfn/#sub-modules","text":"taskcat._cfn.stack taskcat._cfn.stack_url_helper taskcat._cfn.template taskcat._cfn.threaded","title":"Sub-modules"},{"location":"reference/taskcat/_cfn/stack/","text":"Module taskcat._cfn.stack None None View Source import json import logging import os import random import re import string from datetime import datetime , timedelta from pathlib import Path from threading import Timer from typing import Callable , List , Optional , Tuple from uuid import UUID , uuid4 import boto3 import yaml from taskcat._cfn.template import Template , tcat_template_cache from taskcat._common_utils import ordered_dump , pascal_to_snake , s3_url_maker from taskcat._dataclasses import Tag , TestRegion LOG = logging . getLogger ( __name__ ) GENERIC_ERROR_PATTERNS = [ r \"(The following resource\\(s\\) failed to create: )\" , r \"(^Resource creation cancelled$)\" , ] def criteria_matches ( criteria : dict , instance ): # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ): raise ValueError ( f \" { k } is not a valid property of { type ( instance ) } \" ) for k , v in criteria . items (): # matching is AND for multiple criteria, so as soon as one fails, # it's not a match if getattr ( instance , k ) != v : return False return True class StackStatus : COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] class Capabilities : IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \" {} {} {} \" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {} >\" . format ( self . event_id , hex ( id ( self ))) class Resource : def __init__ ( self , stack_id : str , resource_dict : dict , test_name : str = \"\" , uuid : UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id : str = stack_id self . test_name : str = test_name self . uuid : UUID = uuid self . logical_id : str = resource_dict [ \"LogicalResourceId\" ] self . type : str = resource_dict [ \"ResourceType\" ] self . status : str = resource_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . last_updated_timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {} >\" . format ( self . logical_id , self . status ) class Parameter : def __init__ ( self , param_dict : dict ): self . key : str = param_dict [ \"ParameterKey\" ] self . value : str = \"\" self . raw_value : str = \"\" self . use_previous_value : bool = False self . resolved_value : str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value : self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value : param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value : param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] class FilterableList ( list ): def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ): if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ): flist . append ( item ) return flist class Stacks ( FilterableList ): pass class Resources ( FilterableList ): pass class Events ( FilterableList ): pass class Tags ( FilterableList ): pass class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {} >\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @property def launch_succeeded ( self ): return self . _launch_succeeded @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \" { stack_properties [ 'StackId' ] } . tried \" f \" { parent_stack . template . project_root / relative_path } \" f \" and { absolute_path } \" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: { str ( e ) } \" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: { stack_id } \" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Variables GENERIC_ERROR_PATTERNS LOG Functions criteria_matches def criteria_matches ( criteria : dict , instance ) View Source def criteria_matches ( criteria : dict , instance ) : # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ) : raise ValueError ( f \" {k} is not a valid property of {type(instance)} \" ) for k , v in criteria . items () : # matching is AND for multiple criteria , so as soon as one fails , # it ' s not a match if getattr ( instance , k ) != v : return False return True Classes Capabilities class Capabilities ( / , * args , ** kwargs ) View Source class Capabilities: IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] Class variables ALL AUTO_EXPAND IAM NAMED_IAM Event class Event ( event_dict : dict ) View Source class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \"{} {} {}\" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {}>\" . format ( self . event_id , hex ( id ( self ))) Events class Events ( / , * args , ** kwargs ) View Source class Events ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. FilterableList class FilterableList ( / , * args , ** kwargs ) View Source class FilterableList ( list ) : def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist Ancestors (in MRO) builtins.list Descendants taskcat._cfn.stack.Stacks taskcat._cfn.stack.Resources taskcat._cfn.stack.Events taskcat._cfn.stack.Tags Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Output class Output ( output_dict : dict ) View Source class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] Parameter class Parameter ( param_dict : dict ) View Source class Parameter: def __init__ ( self , param_dict: dict ): self . key: str = param_dict [ \"ParameterKey\" ] self . value: str = \"\" self . raw_value: str = \"\" self . use_previous_value: bool = False self . resolved_value: str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value: self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value: param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value: param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict Methods dump def dump ( self ) View Source def dump ( self ) : param_dict = { \" ParameterKey \" : self . key } if self . value : param_dict [ \" ParameterValue \" ] = self . value if self . use_previous_value : param_dict [ \" UsePreviousValue \" ] = self . use_previous_value return param_dict Resource class Resource ( stack_id : str , resource_dict : dict , test_name : str = '' , uuid : uuid . UUID = None ) View Source class Resource: def __init__ ( self , stack_id: str , resource_dict: dict , test_name: str = \"\" , uuid: UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id: str = stack_id self . test_name: str = test_name self . uuid: UUID = uuid self . logical_id: str = resource_dict [ \"LogicalResourceId\" ] self . type: str = resource_dict [ \"ResourceType\" ] self . status: str = resource_dict [ \"ResourceStatus\" ] self . physical_id: str = \"\" self . last_updated_timestamp: datetime = datetime . fromtimestamp ( 0 ) self . status_reason: str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {}>\" . format ( self . logical_id , self . status ) Resources class Resources ( / , * args , ** kwargs ) View Source class Resources ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Stack class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Class variables REMOTE_TEMPLATE_PATH Static methods create def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack delete def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) import_existing def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack Instance variables launch_succeeded status Methods children def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children descendants def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self ) error_events def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors events def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events refresh def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () resources def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources set_stack_properties def set_stack_properties ( self , stack_properties : Union [ dict , NoneType ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () update def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\") StackStatus class StackStatus ( / , * args , ** kwargs ) View Source class StackStatus: COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] Class variables COMPLETE FAILED IN_PROGRESS Stacks class Stacks ( / , * args , ** kwargs ) View Source class Stacks ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Tags class Tags ( / , * args , ** kwargs ) View Source class Tags ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"Stack"},{"location":"reference/taskcat/_cfn/stack/#module-taskcat_cfnstack","text":"None None View Source import json import logging import os import random import re import string from datetime import datetime , timedelta from pathlib import Path from threading import Timer from typing import Callable , List , Optional , Tuple from uuid import UUID , uuid4 import boto3 import yaml from taskcat._cfn.template import Template , tcat_template_cache from taskcat._common_utils import ordered_dump , pascal_to_snake , s3_url_maker from taskcat._dataclasses import Tag , TestRegion LOG = logging . getLogger ( __name__ ) GENERIC_ERROR_PATTERNS = [ r \"(The following resource\\(s\\) failed to create: )\" , r \"(^Resource creation cancelled$)\" , ] def criteria_matches ( criteria : dict , instance ): # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ): raise ValueError ( f \" { k } is not a valid property of { type ( instance ) } \" ) for k , v in criteria . items (): # matching is AND for multiple criteria, so as soon as one fails, # it's not a match if getattr ( instance , k ) != v : return False return True class StackStatus : COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] class Capabilities : IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \" {} {} {} \" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {} >\" . format ( self . event_id , hex ( id ( self ))) class Resource : def __init__ ( self , stack_id : str , resource_dict : dict , test_name : str = \"\" , uuid : UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id : str = stack_id self . test_name : str = test_name self . uuid : UUID = uuid self . logical_id : str = resource_dict [ \"LogicalResourceId\" ] self . type : str = resource_dict [ \"ResourceType\" ] self . status : str = resource_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . last_updated_timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {} >\" . format ( self . logical_id , self . status ) class Parameter : def __init__ ( self , param_dict : dict ): self . key : str = param_dict [ \"ParameterKey\" ] self . value : str = \"\" self . raw_value : str = \"\" self . use_previous_value : bool = False self . resolved_value : str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value : self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value : param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value : param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] class FilterableList ( list ): def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ): if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ): flist . append ( item ) return flist class Stacks ( FilterableList ): pass class Resources ( FilterableList ): pass class Events ( FilterableList ): pass class Tags ( FilterableList ): pass class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {} >\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @property def launch_succeeded ( self ): return self . _launch_succeeded @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \" { stack_properties [ 'StackId' ] } . tried \" f \" { parent_stack . template . project_root / relative_path } \" f \" and { absolute_path } \" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: { str ( e ) } \" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: { stack_id } \" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Module taskcat._cfn.stack"},{"location":"reference/taskcat/_cfn/stack/#variables","text":"GENERIC_ERROR_PATTERNS LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/stack/#functions","text":"","title":"Functions"},{"location":"reference/taskcat/_cfn/stack/#criteria_matches","text":"def criteria_matches ( criteria : dict , instance ) View Source def criteria_matches ( criteria : dict , instance ) : # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ) : raise ValueError ( f \" {k} is not a valid property of {type(instance)} \" ) for k , v in criteria . items () : # matching is AND for multiple criteria , so as soon as one fails , # it ' s not a match if getattr ( instance , k ) != v : return False return True","title":"criteria_matches"},{"location":"reference/taskcat/_cfn/stack/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/stack/#capabilities","text":"class Capabilities ( / , * args , ** kwargs ) View Source class Capabilities: IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ]","title":"Capabilities"},{"location":"reference/taskcat/_cfn/stack/#class-variables","text":"ALL AUTO_EXPAND IAM NAMED_IAM","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack/#event","text":"class Event ( event_dict : dict ) View Source class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \"{} {} {}\" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {}>\" . format ( self . event_id , hex ( id ( self )))","title":"Event"},{"location":"reference/taskcat/_cfn/stack/#events","text":"class Events ( / , * args , ** kwargs ) View Source class Events ( FilterableList ): pass","title":"Events"},{"location":"reference/taskcat/_cfn/stack/#ancestors-in-mro","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#append","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack/#clear","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack/#copy","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack/#extend","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack/#filter","text":"def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack/#insert","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack/#pop","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack/#remove","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack/#reverse","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack/#sort","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack/#filterablelist","text":"class FilterableList ( / , * args , ** kwargs ) View Source class FilterableList ( list ) : def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"FilterableList"},{"location":"reference/taskcat/_cfn/stack/#ancestors-in-mro_1","text":"builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack/#descendants","text":"taskcat._cfn.stack.Stacks taskcat._cfn.stack.Resources taskcat._cfn.stack.Events taskcat._cfn.stack.Tags","title":"Descendants"},{"location":"reference/taskcat/_cfn/stack/#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#append_1","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack/#clear_1","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack/#copy_1","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack/#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack/#extend_1","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack/#filter_1","text":"def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack/#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack/#insert_1","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack/#pop_1","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack/#remove_1","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack/#reverse_1","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack/#sort_1","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack/#output","text":"class Output ( output_dict : dict ) View Source class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ]","title":"Output"},{"location":"reference/taskcat/_cfn/stack/#parameter","text":"class Parameter ( param_dict : dict ) View Source class Parameter: def __init__ ( self , param_dict: dict ): self . key: str = param_dict [ \"ParameterKey\" ] self . value: str = \"\" self . raw_value: str = \"\" self . use_previous_value: bool = False self . resolved_value: str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value: self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value: param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value: param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict","title":"Parameter"},{"location":"reference/taskcat/_cfn/stack/#methods_2","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#dump","text":"def dump ( self ) View Source def dump ( self ) : param_dict = { \" ParameterKey \" : self . key } if self . value : param_dict [ \" ParameterValue \" ] = self . value if self . use_previous_value : param_dict [ \" UsePreviousValue \" ] = self . use_previous_value return param_dict","title":"dump"},{"location":"reference/taskcat/_cfn/stack/#resource","text":"class Resource ( stack_id : str , resource_dict : dict , test_name : str = '' , uuid : uuid . UUID = None ) View Source class Resource: def __init__ ( self , stack_id: str , resource_dict: dict , test_name: str = \"\" , uuid: UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id: str = stack_id self . test_name: str = test_name self . uuid: UUID = uuid self . logical_id: str = resource_dict [ \"LogicalResourceId\" ] self . type: str = resource_dict [ \"ResourceType\" ] self . status: str = resource_dict [ \"ResourceStatus\" ] self . physical_id: str = \"\" self . last_updated_timestamp: datetime = datetime . fromtimestamp ( 0 ) self . status_reason: str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {}>\" . format ( self . logical_id , self . status )","title":"Resource"},{"location":"reference/taskcat/_cfn/stack/#resources","text":"class Resources ( / , * args , ** kwargs ) View Source class Resources ( FilterableList ): pass","title":"Resources"},{"location":"reference/taskcat/_cfn/stack/#ancestors-in-mro_2","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack/#methods_3","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#append_2","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack/#clear_2","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack/#copy_2","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack/#count_2","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack/#extend_2","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack/#filter_2","text":"def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack/#index_2","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack/#insert_2","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack/#pop_2","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack/#remove_2","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack/#reverse_2","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack/#sort_2","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack/#stack","text":"class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" , encoding = \"utf-8\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Stack"},{"location":"reference/taskcat/_cfn/stack/#class-variables_1","text":"REMOTE_TEMPLATE_PATH","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/stack/#create","text":"def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack","title":"create"},{"location":"reference/taskcat/_cfn/stack/#delete","text":"def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" )","title":"delete"},{"location":"reference/taskcat/_cfn/stack/#import_existing","text":"def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack","title":"import_existing"},{"location":"reference/taskcat/_cfn/stack/#instance-variables","text":"launch_succeeded status","title":"Instance variables"},{"location":"reference/taskcat/_cfn/stack/#methods_4","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#children","text":"def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children","title":"children"},{"location":"reference/taskcat/_cfn/stack/#descendants_1","text":"def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self )","title":"descendants"},{"location":"reference/taskcat/_cfn/stack/#error_events","text":"def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors","title":"error_events"},{"location":"reference/taskcat/_cfn/stack/#events_1","text":"def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events","title":"events"},{"location":"reference/taskcat/_cfn/stack/#refresh","text":"def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now ()","title":"refresh"},{"location":"reference/taskcat/_cfn/stack/#resources_1","text":"def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources","title":"resources"},{"location":"reference/taskcat/_cfn/stack/#set_stack_properties","text":"def set_stack_properties ( self , stack_properties : Union [ dict , NoneType ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start ()","title":"set_stack_properties"},{"location":"reference/taskcat/_cfn/stack/#update","text":"def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\")","title":"update"},{"location":"reference/taskcat/_cfn/stack/#stackstatus","text":"class StackStatus ( / , * args , ** kwargs ) View Source class StackStatus: COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ]","title":"StackStatus"},{"location":"reference/taskcat/_cfn/stack/#class-variables_2","text":"COMPLETE FAILED IN_PROGRESS","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack/#stacks","text":"class Stacks ( / , * args , ** kwargs ) View Source class Stacks ( FilterableList ): pass","title":"Stacks"},{"location":"reference/taskcat/_cfn/stack/#ancestors-in-mro_3","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack/#methods_5","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#append_3","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack/#clear_3","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack/#copy_3","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack/#count_3","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack/#extend_3","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack/#filter_3","text":"def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack/#index_3","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack/#insert_3","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack/#pop_3","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack/#remove_3","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack/#reverse_3","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack/#sort_3","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack/#tags","text":"class Tags ( / , * args , ** kwargs ) View Source class Tags ( FilterableList ): pass","title":"Tags"},{"location":"reference/taskcat/_cfn/stack/#ancestors-in-mro_4","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack/#methods_6","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack/#append_4","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack/#clear_4","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack/#copy_4","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack/#count_4","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack/#extend_4","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack/#filter_4","text":"def filter ( self , criteria : Union [ dict , NoneType ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack/#index_4","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack/#insert_4","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack/#pop_4","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack/#remove_4","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack/#reverse_4","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack/#sort_4","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack_url_helper/","text":"Module taskcat._cfn.stack_url_helper Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. View Source \"\"\" Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \"\"\" import json import logging import os from pathlib import Path from urllib.parse import urlparse LOG = logging . getLogger ( __name__ ) class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @staticmethod def evaluate_fn_join ( expression ): \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @staticmethod def evaluate_fn_if ( expression ): \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] # pylint: disable=consider-iterating-dictionary if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\"Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths Variables LOG Classes StackURLHelper class StackURLHelper ( template_mappings = None , template_parameters = None , parameter_values = None ) View Source class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @ staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @ staticmethod def evaluate_fn_join ( expression ): \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @ staticmethod def evaluate_fn_if ( expression ): \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] # pylint: disable=consider-iterating-dictionary if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @ staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @ staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\"Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @ staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths Class variables MAX_DEPTH SUBSTITUTION Static methods evaluate_fn_getatt def evaluate_fn_getatt ( expression ) View Source @staticmethod def evaluate_fn_getatt ( expression ) : raise Exception ( \"Fn::GetAtt: not supported\" ) evaluate_fn_if def evaluate_fn_if ( expression ) Return both possible parts of the expression View Source @staticmethod def evaluate_fn_if ( expression ) : \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" ) [ 1 ] . strip () value_false = expression . split ( \",\" ) [ 2 ] . strip (). strip ( \"]\" ) # if we don 't have '' this can break things results.append(\"' \" + value_true.strip(\" '\") + \"' \") results.append(\" '\" + value_false.strip(\"' \") + \" '\" ) return results evaluate_fn_join def evaluate_fn_join ( expression ) Return the joined stuff View Source @staticmethod def evaluate_fn_join ( expression ) : \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" ) [ 1 ] delimiter = temp . split ( \",\" ) [ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" ) [ 2 ] values = values . split ( \"]]\" ) [ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results evaluate_fn_split def evaluate_fn_split ( expression ) View Source @staticmethod def evaluate_fn_split ( expression ) : raise Exception ( \"Fn::Split: not supported\" ) rewrite_sub_vars_with_values def rewrite_sub_vars_with_values ( expression , values ) Rewrite sub vars with actual variable values View Source @staticmethod def rewrite_sub_vars_with_values ( expression , values ) : \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ] ) + \"\" result = result . replace ( rep_text , rep_with ) return result values_to_dict def values_to_dict ( values ) Rewrite sub vars with actual variable values View Source @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict Methods evaluate_expression_controller def evaluate_expression_controller ( self , expression ) Figure out what type of expression and pass off to handler View Source def evaluate_expression_controller ( self , expression ) : \"\"\" Figure out what type of expression and pass off to handler \"\"\" results = [] if \" Fn::If \" in expression : results = self . evaluate_fn_if ( expression ) elif \" Fn::Sub \" in expression : results = self . evaluate_fn_sub ( expression ) elif \" Fn::Join \" in expression : results = self . evaluate_fn_join ( expression ) elif \" Ref \" in expression : results = self . evaluate_fn_ref ( expression ) elif \" Fn::FindInMap \" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \" Fn::GetAtt \" in expression : results = self . evaluate_fn_getatt ( expression ) elif \" Fn::Split \" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \" ( \" + expression + \" ) \" ) return results evaluate_fn_findinmap def evaluate_fn_findinmap ( self , expression ) View Source def evaluate_fn_findinmap ( self , expression ) : result = [] mappings_map = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 0 ]. strip () first_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 1 ]. strip () final_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 2 ]. strip () result . append ( \" ' \" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \" ' \" ) return result evaluate_fn_ref def evaluate_fn_ref ( self , expression ) Since this is runtime data the best we can do is the name in place View Source def evaluate_fn_ref ( self , expression ) : \"\"\" Since this is runtime data the best we can do is the name in place \"\"\" # TODO : Allow user to inject RunTime values for these results = [] temp = expression . split ( \" : \" ) [ 1 ] # pylint : disable = consider - iterating - dictionary if temp . strip ( \" ' \" ) in self . SUBSTITUTION . keys () : temp = self . SUBSTITUTION [ temp . strip ( \" ' \" ) ] temp = \" ' \" + temp + \" ' \" results . append ( temp ) return results evaluate_fn_sub def evaluate_fn_sub ( self , expression ) Return expression with values replaced View Source def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results evaluate_string def evaluate_string ( self , template_url , depth = 0 ) Recursively find expressions in the URL and send them to be evaluated View Source def evaluate_string ( self , template_url , depth = 0 ) : \"\"\" Recursively find expressions in the URL and send them to be evaluated \"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \" Template URL contains more than {} levels or nesting \" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \" { \" in template_url : parts = template_url . split ( \" { \" ) parts = parts [ - 1 ]. split ( \" } \" ) # Last open bracket # This function will handle Fn :: Sub Fn :: If etc . replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \" { \" + parts [ 0 ] + \" } \" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls find_in_map_lookup def find_in_map_lookup ( self , mappings_map , first_key , final_key ) View Source def find_in_map_lookup ( self , mappings_map , first_key , final_key ) : step1 = self . mappings [ mappings_map . strip ( \" ' \" ) ] step2 = step1 [ first_key . strip ( \" ' \" ) ] result = step2 [ final_key . strip ( \" ' \" ) ] return result find_local_child_template def find_local_child_template ( self , parent_template_path , child_template_path ) View Source def find_local_child_template ( self , parent_template_path , child_template_path ) : final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \" /../ \" ) ) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) return \"\" flatten_template_url def flatten_template_url ( self , template_url ) Flatten template_url and return all permutations View Source def flatten_template_url ( self , template_url ) : \"\"\" Flatten template_url and return all permutations \"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO : figure where the ' is coming from output = urlparse ( str ( url . strip ( \" ' \" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print ( url_list ) # print ( path_list ) return path_list rewrite_sub_vars def rewrite_sub_vars ( self , original_string , depth = 1 ) Replace the '##var##' placeholders with 'var' View Source def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result rewrite_vars def rewrite_vars ( self , original_string , depth = 1 ) Replace the ${var} placeholders with ##var## View Source def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result template_url_to_path def template_url_to_path ( self , current_template_path , template_url ) View Source def template_url_to_path ( self , current_template_path , template_url , ) : child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO : Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"Stack Url Helper"},{"location":"reference/taskcat/_cfn/stack_url_helper/#module-taskcat_cfnstack_url_helper","text":"Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. View Source \"\"\" Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \"\"\" import json import logging import os from pathlib import Path from urllib.parse import urlparse LOG = logging . getLogger ( __name__ ) class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @staticmethod def evaluate_fn_join ( expression ): \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @staticmethod def evaluate_fn_if ( expression ): \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] # pylint: disable=consider-iterating-dictionary if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\"Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"Module taskcat._cfn.stack_url_helper"},{"location":"reference/taskcat/_cfn/stack_url_helper/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/stack_url_helper/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/stack_url_helper/#stackurlhelper","text":"class StackURLHelper ( template_mappings = None , template_parameters = None , parameter_values = None ) View Source class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @ staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @ staticmethod def evaluate_fn_join ( expression ): \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @ staticmethod def evaluate_fn_if ( expression ): \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] # pylint: disable=consider-iterating-dictionary if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @ staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @ staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\"Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @ staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"StackURLHelper"},{"location":"reference/taskcat/_cfn/stack_url_helper/#class-variables","text":"MAX_DEPTH SUBSTITUTION","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack_url_helper/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_getatt","text":"def evaluate_fn_getatt ( expression ) View Source @staticmethod def evaluate_fn_getatt ( expression ) : raise Exception ( \"Fn::GetAtt: not supported\" )","title":"evaluate_fn_getatt"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_if","text":"def evaluate_fn_if ( expression ) Return both possible parts of the expression View Source @staticmethod def evaluate_fn_if ( expression ) : \"\"\"Return both possible parts of the expression\"\"\" results = [] value_true = expression . split ( \",\" ) [ 1 ] . strip () value_false = expression . split ( \",\" ) [ 2 ] . strip (). strip ( \"]\" ) # if we don 't have '' this can break things results.append(\"' \" + value_true.strip(\" '\") + \"' \") results.append(\" '\" + value_false.strip(\"' \") + \" '\" ) return results","title":"evaluate_fn_if"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_join","text":"def evaluate_fn_join ( expression ) Return the joined stuff View Source @staticmethod def evaluate_fn_join ( expression ) : \"\"\"Return the joined stuff\"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" ) [ 1 ] delimiter = temp . split ( \",\" ) [ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" ) [ 2 ] values = values . split ( \"]]\" ) [ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results","title":"evaluate_fn_join"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_split","text":"def evaluate_fn_split ( expression ) View Source @staticmethod def evaluate_fn_split ( expression ) : raise Exception ( \"Fn::Split: not supported\" )","title":"evaluate_fn_split"},{"location":"reference/taskcat/_cfn/stack_url_helper/#rewrite_sub_vars_with_values","text":"def rewrite_sub_vars_with_values ( expression , values ) Rewrite sub vars with actual variable values View Source @staticmethod def rewrite_sub_vars_with_values ( expression , values ) : \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ] ) + \"\" result = result . replace ( rep_text , rep_with ) return result","title":"rewrite_sub_vars_with_values"},{"location":"reference/taskcat/_cfn/stack_url_helper/#values_to_dict","text":"def values_to_dict ( values ) Rewrite sub vars with actual variable values View Source @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict","title":"values_to_dict"},{"location":"reference/taskcat/_cfn/stack_url_helper/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_expression_controller","text":"def evaluate_expression_controller ( self , expression ) Figure out what type of expression and pass off to handler View Source def evaluate_expression_controller ( self , expression ) : \"\"\" Figure out what type of expression and pass off to handler \"\"\" results = [] if \" Fn::If \" in expression : results = self . evaluate_fn_if ( expression ) elif \" Fn::Sub \" in expression : results = self . evaluate_fn_sub ( expression ) elif \" Fn::Join \" in expression : results = self . evaluate_fn_join ( expression ) elif \" Ref \" in expression : results = self . evaluate_fn_ref ( expression ) elif \" Fn::FindInMap \" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \" Fn::GetAtt \" in expression : results = self . evaluate_fn_getatt ( expression ) elif \" Fn::Split \" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \" ( \" + expression + \" ) \" ) return results","title":"evaluate_expression_controller"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_findinmap","text":"def evaluate_fn_findinmap ( self , expression ) View Source def evaluate_fn_findinmap ( self , expression ) : result = [] mappings_map = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 0 ]. strip () first_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 1 ]. strip () final_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 2 ]. strip () result . append ( \" ' \" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \" ' \" ) return result","title":"evaluate_fn_findinmap"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_ref","text":"def evaluate_fn_ref ( self , expression ) Since this is runtime data the best we can do is the name in place View Source def evaluate_fn_ref ( self , expression ) : \"\"\" Since this is runtime data the best we can do is the name in place \"\"\" # TODO : Allow user to inject RunTime values for these results = [] temp = expression . split ( \" : \" ) [ 1 ] # pylint : disable = consider - iterating - dictionary if temp . strip ( \" ' \" ) in self . SUBSTITUTION . keys () : temp = self . SUBSTITUTION [ temp . strip ( \" ' \" ) ] temp = \" ' \" + temp + \" ' \" results . append ( temp ) return results","title":"evaluate_fn_ref"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_fn_sub","text":"def evaluate_fn_sub ( self , expression ) Return expression with values replaced View Source def evaluate_fn_sub ( self , expression ): \"\"\"Return expression with values replaced\"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results","title":"evaluate_fn_sub"},{"location":"reference/taskcat/_cfn/stack_url_helper/#evaluate_string","text":"def evaluate_string ( self , template_url , depth = 0 ) Recursively find expressions in the URL and send them to be evaluated View Source def evaluate_string ( self , template_url , depth = 0 ) : \"\"\" Recursively find expressions in the URL and send them to be evaluated \"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \" Template URL contains more than {} levels or nesting \" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \" { \" in template_url : parts = template_url . split ( \" { \" ) parts = parts [ - 1 ]. split ( \" } \" ) # Last open bracket # This function will handle Fn :: Sub Fn :: If etc . replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \" { \" + parts [ 0 ] + \" } \" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls","title":"evaluate_string"},{"location":"reference/taskcat/_cfn/stack_url_helper/#find_in_map_lookup","text":"def find_in_map_lookup ( self , mappings_map , first_key , final_key ) View Source def find_in_map_lookup ( self , mappings_map , first_key , final_key ) : step1 = self . mappings [ mappings_map . strip ( \" ' \" ) ] step2 = step1 [ first_key . strip ( \" ' \" ) ] result = step2 [ final_key . strip ( \" ' \" ) ] return result","title":"find_in_map_lookup"},{"location":"reference/taskcat/_cfn/stack_url_helper/#find_local_child_template","text":"def find_local_child_template ( self , parent_template_path , child_template_path ) View Source def find_local_child_template ( self , parent_template_path , child_template_path ) : final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \" /../ \" ) ) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) return \"\"","title":"find_local_child_template"},{"location":"reference/taskcat/_cfn/stack_url_helper/#flatten_template_url","text":"def flatten_template_url ( self , template_url ) Flatten template_url and return all permutations View Source def flatten_template_url ( self , template_url ) : \"\"\" Flatten template_url and return all permutations \"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO : figure where the ' is coming from output = urlparse ( str ( url . strip ( \" ' \" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print ( url_list ) # print ( path_list ) return path_list","title":"flatten_template_url"},{"location":"reference/taskcat/_cfn/stack_url_helper/#rewrite_sub_vars","text":"def rewrite_sub_vars ( self , original_string , depth = 1 ) Replace the '##var##' placeholders with 'var' View Source def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result","title":"rewrite_sub_vars"},{"location":"reference/taskcat/_cfn/stack_url_helper/#rewrite_vars","text":"def rewrite_vars ( self , original_string , depth = 1 ) Replace the ${var} placeholders with ##var## View Source def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result","title":"rewrite_vars"},{"location":"reference/taskcat/_cfn/stack_url_helper/#template_url_to_path","text":"def template_url_to_path ( self , current_template_path , template_url ) View Source def template_url_to_path ( self , current_template_path , template_url , ) : child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO : Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"template_url_to_path"},{"location":"reference/taskcat/_cfn/template/","text":"Module taskcat._cfn.template None None View Source import logging import re from pathlib import Path from time import sleep from typing import Dict , List , Union from yaml . scanner import ScannerError import cfnlint from taskcat . _ cfn . stack_url_helper import StackURLHelper from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class TemplateCache : def __ init__ ( self , store : dict = None ) : self . _ templates = store if store else {} self . _ lock : Dict [ str , bool ] = {} def get ( self , template_path: str ) -> cfnlint . Template : while self . _ lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _ templates : try : self . _ lock [ template_path ] = True try : self . _ templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _ lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _ lock [ template_path ] = False raise return self . _ templates [ template_path ] template_cache_store: Dict [ str , cfnlint . Template ] = {} tcat_template_cache = TemplateCache ( template_cache_store ) # pylint : disable = C0103 class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Variables LOG tcat_template_cache template_cache_store Classes Template class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x7fea8881f280 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Instance variables descendents linesplit s3_key s3_key_prefix Methods parameters def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ str , int ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters url_prefix def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix write def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding = \"utf-8\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children () TemplateCache class TemplateCache ( store : dict = None ) View Source class TemplateCache : def __init__ ( self , store : dict = None ) : self . _templates = store if store else {} self . _lock : Dict [ str, bool ] = {} def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ] Methods get def get ( self , template_path : str ) -> cfnlint . decorators . refactored . refactored .< locals >. cls_wrapper .< locals >. Wrapped View Source def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"Template"},{"location":"reference/taskcat/_cfn/template/#module-taskcat_cfntemplate","text":"None None View Source import logging import re from pathlib import Path from time import sleep from typing import Dict , List , Union from yaml . scanner import ScannerError import cfnlint from taskcat . _ cfn . stack_url_helper import StackURLHelper from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class TemplateCache : def __ init__ ( self , store : dict = None ) : self . _ templates = store if store else {} self . _ lock : Dict [ str , bool ] = {} def get ( self , template_path: str ) -> cfnlint . Template : while self . _ lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _ templates : try : self . _ lock [ template_path ] = True try : self . _ templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _ lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _ lock [ template_path ] = False raise return self . _ templates [ template_path ] template_cache_store: Dict [ str , cfnlint . Template ] = {} tcat_template_cache = TemplateCache ( template_cache_store ) # pylint : disable = C0103 class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Module taskcat._cfn.template"},{"location":"reference/taskcat/_cfn/template/#variables","text":"LOG tcat_template_cache template_cache_store","title":"Variables"},{"location":"reference/taskcat/_cfn/template/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/template/#template","text":"class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x7fea8881f280 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" , encoding= \"utf-8\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding= \"utf-8\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Template"},{"location":"reference/taskcat/_cfn/template/#instance-variables","text":"descendents linesplit s3_key s3_key_prefix","title":"Instance variables"},{"location":"reference/taskcat/_cfn/template/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/template/#parameters","text":"def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ str , int ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters","title":"parameters"},{"location":"reference/taskcat/_cfn/template/#url_prefix","text":"def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix","title":"url_prefix"},{"location":"reference/taskcat/_cfn/template/#write","text":"def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" , encoding = \"utf-8\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"write"},{"location":"reference/taskcat/_cfn/template/#templatecache","text":"class TemplateCache ( store : dict = None ) View Source class TemplateCache : def __init__ ( self , store : dict = None ) : self . _templates = store if store else {} self . _lock : Dict [ str, bool ] = {} def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"TemplateCache"},{"location":"reference/taskcat/_cfn/template/#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/template/#get","text":"def get ( self , template_path : str ) -> cfnlint . decorators . refactored . refactored .< locals >. cls_wrapper .< locals >. Wrapped View Source def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"get"},{"location":"reference/taskcat/_cfn/threaded/","text":"Module taskcat._cfn.threaded None None View Source import logging import uuid from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List import boto3 from taskcat._cfn.stack import Stack , Stacks , StackStatus from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_dicts from taskcat._dataclasses import Tag , TestObj , TestRegion from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str , TestObj ], uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ): self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str , TestObj ]): return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ): if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ({ \"Key\" : \"taskcat-id\" , \"Value\" : self . uid . hex })] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \"taskcat-project-name\" , \"taskcat-test-name\" , \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags }, tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ): stack_name = test . stack_name tags . append ( Tag ({ \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name })) tags . append ( Tag ({ \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name })) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ): raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ): if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ): fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _delete_stack ( stack : Stack ): stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results : for status in region : statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ): return fan_out ( self . _status , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _status ( stack : Stack ): for status_group in [ \"COMPLETE\" , \"IN_PROGRESS\" , \"FAILED\" ]: if stack . status in getattr ( StackStatus , status_group ): return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack { stack } \" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ): return { stack . id : stack . events () . filter ( criteria )} def resources ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs }, self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _resources , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ): return { stack . id : stack . resources () . filter ( criteria )} @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str , TestObj ], include_deleted = False , recurse = False , threads = 32 , ): if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3 . client , List [ TestRegion ]] = {} for test in tests . values (): for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests }, clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ([ item for sublist in results for item in sublist ]) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ): # pylint: disable=too-many-locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ], test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ]: stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client [ r ] for r in stacks_by_client # pylint: disable=consider-using-dict-items ] @staticmethod def list_stacks ( profiles , regions ): stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache ()}, profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ): stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile }, regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ): stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" )[ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k . startswith ( \"taskcat-\" ): stack [ k ] = v if stack . get ( \"taskcat-id\" ): stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ]) stacks . append ( stack ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to fetch stacks for region { region } using profile \" f \" { profile } { type ( e ) } { e } \" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks Variables LOG Functions fan_out def fan_out ( func , partial_kwargs , payload , threads ) View Source def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results Classes Stacker class Stacker ( project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], uid : uuid . UUID = UUID ( '00000000-0000-0000-0000-000000000000' ), stack_name_prefix : str = 'tCaT' , shorten_stack_name : bool = False , tags : list = None ) View Source class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str, TestObj ] , uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ) : self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str, TestObj ] ) : return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag({\"Key\": \"taskcat-id\", \"Value\": self.uid.hex}) ] tags += [ Tag(t) for t in self.tags if t.key not in [\"taskcat-project-name\", \"taskcat-test-name\", \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags } , tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ) : stack_name = test . stack_name tags . append ( Tag ( { \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name } )) tags . append ( Tag ( { \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name } )) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ) : raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ) : fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _delete_stack ( stack : Stack ) : stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str, dict ] = { \"IN_PROGRESS\" : {} , \"COMPLETE\" : {} , \"FAILED\" : {}} for region in results : for status in region : statuses [ status[1 ] ] [ status[0 ] ] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ) : return fan_out ( self . _status , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _status ( stack : Stack ) : for status_group in [ \"COMPLETE\", \"IN_PROGRESS\", \"FAILED\" ] : if stack . status in getattr ( StackStatus , status_group ) : return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack {stack}\" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs } , per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ) : return { stack . id : stack . events (). filter ( criteria ) } def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs } , self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _resources , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ) : return { stack . id : stack . resources (). filter ( criteria ) } @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ) : # pylint : disable = too - many - locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ] , test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ] : stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client[r ] for r in stacks_by_client # pylint : disable = consider - using - dict - items ] @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ) : stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile } , regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ) : stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" ) [ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k . startswith ( \"taskcat-\" ) : stack [ k ] = v if stack . get ( \"taskcat-id\" ) : stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ] ) stacks . append ( stack ) except Exception as e : # pylint : disable = broad - except LOG . warning ( f \"Failed to fetch stacks for region {region} using profile \" f \"{profile} {type(e)} {e}\" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks Class variables NULL_UUID Static methods from_existing def from_existing ( uid : uuid . UUID , project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], include_deleted = False , recurse = False , threads = 32 ) View Source @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker list_stacks def list_stacks ( profiles , regions ) View Source @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] Methods create_stacks def create_stacks ( self , threads : int = 8 ) View Source def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \" Stacker already initialised with stack objects \" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ( { \" Key \" : \" taskcat-id \" , \" Value \" : self . uid . hex } ) ] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \" taskcat-project-name \" , \" taskcat-test-name \" , \" taskcat-id \" ] ] fan_out ( self . _create_stacks_for_test , { \" tags \" : tags }, tests , threads ) delete_stacks def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) View Source def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \" deep delete not yet implemented \" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )) , threads , ) events def events ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def events ( self , recurse = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) resources def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \" recurse not implemented \" ) results = fan_out ( self . _resources_per_client , { \" criteria \" : kwargs }, self . _group_stacks ( self . stacks ) , threads , ) return merge_dicts ( results ) status def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) View Source def status ( self , recurse: bool = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses: Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results: for status in region: statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses update_stacks def update_stacks ( self ) View Source def update_stacks(self): raise NotImplementedError()","title":"Threaded"},{"location":"reference/taskcat/_cfn/threaded/#module-taskcat_cfnthreaded","text":"None None View Source import logging import uuid from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List import boto3 from taskcat._cfn.stack import Stack , Stacks , StackStatus from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_dicts from taskcat._dataclasses import Tag , TestObj , TestRegion from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str , TestObj ], uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ): self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str , TestObj ]): return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ): if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ({ \"Key\" : \"taskcat-id\" , \"Value\" : self . uid . hex })] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \"taskcat-project-name\" , \"taskcat-test-name\" , \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags }, tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ): stack_name = test . stack_name tags . append ( Tag ({ \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name })) tags . append ( Tag ({ \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name })) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ): raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ): if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ): fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _delete_stack ( stack : Stack ): stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results : for status in region : statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ): return fan_out ( self . _status , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _status ( stack : Stack ): for status_group in [ \"COMPLETE\" , \"IN_PROGRESS\" , \"FAILED\" ]: if stack . status in getattr ( StackStatus , status_group ): return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack { stack } \" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ): return { stack . id : stack . events () . filter ( criteria )} def resources ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs }, self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _resources , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ): return { stack . id : stack . resources () . filter ( criteria )} @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str , TestObj ], include_deleted = False , recurse = False , threads = 32 , ): if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3 . client , List [ TestRegion ]] = {} for test in tests . values (): for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests }, clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ([ item for sublist in results for item in sublist ]) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ): # pylint: disable=too-many-locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ], test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ]: stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client [ r ] for r in stacks_by_client # pylint: disable=consider-using-dict-items ] @staticmethod def list_stacks ( profiles , regions ): stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache ()}, profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ): stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile }, regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ): stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" )[ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k . startswith ( \"taskcat-\" ): stack [ k ] = v if stack . get ( \"taskcat-id\" ): stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ]) stacks . append ( stack ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to fetch stacks for region { region } using profile \" f \" { profile } { type ( e ) } { e } \" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks","title":"Module taskcat._cfn.threaded"},{"location":"reference/taskcat/_cfn/threaded/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/threaded/#functions","text":"","title":"Functions"},{"location":"reference/taskcat/_cfn/threaded/#fan_out","text":"def fan_out ( func , partial_kwargs , payload , threads ) View Source def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results","title":"fan_out"},{"location":"reference/taskcat/_cfn/threaded/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/threaded/#stacker","text":"class Stacker ( project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], uid : uuid . UUID = UUID ( '00000000-0000-0000-0000-000000000000' ), stack_name_prefix : str = 'tCaT' , shorten_stack_name : bool = False , tags : list = None ) View Source class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str, TestObj ] , uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ) : self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str, TestObj ] ) : return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag({\"Key\": \"taskcat-id\", \"Value\": self.uid.hex}) ] tags += [ Tag(t) for t in self.tags if t.key not in [\"taskcat-project-name\", \"taskcat-test-name\", \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags } , tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ) : stack_name = test . stack_name tags . append ( Tag ( { \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name } )) tags . append ( Tag ( { \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name } )) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ) : raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ) : fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _delete_stack ( stack : Stack ) : stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str, dict ] = { \"IN_PROGRESS\" : {} , \"COMPLETE\" : {} , \"FAILED\" : {}} for region in results : for status in region : statuses [ status[1 ] ] [ status[0 ] ] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ) : return fan_out ( self . _status , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _status ( stack : Stack ) : for status_group in [ \"COMPLETE\", \"IN_PROGRESS\", \"FAILED\" ] : if stack . status in getattr ( StackStatus , status_group ) : return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack {stack}\" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs } , per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ) : return { stack . id : stack . events (). filter ( criteria ) } def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs } , self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _resources , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ) : return { stack . id : stack . resources (). filter ( criteria ) } @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ) : # pylint : disable = too - many - locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ] , test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ] : stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client[r ] for r in stacks_by_client # pylint : disable = consider - using - dict - items ] @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ) : stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile } , regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ) : stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" ) [ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k . startswith ( \"taskcat-\" ) : stack [ k ] = v if stack . get ( \"taskcat-id\" ) : stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ] ) stacks . append ( stack ) except Exception as e : # pylint : disable = broad - except LOG . warning ( f \"Failed to fetch stacks for region {region} using profile \" f \"{profile} {type(e)} {e}\" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks","title":"Stacker"},{"location":"reference/taskcat/_cfn/threaded/#class-variables","text":"NULL_UUID","title":"Class variables"},{"location":"reference/taskcat/_cfn/threaded/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/threaded/#from_existing","text":"def from_existing ( uid : uuid . UUID , project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], include_deleted = False , recurse = False , threads = 32 ) View Source @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker","title":"from_existing"},{"location":"reference/taskcat/_cfn/threaded/#list_stacks","text":"def list_stacks ( profiles , regions ) View Source @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ]","title":"list_stacks"},{"location":"reference/taskcat/_cfn/threaded/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/threaded/#create_stacks","text":"def create_stacks ( self , threads : int = 8 ) View Source def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \" Stacker already initialised with stack objects \" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ( { \" Key \" : \" taskcat-id \" , \" Value \" : self . uid . hex } ) ] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \" taskcat-project-name \" , \" taskcat-test-name \" , \" taskcat-id \" ] ] fan_out ( self . _create_stacks_for_test , { \" tags \" : tags }, tests , threads )","title":"create_stacks"},{"location":"reference/taskcat/_cfn/threaded/#delete_stacks","text":"def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) View Source def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \" deep delete not yet implemented \" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )) , threads , )","title":"delete_stacks"},{"location":"reference/taskcat/_cfn/threaded/#events","text":"def events ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def events ( self , recurse = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results )","title":"events"},{"location":"reference/taskcat/_cfn/threaded/#resources","text":"def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \" recurse not implemented \" ) results = fan_out ( self . _resources_per_client , { \" criteria \" : kwargs }, self . _group_stacks ( self . stacks ) , threads , ) return merge_dicts ( results )","title":"resources"},{"location":"reference/taskcat/_cfn/threaded/#status","text":"def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) View Source def status ( self , recurse: bool = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses: Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results: for status in region: statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses","title":"status"},{"location":"reference/taskcat/_cfn/threaded/#update_stacks","text":"def update_stacks ( self ) View Source def update_stacks(self): raise NotImplementedError()","title":"update_stacks"},{"location":"reference/taskcat/_cli_modules/","text":"Module taskcat._cli_modules None None View Source from .config import ConvertConfig # noqa: F401 from .delete import Delete # noqa: F401 from .deploy import Deploy # noqa: F401 from .generate_iam_policy import GenerateIAMPolicy # noqa: F401 from .lint import Lint # noqa: F401 from .list import List # noqa: F401 from .package import Package # noqa: F401 from .test import Test # noqa: F401 from .update_ami import UpdateAMI # noqa: F401 from .upload import Upload # noqa: F401 Sub-modules taskcat._cli_modules.config taskcat._cli_modules.delete taskcat._cli_modules.deploy taskcat._cli_modules.generate_iam_policy taskcat._cli_modules.lint taskcat._cli_modules.list taskcat._cli_modules.package taskcat._cli_modules.test taskcat._cli_modules.update_ami taskcat._cli_modules.upload","title":"Index"},{"location":"reference/taskcat/_cli_modules/#module-taskcat_cli_modules","text":"None None View Source from .config import ConvertConfig # noqa: F401 from .delete import Delete # noqa: F401 from .deploy import Deploy # noqa: F401 from .generate_iam_policy import GenerateIAMPolicy # noqa: F401 from .lint import Lint # noqa: F401 from .list import List # noqa: F401 from .package import Package # noqa: F401 from .test import Test # noqa: F401 from .update_ami import UpdateAMI # noqa: F401 from .upload import Upload # noqa: F401","title":"Module taskcat._cli_modules"},{"location":"reference/taskcat/_cli_modules/#sub-modules","text":"taskcat._cli_modules.config taskcat._cli_modules.delete taskcat._cli_modules.deploy taskcat._cli_modules.generate_iam_policy taskcat._cli_modules.lint taskcat._cli_modules.list taskcat._cli_modules.package taskcat._cli_modules.test taskcat._cli_modules.update_ami taskcat._cli_modules.upload","title":"Sub-modules"},{"location":"reference/taskcat/_cli_modules/config/","text":"Module taskcat._cli_modules.config None None View Source import logging from pathlib import Path from taskcat._legacy_config import parse_legacy_config LOG = logging . getLogger ( __name__ ) class ConvertConfig : \"\"\" Mutating actions regarding the config file \"\"\" @staticmethod def convert ( project_root : str = \"./\" , ): # pylint: disable=too-many-locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () parse_legacy_config ( project_root_path ) Variables LOG Classes ConvertConfig class ConvertConfig ( / , * args , ** kwargs ) View Source class ConvertConfig : \"\"\" Mutating actions regarding the config file \"\"\" @staticmethod def convert ( project_root : str = \"./\" , ) : # pylint : disable = too - many - locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ). expanduser (). resolve () parse_legacy_config ( project_root_path ) Static methods convert def convert ( project_root : str = './' ) Converts config from legacy to new format. View Source @staticmethod def convert ( project_root : str = \"./\" , ) : # pylint : disable = too - many - locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ). expanduser (). resolve () parse_legacy_config ( project_root_path )","title":"Config"},{"location":"reference/taskcat/_cli_modules/config/#module-taskcat_cli_modulesconfig","text":"None None View Source import logging from pathlib import Path from taskcat._legacy_config import parse_legacy_config LOG = logging . getLogger ( __name__ ) class ConvertConfig : \"\"\" Mutating actions regarding the config file \"\"\" @staticmethod def convert ( project_root : str = \"./\" , ): # pylint: disable=too-many-locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () parse_legacy_config ( project_root_path )","title":"Module taskcat._cli_modules.config"},{"location":"reference/taskcat/_cli_modules/config/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/config/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/config/#convertconfig","text":"class ConvertConfig ( / , * args , ** kwargs ) View Source class ConvertConfig : \"\"\" Mutating actions regarding the config file \"\"\" @staticmethod def convert ( project_root : str = \"./\" , ) : # pylint : disable = too - many - locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ). expanduser (). resolve () parse_legacy_config ( project_root_path )","title":"ConvertConfig"},{"location":"reference/taskcat/_cli_modules/config/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cli_modules/config/#convert","text":"def convert ( project_root : str = './' ) Converts config from legacy to new format. View Source @staticmethod def convert ( project_root : str = \"./\" , ) : # pylint : disable = too - many - locals \"\"\"Converts config from legacy to new format.\"\"\" project_root_path : Path = Path ( project_root ). expanduser (). resolve () parse_legacy_config ( project_root_path )","title":"convert"},{"location":"reference/taskcat/_cli_modules/delete/","text":"Module taskcat._cli_modules.delete None None View Source # pylint: disable=duplicate-code import logging import sys from concurrent.futures import ThreadPoolExecutor , as_completed import boto3 from taskcat._cfn.stack import Stack from taskcat._cfn.threaded import Stacker from taskcat._client_factory import Boto3Cache from taskcat.regions_to_partitions import REGIONS LOG = logging . getLogger ( __name__ ) class Delete : \"\"\"[ALPHA] Deletes an installed project in an AWS account/region\"\"\" # pylint: disable=too-many-locals def __init__ ( self , project : str , aws_profile : str = \"default\" , region = \"ALL\" , no_verify : bool = False , stack_type : str = \"ALL\" , ): \"\"\" :param project: installed project to delete, can be an install name, uuid, or project name :param aws_profile: aws profile to use for deletion :param region: region(s) to delete from, by default, will delete all applicable\\ stacks, supply a csv \"us-east-1,us-west-1\" to override this default :param no_verify: ignore region verification, delete will not error if an invalid\\ region is detected :param stack_type: type of stacks to delete, allowable options are [\"project\",\"test\",\"ALL\"] \"\"\" boto3_cache = Boto3Cache () if region == \"default\" : regions = boto3_cache . get_default_region ( aws_profile ) elif region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint: disable=duplicate-code set ( boto3 . Session ( profile_name = aws_profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) elif isinstance ( region , str ): regions = ( self . _validate_regions ( region ) if not no_verify else region . split ( \",\" ) ) stacks = Stacker . list_stacks ([ aws_profile ], regions ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ]) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ], \"test_name\" : stack [ \"taskcat-test-name\" ], \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ], \"stack_id\" : stack [ \"stack-id\" ], } if stack_type in [ \"project\" , \"ALL\" ] and project in [ job [ \"name\" ], job [ \"taskcat_id\" ], \"ALL\" , ]: jobs . append ( job ) if stack_type in [ \"test\" , \"ALL\" ] and project in [ job [ \"project_name\" ], \"ALL\" , ]: jobs . append ( job ) with ThreadPoolExecutor () as executor : stack_futures = { executor . submit ( self . _delete_stack , boto3_cache = boto3_cache , job = job , aws_profile = aws_profile , ): [ job [ \"name\" ], job [ \"region\" ]] for job in jobs } for stack_future in as_completed ( stack_futures ): name_and_region = stack_futures [ stack_future ] try : stack_future . result () # pylint: disable=broad-except except Exception : LOG . error ( f \" { name_and_region [ 0 ] } failed in { name_and_region [ 1 ] } \" ) else : LOG . info ( f \" { name_and_region [ 0 ] } deleted in { name_and_region [ 1 ] } \" ) @staticmethod def _delete_stack ( boto3_cache , job , aws_profile ): client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ]) # Checks if all regions are valid @staticmethod def _validate_regions ( region_string ): regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: { region } \" ) sys . exit ( 1 ) return regions Variables LOG REGIONS Classes Delete class Delete ( project : str , aws_profile : str = 'default' , region = 'ALL' , no_verify : bool = False , stack_type : str = 'ALL' ) View Source class Delete : \"\"\"[ALPHA] Deletes an installed project in an AWS account/region\"\"\" # pylint : disable = too - many - locals def __init__ ( self , project : str , aws_profile : str = \"default\" , region = \"ALL\" , no_verify : bool = False , stack_type : str = \"ALL\" , ) : \"\"\" :param project: installed project to delete, can be an install name, uuid, or project name :param aws_profile: aws profile to use for deletion :param region: region(s) to delete from, by default, will delete all applicable\\ stacks, supply a csv \" us - east - 1 , us - west - 1 \" to override this default :param no_verify: ignore region verification, delete will not error if an invalid\\ region is detected :param stack_type: type of stacks to delete, allowable options are [\" project \",\" test \",\" ALL \"] \"\"\" boto3_cache = Boto3Cache () if region == \"default\" : regions = boto3_cache . get_default_region ( aws_profile ) elif region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) elif isinstance ( region , str ) : regions = ( self . _validate_regions ( region ) if not no_verify else region . split ( \",\" ) ) stacks = Stacker . list_stacks ( [ aws_profile ] , regions ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ] ) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"test_name\" : stack [ \"taskcat-test-name\" ] , \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ] , \"stack_id\" : stack [ \"stack-id\" ] , } if stack_type in [ \"project\", \"ALL\" ] and project in [ job[\"name\" ] , job [ \"taskcat_id\" ] , \"ALL\" , ]: jobs . append ( job ) if stack_type in [ \"test\", \"ALL\" ] and project in [ job[\"project_name\" ] , \"ALL\" , ]: jobs . append ( job ) with ThreadPoolExecutor () as executor : stack_futures = { executor . submit ( self . _delete_stack , boto3_cache = boto3_cache , job = job , aws_profile = aws_profile , ) : [ job[\"name\" ] , job [ \"region\" ] ] for job in jobs } for stack_future in as_completed ( stack_futures ) : name_and_region = stack_futures [ stack_future ] try : stack_future . result () # pylint : disable = broad - except except Exception : LOG . error ( f \"{name_and_region[0]} failed in {name_and_region[1]}\" ) else : LOG . info ( f \"{name_and_region[0]} deleted in {name_and_region[1]}\" ) @staticmethod def _delete_stack ( boto3_cache , job , aws_profile ) : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ] ) # Checks if all regions are valid @staticmethod def _validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions","title":"Delete"},{"location":"reference/taskcat/_cli_modules/delete/#module-taskcat_cli_modulesdelete","text":"None None View Source # pylint: disable=duplicate-code import logging import sys from concurrent.futures import ThreadPoolExecutor , as_completed import boto3 from taskcat._cfn.stack import Stack from taskcat._cfn.threaded import Stacker from taskcat._client_factory import Boto3Cache from taskcat.regions_to_partitions import REGIONS LOG = logging . getLogger ( __name__ ) class Delete : \"\"\"[ALPHA] Deletes an installed project in an AWS account/region\"\"\" # pylint: disable=too-many-locals def __init__ ( self , project : str , aws_profile : str = \"default\" , region = \"ALL\" , no_verify : bool = False , stack_type : str = \"ALL\" , ): \"\"\" :param project: installed project to delete, can be an install name, uuid, or project name :param aws_profile: aws profile to use for deletion :param region: region(s) to delete from, by default, will delete all applicable\\ stacks, supply a csv \"us-east-1,us-west-1\" to override this default :param no_verify: ignore region verification, delete will not error if an invalid\\ region is detected :param stack_type: type of stacks to delete, allowable options are [\"project\",\"test\",\"ALL\"] \"\"\" boto3_cache = Boto3Cache () if region == \"default\" : regions = boto3_cache . get_default_region ( aws_profile ) elif region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint: disable=duplicate-code set ( boto3 . Session ( profile_name = aws_profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) elif isinstance ( region , str ): regions = ( self . _validate_regions ( region ) if not no_verify else region . split ( \",\" ) ) stacks = Stacker . list_stacks ([ aws_profile ], regions ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ]) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ], \"test_name\" : stack [ \"taskcat-test-name\" ], \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ], \"stack_id\" : stack [ \"stack-id\" ], } if stack_type in [ \"project\" , \"ALL\" ] and project in [ job [ \"name\" ], job [ \"taskcat_id\" ], \"ALL\" , ]: jobs . append ( job ) if stack_type in [ \"test\" , \"ALL\" ] and project in [ job [ \"project_name\" ], \"ALL\" , ]: jobs . append ( job ) with ThreadPoolExecutor () as executor : stack_futures = { executor . submit ( self . _delete_stack , boto3_cache = boto3_cache , job = job , aws_profile = aws_profile , ): [ job [ \"name\" ], job [ \"region\" ]] for job in jobs } for stack_future in as_completed ( stack_futures ): name_and_region = stack_futures [ stack_future ] try : stack_future . result () # pylint: disable=broad-except except Exception : LOG . error ( f \" { name_and_region [ 0 ] } failed in { name_and_region [ 1 ] } \" ) else : LOG . info ( f \" { name_and_region [ 0 ] } deleted in { name_and_region [ 1 ] } \" ) @staticmethod def _delete_stack ( boto3_cache , job , aws_profile ): client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ]) # Checks if all regions are valid @staticmethod def _validate_regions ( region_string ): regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: { region } \" ) sys . exit ( 1 ) return regions","title":"Module taskcat._cli_modules.delete"},{"location":"reference/taskcat/_cli_modules/delete/#variables","text":"LOG REGIONS","title":"Variables"},{"location":"reference/taskcat/_cli_modules/delete/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/delete/#delete","text":"class Delete ( project : str , aws_profile : str = 'default' , region = 'ALL' , no_verify : bool = False , stack_type : str = 'ALL' ) View Source class Delete : \"\"\"[ALPHA] Deletes an installed project in an AWS account/region\"\"\" # pylint : disable = too - many - locals def __init__ ( self , project : str , aws_profile : str = \"default\" , region = \"ALL\" , no_verify : bool = False , stack_type : str = \"ALL\" , ) : \"\"\" :param project: installed project to delete, can be an install name, uuid, or project name :param aws_profile: aws profile to use for deletion :param region: region(s) to delete from, by default, will delete all applicable\\ stacks, supply a csv \" us - east - 1 , us - west - 1 \" to override this default :param no_verify: ignore region verification, delete will not error if an invalid\\ region is detected :param stack_type: type of stacks to delete, allowable options are [\" project \",\" test \",\" ALL \"] \"\"\" boto3_cache = Boto3Cache () if region == \"default\" : regions = boto3_cache . get_default_region ( aws_profile ) elif region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) elif isinstance ( region , str ) : regions = ( self . _validate_regions ( region ) if not no_verify else region . split ( \",\" ) ) stacks = Stacker . list_stacks ( [ aws_profile ] , regions ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ] ) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"test_name\" : stack [ \"taskcat-test-name\" ] , \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ] , \"stack_id\" : stack [ \"stack-id\" ] , } if stack_type in [ \"project\", \"ALL\" ] and project in [ job[\"name\" ] , job [ \"taskcat_id\" ] , \"ALL\" , ]: jobs . append ( job ) if stack_type in [ \"test\", \"ALL\" ] and project in [ job[\"project_name\" ] , \"ALL\" , ]: jobs . append ( job ) with ThreadPoolExecutor () as executor : stack_futures = { executor . submit ( self . _delete_stack , boto3_cache = boto3_cache , job = job , aws_profile = aws_profile , ) : [ job[\"name\" ] , job [ \"region\" ] ] for job in jobs } for stack_future in as_completed ( stack_futures ) : name_and_region = stack_futures [ stack_future ] try : stack_future . result () # pylint : disable = broad - except except Exception : LOG . error ( f \"{name_and_region[0]} failed in {name_and_region[1]}\" ) else : LOG . info ( f \"{name_and_region[0]} deleted in {name_and_region[1]}\" ) @staticmethod def _delete_stack ( boto3_cache , job , aws_profile ) : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ] ) # Checks if all regions are valid @staticmethod def _validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions","title":"Delete"},{"location":"reference/taskcat/_cli_modules/deploy/","text":"Module taskcat._cli_modules.deploy None None View Source # pylint : disable = duplicate - code import logging import sys from io import BytesIO from pathlib import Path from dulwich import porcelain from dulwich . config import ConfigFile , parse_submodules from taskcat . _ cli_modules . test import Test from taskcat . _ dataclasses import Tag from taskcat . _ name_generator import generate_name from taskcat . regions_to_partitions import REGIONS from . list import List LOG = logging . getLogger ( __ name__ ) class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/regions\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def run ( # noqa : C901 self , project : str = \"./\" , test_names: str = \"ALL\" , regions : str = \"ALL\" , name= \"\" , input_file: str = \"./.taskcat.yml\" , ) : \"\"\" :param project: name of project to install can be a path to a local project,\\ a github org/repo, or an AWS Quick Start name :param test_names: comma separated list of tests (specified in .taskcat.yml) to run\\ defaults to the 'default' test. Set to 'ALL' to deploy every entry :param regions: comma separated list of regions to test in\\ default :param name: stack name to use, if not specified one will be automatically\\ generated :param input_file: path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" project = f\"aws-quickstart/quickstart-{project}\" if package_type == \"github\": if project.startswith(\"https://\") or project.startswith(\"git@\"): url = project org, repo = ( project.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = project.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) _extra_tags = [(Tag({\"Key\": \"taskcat-installer\", \"Value\": name}))] Test.run( regions=regions, no_delete=True, project_root=path, test_names=test_names, input_file=input_file, _extra_tags=_extra_tags, ) @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \"path already exists, updating from remote is not yet implemented\" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\"utf-8\")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \".gitmodules\" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\"utf-8\") url = url.decode(\"utf-8\") name = name.decode(\"utf-8\") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule ... sha = None try : porcelain . get_object_by_path ( str ( path ), sub_path ) except KeyError as e : sha = e . args [ 0 ]. decode ( \"utf-8\" ) if not sha : raise ValueError ( f \"Could not find sha for submodule {name}\" ) if url . startswith ( \"../\" ) : base_url = parent_url for _ in range ( url . count ( \"../\" )) : base_url = \"/\" . join ( base_url . split ( \"/\" )[:- 1 ]) url = base_url + \"/\" + url . replace ( \"../\" , \"\" ) outp = BytesIO () if not ( path / sub_path / \".git\" ). is_dir () : LOG . info ( f \"fetching git submodule {url}\" ) porcelain . clone ( url , str ( path / sub_path ), checkout = sha , errstream = outp , outstream = outp , ) LOG . debug ( outp . getvalue (). decode ( \"utf-8\" )) self . _ recurse_submodules (( path / sub_path ), url ) @staticmethod def list ( profiles : str = \"default\" , regions= \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type= \"project\" ) # Checks if all regions are valid @staticmethod def _ validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions Variables LOG REGIONS Classes Deploy class Deploy ( / , * args , ** kwargs ) View Source class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/regions\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def run ( # noqa : C901 self , project : str = \"./\" , test_names: str = \"ALL\" , regions : str = \"ALL\" , name= \"\" , input_file: str = \"./.taskcat.yml\" , ) : \"\"\" :param project: name of project to install can be a path to a local project,\\ a github org/repo, or an AWS Quick Start name :param test_names: comma separated list of tests (specified in .taskcat.yml) to run\\ defaults to the 'default' test. Set to 'ALL' to deploy every entry :param regions: comma separated list of regions to test in\\ default :param name: stack name to use, if not specified one will be automatically\\ generated :param input_file: path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" project = f\"aws-quickstart/quickstart-{project}\" if package_type == \"github\": if project.startswith(\"https://\") or project.startswith(\"git@\"): url = project org, repo = ( project.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = project.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) _extra_tags = [(Tag({\"Key\": \"taskcat-installer\", \"Value\": name}))] Test.run( regions=regions, no_delete=True, project_root=path, test_names=test_names, input_file=input_file, _extra_tags=_extra_tags, ) @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \"path already exists, updating from remote is not yet implemented\" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\"utf-8\")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \".gitmodules\" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\"utf-8\") url = url.decode(\"utf-8\") name = name.decode(\"utf-8\") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule ... sha = None try : porcelain . get_object_by_path ( str ( path ), sub_path ) except KeyError as e : sha = e . args [ 0 ]. decode ( \"utf-8\" ) if not sha : raise ValueError ( f \"Could not find sha for submodule {name}\" ) if url . startswith ( \"../\" ) : base_url = parent_url for _ in range ( url . count ( \"../\" )) : base_url = \"/\" . join ( base_url . split ( \"/\" )[:- 1 ]) url = base_url + \"/\" + url . replace ( \"../\" , \"\" ) outp = BytesIO () if not ( path / sub_path / \".git\" ). is_dir () : LOG . info ( f \"fetching git submodule {url}\" ) porcelain . clone ( url , str ( path / sub_path ), checkout = sha , errstream = outp , outstream = outp , ) LOG . debug ( outp . getvalue (). decode ( \"utf-8\" )) self . _ recurse_submodules (( path / sub_path ), url ) @staticmethod def list ( profiles : str = \"default\" , regions= \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type= \"project\" ) # Checks if all regions are valid @staticmethod def _ validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions Class variables PKG_CACHE_PATH Static methods list def list ( profiles : str = 'default' , regions = 'ALL' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"project\" ) Methods run def run ( self , project : str = './' , test_names : str = 'ALL' , regions : str = 'ALL' , name = '' , input_file : str = './.taskcat.yml' ) Parameters: Name Type Description Default project None name of project to install can be a path to a local project, a github org/repo, or an AWS Quick Start name None test_names None comma separated list of tests (specified in .taskcat.yml) to run defaults to the 'default' test. Set to 'ALL' to deploy every entry the 'default' test. Set to 'ALL' to deploy every entry regions None comma separated list of regions to test in default None name None stack name to use, if not specified one will be automatically generated None input_file None path to either a taskcat project config file or a CloudFormation template None View Source def run ( # noqa : C901 self , project : str = \"./\" , test_names : str = \"ALL\" , regions : str = \"ALL\" , name = \"\" , input_file : str = \"./.taskcat.yml\" , ) : \"\"\" : param project : name of project to install can be a path to a local project , \\ a github org / repo , or an AWS Quick Start name : param test_names : comma separated list of tests ( specified in . taskcat . yml ) to run \\ defaults to the ' default ' test . Set to ' ALL ' to deploy every entry : param regions : comma separated list of regions to test in \\ default : param name : stack name to use , if not specified one will be automatically \\ generated : param input_file : path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it ' s an AWS Quick Start package_type = \"github\" project = f \"aws-quickstart/quickstart- { project } \" if package_type == \"github\" : if project . startswith ( \"https://\" ) or project . startswith ( \"git@\" ) : url = project org , repo = ( project . replace ( \".git\" , \"\" ). replace ( \":\" , \"/\" ). split ( \"/\" )[ -2 : ] ) else : org , repo = project . split ( \"/\" ) url = f \"https://github.com/{org}/{repo}.git\" path = Deploy . PKG_CACHE_PATH / org / repo LOG . info ( f \"fetching git repo {url}\" ) self . _git_clone ( url , path ) self . _recurse_submodules ( path , url ) _extra_tags = [( Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name }))] Test . run ( regions = regions , no_delete = True , project_root = path , test_names = test_names , input_file = input_file , _extra_tags = _extra_tags , )","title":"Deploy"},{"location":"reference/taskcat/_cli_modules/deploy/#module-taskcat_cli_modulesdeploy","text":"None None View Source # pylint : disable = duplicate - code import logging import sys from io import BytesIO from pathlib import Path from dulwich import porcelain from dulwich . config import ConfigFile , parse_submodules from taskcat . _ cli_modules . test import Test from taskcat . _ dataclasses import Tag from taskcat . _ name_generator import generate_name from taskcat . regions_to_partitions import REGIONS from . list import List LOG = logging . getLogger ( __ name__ ) class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/regions\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def run ( # noqa : C901 self , project : str = \"./\" , test_names: str = \"ALL\" , regions : str = \"ALL\" , name= \"\" , input_file: str = \"./.taskcat.yml\" , ) : \"\"\" :param project: name of project to install can be a path to a local project,\\ a github org/repo, or an AWS Quick Start name :param test_names: comma separated list of tests (specified in .taskcat.yml) to run\\ defaults to the 'default' test. Set to 'ALL' to deploy every entry :param regions: comma separated list of regions to test in\\ default :param name: stack name to use, if not specified one will be automatically\\ generated :param input_file: path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" project = f\"aws-quickstart/quickstart-{project}\" if package_type == \"github\": if project.startswith(\"https://\") or project.startswith(\"git@\"): url = project org, repo = ( project.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = project.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) _extra_tags = [(Tag({\"Key\": \"taskcat-installer\", \"Value\": name}))] Test.run( regions=regions, no_delete=True, project_root=path, test_names=test_names, input_file=input_file, _extra_tags=_extra_tags, ) @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \"path already exists, updating from remote is not yet implemented\" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\"utf-8\")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \".gitmodules\" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\"utf-8\") url = url.decode(\"utf-8\") name = name.decode(\"utf-8\") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule ... sha = None try : porcelain . get_object_by_path ( str ( path ), sub_path ) except KeyError as e : sha = e . args [ 0 ]. decode ( \"utf-8\" ) if not sha : raise ValueError ( f \"Could not find sha for submodule {name}\" ) if url . startswith ( \"../\" ) : base_url = parent_url for _ in range ( url . count ( \"../\" )) : base_url = \"/\" . join ( base_url . split ( \"/\" )[:- 1 ]) url = base_url + \"/\" + url . replace ( \"../\" , \"\" ) outp = BytesIO () if not ( path / sub_path / \".git\" ). is_dir () : LOG . info ( f \"fetching git submodule {url}\" ) porcelain . clone ( url , str ( path / sub_path ), checkout = sha , errstream = outp , outstream = outp , ) LOG . debug ( outp . getvalue (). decode ( \"utf-8\" )) self . _ recurse_submodules (( path / sub_path ), url ) @staticmethod def list ( profiles : str = \"default\" , regions= \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type= \"project\" ) # Checks if all regions are valid @staticmethod def _ validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions","title":"Module taskcat._cli_modules.deploy"},{"location":"reference/taskcat/_cli_modules/deploy/#variables","text":"LOG REGIONS","title":"Variables"},{"location":"reference/taskcat/_cli_modules/deploy/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/deploy/#deploy","text":"class Deploy ( / , * args , ** kwargs ) View Source class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/regions\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def run ( # noqa : C901 self , project : str = \"./\" , test_names: str = \"ALL\" , regions : str = \"ALL\" , name= \"\" , input_file: str = \"./.taskcat.yml\" , ) : \"\"\" :param project: name of project to install can be a path to a local project,\\ a github org/repo, or an AWS Quick Start name :param test_names: comma separated list of tests (specified in .taskcat.yml) to run\\ defaults to the 'default' test. Set to 'ALL' to deploy every entry :param regions: comma separated list of regions to test in\\ default :param name: stack name to use, if not specified one will be automatically\\ generated :param input_file: path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" project = f\"aws-quickstart/quickstart-{project}\" if package_type == \"github\": if project.startswith(\"https://\") or project.startswith(\"git@\"): url = project org, repo = ( project.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = project.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) _extra_tags = [(Tag({\"Key\": \"taskcat-installer\", \"Value\": name}))] Test.run( regions=regions, no_delete=True, project_root=path, test_names=test_names, input_file=input_file, _extra_tags=_extra_tags, ) @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \"path already exists, updating from remote is not yet implemented\" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\"utf-8\")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \".gitmodules\" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\"utf-8\") url = url.decode(\"utf-8\") name = name.decode(\"utf-8\") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule ... sha = None try : porcelain . get_object_by_path ( str ( path ), sub_path ) except KeyError as e : sha = e . args [ 0 ]. decode ( \"utf-8\" ) if not sha : raise ValueError ( f \"Could not find sha for submodule {name}\" ) if url . startswith ( \"../\" ) : base_url = parent_url for _ in range ( url . count ( \"../\" )) : base_url = \"/\" . join ( base_url . split ( \"/\" )[:- 1 ]) url = base_url + \"/\" + url . replace ( \"../\" , \"\" ) outp = BytesIO () if not ( path / sub_path / \".git\" ). is_dir () : LOG . info ( f \"fetching git submodule {url}\" ) porcelain . clone ( url , str ( path / sub_path ), checkout = sha , errstream = outp , outstream = outp , ) LOG . debug ( outp . getvalue (). decode ( \"utf-8\" )) self . _ recurse_submodules (( path / sub_path ), url ) @staticmethod def list ( profiles : str = \"default\" , regions= \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type= \"project\" ) # Checks if all regions are valid @staticmethod def _ validate_regions ( region_string ) : regions = region_string . split ( \",\" ) for region in regions : if region not in REGIONS : LOG . error ( f \"Bad region detected: {region}\" ) sys . exit ( 1 ) return regions","title":"Deploy"},{"location":"reference/taskcat/_cli_modules/deploy/#class-variables","text":"PKG_CACHE_PATH","title":"Class variables"},{"location":"reference/taskcat/_cli_modules/deploy/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cli_modules/deploy/#list","text":"def list ( profiles : str = 'default' , regions = 'ALL' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"project\" )","title":"list"},{"location":"reference/taskcat/_cli_modules/deploy/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cli_modules/deploy/#run","text":"def run ( self , project : str = './' , test_names : str = 'ALL' , regions : str = 'ALL' , name = '' , input_file : str = './.taskcat.yml' ) Parameters: Name Type Description Default project None name of project to install can be a path to a local project, a github org/repo, or an AWS Quick Start name None test_names None comma separated list of tests (specified in .taskcat.yml) to run defaults to the 'default' test. Set to 'ALL' to deploy every entry the 'default' test. Set to 'ALL' to deploy every entry regions None comma separated list of regions to test in default None name None stack name to use, if not specified one will be automatically generated None input_file None path to either a taskcat project config file or a CloudFormation template None View Source def run ( # noqa : C901 self , project : str = \"./\" , test_names : str = \"ALL\" , regions : str = \"ALL\" , name = \"\" , input_file : str = \"./.taskcat.yml\" , ) : \"\"\" : param project : name of project to install can be a path to a local project , \\ a github org / repo , or an AWS Quick Start name : param test_names : comma separated list of tests ( specified in . taskcat . yml ) to run \\ defaults to the ' default ' test . Set to ' ALL ' to deploy every entry : param regions : comma separated list of regions to test in \\ default : param name : stack name to use , if not specified one will be automatically \\ generated : param input_file : path to either a taskcat project config file or a CloudFormation template \"\"\" if not name : name = generate_name () path = Path ( project ). resolve () if Path ( project ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in project : package_type = \"github\" else : # assuming it ' s an AWS Quick Start package_type = \"github\" project = f \"aws-quickstart/quickstart- { project } \" if package_type == \"github\" : if project . startswith ( \"https://\" ) or project . startswith ( \"git@\" ) : url = project org , repo = ( project . replace ( \".git\" , \"\" ). replace ( \":\" , \"/\" ). split ( \"/\" )[ -2 : ] ) else : org , repo = project . split ( \"/\" ) url = f \"https://github.com/{org}/{repo}.git\" path = Deploy . PKG_CACHE_PATH / org / repo LOG . info ( f \"fetching git repo {url}\" ) self . _git_clone ( url , path ) self . _recurse_submodules ( path , url ) _extra_tags = [( Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name }))] Test . run ( regions = regions , no_delete = True , project_root = path , test_names = test_names , input_file = input_file , _extra_tags = _extra_tags , )","title":"run"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/","text":"Module taskcat._cli_modules.generate_iam_policy None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat.iam_policy.policy import CFNPolicyGenerator LOG = logging . getLogger ( __name__ ) class GenerateIAMPolicy : \"\"\" [ALPHA] Introspects CFN Template(s) and generates an IAM policy necessary to successfully launch the template(s) \"\"\" CLINAME = \"generate-iam-policy\" def __init__ ( self , output_file : str = \"./cfn_stack_policy.json\" , project_root : str = \"./\" ): project_root_path = Path ( project_root ) . expanduser () . resolve () config = Config . create ( project_root = project_root_path ) CFNPolicyGenerator ( config , output_file ) . generate_policy () Variables LOG Classes GenerateIAMPolicy class GenerateIAMPolicy ( output_file : str = './cfn_stack_policy.json' , project_root : str = './' ) View Source class GenerateIAMPolicy : \"\"\" [ALPHA] Introspects CFN Template(s) and generates an IAM policy necessary to successfully launch the template(s) \"\"\" CLINAME = \"generate-iam-policy\" def __init__ ( self , output_file : str = \"./cfn_stack_policy.json\" , project_root : str = \"./\" ) : project_root_path = Path ( project_root ). expanduser (). resolve () config = Config . create ( project_root = project_root_path ) CFNPolicyGenerator ( config , output_file ). generate_policy () Class variables CLINAME","title":"Generate Iam Policy"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/#module-taskcat_cli_modulesgenerate_iam_policy","text":"None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat.iam_policy.policy import CFNPolicyGenerator LOG = logging . getLogger ( __name__ ) class GenerateIAMPolicy : \"\"\" [ALPHA] Introspects CFN Template(s) and generates an IAM policy necessary to successfully launch the template(s) \"\"\" CLINAME = \"generate-iam-policy\" def __init__ ( self , output_file : str = \"./cfn_stack_policy.json\" , project_root : str = \"./\" ): project_root_path = Path ( project_root ) . expanduser () . resolve () config = Config . create ( project_root = project_root_path ) CFNPolicyGenerator ( config , output_file ) . generate_policy ()","title":"Module taskcat._cli_modules.generate_iam_policy"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/#generateiampolicy","text":"class GenerateIAMPolicy ( output_file : str = './cfn_stack_policy.json' , project_root : str = './' ) View Source class GenerateIAMPolicy : \"\"\" [ALPHA] Introspects CFN Template(s) and generates an IAM policy necessary to successfully launch the template(s) \"\"\" CLINAME = \"generate-iam-policy\" def __init__ ( self , output_file : str = \"./cfn_stack_policy.json\" , project_root : str = \"./\" ) : project_root_path = Path ( project_root ). expanduser (). resolve () config = Config . create ( project_root = project_root_path ) CFNPolicyGenerator ( config , output_file ). generate_policy ()","title":"GenerateIAMPolicy"},{"location":"reference/taskcat/_cli_modules/generate_iam_policy/#class-variables","text":"CLINAME","title":"Class variables"},{"location":"reference/taskcat/_cli_modules/lint/","text":"Module taskcat._cli_modules.lint None None View Source import logging from pathlib import Path from taskcat._cfn_lint import Lint as TaskCatLint from taskcat._config import Config from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class Lint : \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file : str = \".taskcat.yml\" , project_root : str = \"./\" , strict : bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) Variables LOG Classes Lint class Lint ( input_file : str = '.taskcat.yml' , project_root : str = './' , strict : bool = False ) View Source class Lint: \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file: str = \".taskcat.yml\" , project_root: str = \"./\" , strict: bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () input_file_path: Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed: raise TaskCatException ( \"Lint failed with errors\" )","title":"Lint"},{"location":"reference/taskcat/_cli_modules/lint/#module-taskcat_cli_moduleslint","text":"None None View Source import logging from pathlib import Path from taskcat._cfn_lint import Lint as TaskCatLint from taskcat._config import Config from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class Lint : \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file : str = \".taskcat.yml\" , project_root : str = \"./\" , strict : bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" )","title":"Module taskcat._cli_modules.lint"},{"location":"reference/taskcat/_cli_modules/lint/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/lint/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/lint/#lint","text":"class Lint ( input_file : str = '.taskcat.yml' , project_root : str = './' , strict : bool = False ) View Source class Lint: \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file: str = \".taskcat.yml\" , project_root: str = \"./\" , strict: bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () input_file_path: Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed: raise TaskCatException ( \"Lint failed with errors\" )","title":"Lint"},{"location":"reference/taskcat/_cli_modules/list/","text":"Module taskcat._cli_modules.list None None View Source # pylint: disable=duplicate-code import logging from typing import List as ListType , Union import boto3 from taskcat._cfn.threaded import Stacker LOG = logging . getLogger ( __name__ ) class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint: disable=too-many-locals,too-many-branches def __init__ ( # noqa: C901 self , profiles : Union [ str , ListType [ str ]] = \"default\" , regions = \"ALL\" , stack_type = \"ALL\" , ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check \\ all commercial regions :param stack_type: type of stacks to check, options are 'test', 'project', or 'ALL'. \\ default is 'ALL' \"\"\" if isinstance ( profiles , str ): profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if stack_type == \"ALL\" : if not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif name and stack_type == \"project\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 longest_name = List . _longest ([ v [ \"name\" ] for _ , v in jobs . items ()]) longest_project_name = List . _longest ( [ v [ \"project_name\" ] for _ , v in jobs . items ()] ) if not jobs : LOG . info ( \"no stacks found\" ) return if stack_type != \"test\" : header = ( f \"NAME { List . _spaces ( longest_name ) } PROJECT { List . _spaces ( longest_project_name ) } \" f \"ID { List . _spaces ( 34 ) } REGION\" ) column = \" {} {} {} {} \" else : header = f \"NAME { List . _spaces ( longest_name ) } ID { List . _spaces ( 34 ) } REGION\" column = \" {} {} {} \" LOG . error ( header , extra = { \"nametag\" : \"\" }) for job in jobs . values (): args = [ List . _pad ( job [ \"name\" ], longest_name ), List . _pad ( job [ \"project_name\" ], longest_project_name ), job [ \"id\" ], job [ \"region\" ], ] if stack_type == \"test\" : args = [ List . _pad ( job [ \"name\" ], longest_name ), job [ \"id\" ], job [ \"region\" ]] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" }) @staticmethod def _longest ( things : list ): lengths = [ len ( thing ) for thing in things ] return sorted ( lengths )[ - 1 ] if lengths else 0 @staticmethod def _spaces ( number ): ret = \"\" for _ in range ( number ): ret += \" \" return ret @staticmethod def _pad ( string , length ): while len ( string ) < length : string += \" \" return string Variables LOG Classes List class List ( profiles : Union [ str , List [ str ]] = 'default' , regions = 'ALL' , stack_type = 'ALL' ) View Source class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint : disable = too - many - locals , too - many - branches def __init__ ( # noqa : C901 self , profiles : Union [ str, ListType[str ] ] = \"default\" , regions = \"ALL\" , stack_type = \"ALL\" , ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check \\ all commercial regions :param stack_type: type of stacks to check, options are 'test', 'project', or 'ALL'. \\ default is 'ALL' \"\"\" if isinstance ( profiles , str ) : profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if stack_type == \"ALL\" : if not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif name and stack_type == \"project\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 longest_name = List . _longest ( [ v[\"name\" ] for _ , v in jobs . items () ] ) longest_project_name = List . _longest ( [ v[\"project_name\" ] for _ , v in jobs . items () ] ) if not jobs : LOG . info ( \"no stacks found\" ) return if stack_type != \"test\" : header = ( f \"NAME{List._spaces(longest_name)}PROJECT{List._spaces(longest_project_name)}\" f \"ID{List._spaces(34)}REGION\" ) column = \"{} {} {} {}\" else : header = f \"NAME{List._spaces(longest_name)}ID{List._spaces(34)}REGION\" column = \"{} {} {}\" LOG . error ( header , extra = { \"nametag\" : \"\" } ) for job in jobs . values () : args = [ List._pad(job[\"name\" ] , longest_name ), List . _pad ( job [ \"project_name\" ] , longest_project_name ), job [ \"id\" ] , job [ \"region\" ] , ] if stack_type == \"test\" : args = [ List._pad(job[\"name\" ] , longest_name ), job [ \"id\" ] , job [ \"region\" ] ] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" } ) @staticmethod def _longest ( things : list ) : lengths = [ len(thing) for thing in things ] return sorted ( lengths ) [ -1 ] if lengths else 0 @staticmethod def _spaces ( number ) : ret = \"\" for _ in range ( number ) : ret += \" \" return ret @staticmethod def _pad ( string , length ) : while len ( string ) < length : string += \" \" return string","title":"List"},{"location":"reference/taskcat/_cli_modules/list/#module-taskcat_cli_moduleslist","text":"None None View Source # pylint: disable=duplicate-code import logging from typing import List as ListType , Union import boto3 from taskcat._cfn.threaded import Stacker LOG = logging . getLogger ( __name__ ) class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint: disable=too-many-locals,too-many-branches def __init__ ( # noqa: C901 self , profiles : Union [ str , ListType [ str ]] = \"default\" , regions = \"ALL\" , stack_type = \"ALL\" , ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check \\ all commercial regions :param stack_type: type of stacks to check, options are 'test', 'project', or 'ALL'. \\ default is 'ALL' \"\"\" if isinstance ( profiles , str ): profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if stack_type == \"ALL\" : if not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif name and stack_type == \"project\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 longest_name = List . _longest ([ v [ \"name\" ] for _ , v in jobs . items ()]) longest_project_name = List . _longest ( [ v [ \"project_name\" ] for _ , v in jobs . items ()] ) if not jobs : LOG . info ( \"no stacks found\" ) return if stack_type != \"test\" : header = ( f \"NAME { List . _spaces ( longest_name ) } PROJECT { List . _spaces ( longest_project_name ) } \" f \"ID { List . _spaces ( 34 ) } REGION\" ) column = \" {} {} {} {} \" else : header = f \"NAME { List . _spaces ( longest_name ) } ID { List . _spaces ( 34 ) } REGION\" column = \" {} {} {} \" LOG . error ( header , extra = { \"nametag\" : \"\" }) for job in jobs . values (): args = [ List . _pad ( job [ \"name\" ], longest_name ), List . _pad ( job [ \"project_name\" ], longest_project_name ), job [ \"id\" ], job [ \"region\" ], ] if stack_type == \"test\" : args = [ List . _pad ( job [ \"name\" ], longest_name ), job [ \"id\" ], job [ \"region\" ]] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" }) @staticmethod def _longest ( things : list ): lengths = [ len ( thing ) for thing in things ] return sorted ( lengths )[ - 1 ] if lengths else 0 @staticmethod def _spaces ( number ): ret = \"\" for _ in range ( number ): ret += \" \" return ret @staticmethod def _pad ( string , length ): while len ( string ) < length : string += \" \" return string","title":"Module taskcat._cli_modules.list"},{"location":"reference/taskcat/_cli_modules/list/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/list/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/list/#list","text":"class List ( profiles : Union [ str , List [ str ]] = 'default' , regions = 'ALL' , stack_type = 'ALL' ) View Source class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint : disable = too - many - locals , too - many - branches def __init__ ( # noqa : C901 self , profiles : Union [ str, ListType[str ] ] = \"default\" , regions = \"ALL\" , stack_type = \"ALL\" , ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check \\ all commercial regions :param stack_type: type of stacks to check, options are 'test', 'project', or 'ALL'. \\ default is 'ALL' \"\"\" if isinstance ( profiles , str ) : profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if stack_type == \"ALL\" : if not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif name and stack_type == \"project\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 longest_name = List . _longest ( [ v[\"name\" ] for _ , v in jobs . items () ] ) longest_project_name = List . _longest ( [ v[\"project_name\" ] for _ , v in jobs . items () ] ) if not jobs : LOG . info ( \"no stacks found\" ) return if stack_type != \"test\" : header = ( f \"NAME{List._spaces(longest_name)}PROJECT{List._spaces(longest_project_name)}\" f \"ID{List._spaces(34)}REGION\" ) column = \"{} {} {} {}\" else : header = f \"NAME{List._spaces(longest_name)}ID{List._spaces(34)}REGION\" column = \"{} {} {}\" LOG . error ( header , extra = { \"nametag\" : \"\" } ) for job in jobs . values () : args = [ List._pad(job[\"name\" ] , longest_name ), List . _pad ( job [ \"project_name\" ] , longest_project_name ), job [ \"id\" ] , job [ \"region\" ] , ] if stack_type == \"test\" : args = [ List._pad(job[\"name\" ] , longest_name ), job [ \"id\" ] , job [ \"region\" ] ] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" } ) @staticmethod def _longest ( things : list ) : lengths = [ len(thing) for thing in things ] return sorted ( lengths ) [ -1 ] if lengths else 0 @staticmethod def _spaces ( number ) : ret = \"\" for _ in range ( number ) : ret += \" \" return ret @staticmethod def _pad ( string , length ) : while len ( string ) < length : string += \" \" return string","title":"List"},{"location":"reference/taskcat/_cli_modules/package/","text":"Module taskcat._cli_modules.package None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat._lambda_build import LambdaBuild LOG = logging . getLogger ( __name__ ) class Package : \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root : str = \"./\" , source_folder : str = \"lambda_functions/source\" , zip_folder : str = \"lambda_functions/packages\" , config_file : str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () project_config : Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args = { \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda : LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path ) Variables LOG Classes Package class Package ( project_root : str = './' , source_folder : str = 'lambda_functions/source' , zip_folder : str = 'lambda_functions/packages' , config_file : str = '.taskcat.yml' ) View Source class Package: \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root: str = \"./\" , source_folder: str = \"lambda_functions/source\" , zip_folder: str = \"lambda_functions/packages\" , config_file: str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () project_config: Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args ={ \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda: LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Package"},{"location":"reference/taskcat/_cli_modules/package/#module-taskcat_cli_modulespackage","text":"None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat._lambda_build import LambdaBuild LOG = logging . getLogger ( __name__ ) class Package : \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root : str = \"./\" , source_folder : str = \"lambda_functions/source\" , zip_folder : str = \"lambda_functions/packages\" , config_file : str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () project_config : Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args = { \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda : LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Module taskcat._cli_modules.package"},{"location":"reference/taskcat/_cli_modules/package/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/package/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/package/#package","text":"class Package ( project_root : str = './' , source_folder : str = 'lambda_functions/source' , zip_folder : str = 'lambda_functions/packages' , config_file : str = '.taskcat.yml' ) View Source class Package: \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root: str = \"./\" , source_folder: str = \"lambda_functions/source\" , zip_folder: str = \"lambda_functions/packages\" , config_file: str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () project_config: Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args ={ \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda: LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Package"},{"location":"reference/taskcat/_cli_modules/test/","text":"Module taskcat._cli_modules.test None None View Source # pylint: disable=duplicate-code # noqa: B950,F841 import inspect import logging import os import sys import tempfile from pathlib import Path import boto3 import yaml from taskcat._common_utils import determine_profile_for_region from taskcat._config import Config from taskcat._tui import TerminalPrinter from taskcat.testing import CFNTest from .delete import Delete from .list import List LOG = logging . getLogger ( __name__ ) class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint: disable=too-many-locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ): \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ) . client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name )[ \"StackEvents\" ] resource = [ i for i in events if i [ \"LogicalResourceId\" ] == resource_name ][ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ]) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" )[ 4 :] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ]: cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ]) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ) . wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir ) @staticmethod # pylint: disable=too-many-arguments,W0613,line-too-long def run ( # noqa: C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ): \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa: B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ): setattr ( test , i , values [ i ]) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory ) def resume ( self , run_id ): # pylint: disable=no-self-use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ): \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" ) Variables LOG Classes Test class Test ( / , * args , ** kwargs ) View Source class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint : disable = too - many - locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0 o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir ) @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory ) def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" ) Static methods clean def clean ( project : str , aws_profile : str = 'default' , region = 'ALL' ) Parameters: Name Type Description Default project None project to delete, can be an name or uuid, or ALL to clean all tests None aws_profile None aws profile to use for deletion None region None region to delete from, default will scan all regions None View Source @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" ) list def list ( profiles : str = 'default' , regions = 'ALL' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" ) retry def retry ( region : str , stack_name : str , resource_name : str , config_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False ) [ALPHA] re-launches a child stack using the same parameters as previous launch Parameters: Name Type Description Default region None region stack is in None stack_name None name of parent stack None resource_name None logical id of child stack that will be re-launched None config_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None keep_failed None do not delete failed stacks None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None View Source @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0 o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir ) run def run ( test_names : str = 'ALL' , regions : str = 'ALL' , input_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = './taskcat_outputs' , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : taskcat . _cli_modules . list . List = None ) tests whether CloudFormation templates are able to successfully launch Parameters: Name Type Description Default test_names None comma separated list of tests to run None regions None comma separated list of regions to test in None input_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None lint_disable None disable cfn-lint checks None enable_sig_v2 None enable legacy sigv2 requests for auto-created buckets None keep_failed None do not delete failed stacks None output_directory None Where to store generated logfiles None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None skip_upload None Use templates in an existing cloudformation bucket. None View Source @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory ) Methods resume def resume ( self , run_id ) resumes a monitoring of a previously started test run View Source def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\" resumes a monitoring of a previously started test run \"\"\" # do some stuff raise NotImplementedError ()","title":"Test"},{"location":"reference/taskcat/_cli_modules/test/#module-taskcat_cli_modulestest","text":"None None View Source # pylint: disable=duplicate-code # noqa: B950,F841 import inspect import logging import os import sys import tempfile from pathlib import Path import boto3 import yaml from taskcat._common_utils import determine_profile_for_region from taskcat._config import Config from taskcat._tui import TerminalPrinter from taskcat.testing import CFNTest from .delete import Delete from .list import List LOG = logging . getLogger ( __name__ ) class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint: disable=too-many-locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ): \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ) . client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name )[ \"StackEvents\" ] resource = [ i for i in events if i [ \"LogicalResourceId\" ] == resource_name ][ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ]) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" )[ 4 :] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ]: cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ]) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ) . wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir ) @staticmethod # pylint: disable=too-many-arguments,W0613,line-too-long def run ( # noqa: C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ): \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa: B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ): setattr ( test , i , values [ i ]) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory ) def resume ( self , run_id ): # pylint: disable=no-self-use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ): \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" )","title":"Module taskcat._cli_modules.test"},{"location":"reference/taskcat/_cli_modules/test/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/test/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/test/#test","text":"class Test ( / , * args , ** kwargs ) View Source class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint : disable = too - many - locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0 o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir ) @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory ) def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" )","title":"Test"},{"location":"reference/taskcat/_cli_modules/test/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cli_modules/test/#clean","text":"def clean ( project : str , aws_profile : str = 'default' , region = 'ALL' ) Parameters: Name Type Description Default project None project to delete, can be an name or uuid, or ALL to clean all tests None aws_profile None aws profile to use for deletion None region None region to delete from, default will scan all regions None View Source @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" Delete ( project = project , aws_profile = aws_profile , region = region , stack_type = \"test\" )","title":"clean"},{"location":"reference/taskcat/_cli_modules/test/#list","text":"def list ( profiles : str = 'default' , regions = 'ALL' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , stack_type = \"test\" )","title":"list"},{"location":"reference/taskcat/_cli_modules/test/#retry","text":"def retry ( region : str , stack_name : str , resource_name : str , config_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False ) [ALPHA] re-launches a child stack using the same parameters as previous launch Parameters: Name Type Description Default region None region stack is in None stack_name None name of parent stack None resource_name None logical id of child stack that will be re-launched None config_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None keep_failed None do not delete failed stacks None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None View Source @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" , encoding = \"utf-8\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} tmpdir = tempfile . mkdtemp () name = \".taskcat.yml.temp\" umask = os . umask ( 0 o77 ) file_path = os . path . join ( tmpdir , name ) try : with open ( file_path , \"w\" , encoding = \"utf-8\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = file_path , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) except IOError : LOG . error ( \"IOError when retrying Test Run\" ) sys . exit ( 1 ) else : os . remove ( file_path ) finally : os . umask ( umask ) os . rmdir ( tmpdir )","title":"retry"},{"location":"reference/taskcat/_cli_modules/test/#run","text":"def run ( test_names : str = 'ALL' , regions : str = 'ALL' , input_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = './taskcat_outputs' , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : taskcat . _cli_modules . list . List = None ) tests whether CloudFormation templates are able to successfully launch Parameters: Name Type Description Default test_names None comma separated list of tests to run None regions None comma separated list of regions to test in None input_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None lint_disable None disable cfn-lint checks None enable_sig_v2 None enable legacy sigv2 requests for auto-created buckets None keep_failed None do not delete failed stacks None output_directory None Where to store generated logfiles None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None skip_upload None Use templates in an existing cloudformation bucket. None View Source @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , _extra_tags : List = None , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer # Runs here with test : test . report ( output_directory )","title":"run"},{"location":"reference/taskcat/_cli_modules/test/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cli_modules/test/#resume","text":"def resume ( self , run_id ) resumes a monitoring of a previously started test run View Source def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\" resumes a monitoring of a previously started test run \"\"\" # do some stuff raise NotImplementedError ()","title":"resume"},{"location":"reference/taskcat/_cli_modules/update_ami/","text":"Module taskcat._cli_modules.update_ami None None View Source import logging import os from pathlib import Path from taskcat._amiupdater import ( AMIUpdater , AMIUpdaterCommitNeededException , AMIUpdaterFatalException , ) from taskcat._common_utils import exit_with_code from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class UpdateAMI : \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root : str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else : _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try : amiupdater . update_amis () except AMIUpdaterCommitNeededException : exit_with_code ( 100 ) except AMIUpdaterFatalException : exit_with_code ( 1 ) Variables LOG Classes UpdateAMI class UpdateAMI ( project_root : str = './' ) View Source class UpdateAMI: \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root: str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else: _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try: amiupdater . update_amis () except AMIUpdaterCommitNeededException: exit_with_code ( 100 ) except AMIUpdaterFatalException: exit_with_code ( 1 ) Class variables CLINAME","title":"Update Ami"},{"location":"reference/taskcat/_cli_modules/update_ami/#module-taskcat_cli_modulesupdate_ami","text":"None None View Source import logging import os from pathlib import Path from taskcat._amiupdater import ( AMIUpdater , AMIUpdaterCommitNeededException , AMIUpdaterFatalException , ) from taskcat._common_utils import exit_with_code from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class UpdateAMI : \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root : str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else : _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try : amiupdater . update_amis () except AMIUpdaterCommitNeededException : exit_with_code ( 100 ) except AMIUpdaterFatalException : exit_with_code ( 1 )","title":"Module taskcat._cli_modules.update_ami"},{"location":"reference/taskcat/_cli_modules/update_ami/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/update_ami/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/update_ami/#updateami","text":"class UpdateAMI ( project_root : str = './' ) View Source class UpdateAMI: \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root: str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else: _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try: amiupdater . update_amis () except AMIUpdaterCommitNeededException: exit_with_code ( 100 ) except AMIUpdaterFatalException: exit_with_code ( 1 )","title":"UpdateAMI"},{"location":"reference/taskcat/_cli_modules/update_ami/#class-variables","text":"CLINAME","title":"Class variables"},{"location":"reference/taskcat/_cli_modules/upload/","text":"Module taskcat._cli_modules.upload None None View Source import logging from pathlib import Path from typing import Any , Dict from taskcat._cli_core import CliCore from taskcat._client_factory import Boto3Cache from taskcat._config import Config from taskcat._lambda_build import LambdaBuild from taskcat._s3_stage import stage_in_s3 LOG = logging . getLogger ( __name__ ) class Upload : \"\"\" Uploads project to S3. \"\"\" @CliCore . longform_param_required ( \"exclude_prefix\" ) @CliCore . longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , object_acl : str = \"\" , exclude_prefix : list = None , ): # pylint: disable=too-many-locals \"\"\"does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if object_acl : args [ \"project\" ][ \"s3_object_acl\" ] = object_acl if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ): LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , exclude_prefix , dry_run , ) Variables LOG Classes Upload class Upload ( config_file : str = './.taskcat.yml' , project_root : str = './' , enable_sig_v2 : bool = False , bucket_name : str = '' , disable_lambda_packaging : bool = False , key_prefix : str = '' , dry_run : bool = False , object_acl : str = '' , exclude_prefix : list = None ) View Source class Upload : \" \"\" Uploads project to S3. \"\" \" @CliCore.longform_param_required ( \"exclude_prefix\" ) @CliCore.longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , object_acl : str = \"\" , exclude_prefix : list = None , ) : # pylint: disable=too-many-locals \" \"\" does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\" \" project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if object_acl : args [ \"project\" ][ \"s3_object_acl\" ] = object_acl if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ) : LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , exclude_prefix , dry_run , )","title":"Upload"},{"location":"reference/taskcat/_cli_modules/upload/#module-taskcat_cli_modulesupload","text":"None None View Source import logging from pathlib import Path from typing import Any , Dict from taskcat._cli_core import CliCore from taskcat._client_factory import Boto3Cache from taskcat._config import Config from taskcat._lambda_build import LambdaBuild from taskcat._s3_stage import stage_in_s3 LOG = logging . getLogger ( __name__ ) class Upload : \"\"\" Uploads project to S3. \"\"\" @CliCore . longform_param_required ( \"exclude_prefix\" ) @CliCore . longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , object_acl : str = \"\" , exclude_prefix : list = None , ): # pylint: disable=too-many-locals \"\"\"does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if object_acl : args [ \"project\" ][ \"s3_object_acl\" ] = object_acl if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ): LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , exclude_prefix , dry_run , )","title":"Module taskcat._cli_modules.upload"},{"location":"reference/taskcat/_cli_modules/upload/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/upload/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/upload/#upload","text":"class Upload ( config_file : str = './.taskcat.yml' , project_root : str = './' , enable_sig_v2 : bool = False , bucket_name : str = '' , disable_lambda_packaging : bool = False , key_prefix : str = '' , dry_run : bool = False , object_acl : str = '' , exclude_prefix : list = None ) View Source class Upload : \" \"\" Uploads project to S3. \"\" \" @CliCore.longform_param_required ( \"exclude_prefix\" ) @CliCore.longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , object_acl : str = \"\" , exclude_prefix : list = None , ) : # pylint: disable=too-many-locals \" \"\" does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\" \" project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if object_acl : args [ \"project\" ][ \"s3_object_acl\" ] = object_acl if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ) : LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , exclude_prefix , dry_run , )","title":"Upload"},{"location":"reference/taskcat/iam_policy/","text":"Module taskcat.iam_policy None None Sub-modules taskcat.iam_policy.policy taskcat.iam_policy.tools","title":"Index"},{"location":"reference/taskcat/iam_policy/#module-taskcatiam_policy","text":"None None","title":"Module taskcat.iam_policy"},{"location":"reference/taskcat/iam_policy/#sub-modules","text":"taskcat.iam_policy.policy taskcat.iam_policy.tools","title":"Sub-modules"},{"location":"reference/taskcat/iam_policy/policy/","text":"Module taskcat.iam_policy.policy None None View Source import json import logging from pathlib import Path from typing import List import pkg_resources from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class CFNPolicyGenerator : def __init__ ( self , config : Config , output_file : str ): self . _config = config self . _data_file_path = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/cfn_resource_iam_policy.json\" ) self . _output_file = output_file def generate_policy ( self ): LOG . warning ( \"This is an ALPHA feature. Use with caution\" ) templates = [] for template in self . _config . get_templates () . values (): templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \"Resources\" ] . items (): resource_types . add ( _resource [ \"Type\" ]) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ) . resolve (), \"w\" , encoding = \"utf-8\" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True )) @staticmethod def _generate_placeholder ( resource_type_name ): svc_name = resource_type_name . split ( \"::\" )[ 1 ] . lower () _x = { \"create\" : [ f \" { svc_name } :*\" ], \"read\" : [ f \" { svc_name } :*\" ], \"update\" : [ f \" { svc_name } :*\" ], \"delete\" : [ f \" { svc_name } :*\" ], } return _x def _policy_from_resource_types ( self , resource_types : List [ str ]): with open ( self . _data_file_path , encoding = \"utf-8\" ) as _f : data = json . load ( _f ) _policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : []} _statements : dict = { \"create\" : set (), \"read\" : set (), \"update\" : set (), \"delete\" : set (), } for resource in resource_types : for k , v in data . get ( resource , self . _generate_placeholder ( resource ) ) . items (): for action in v : _statements [ k ] . add ( action ) for k , v in _statements . items (): _policy [ \"Statement\" ] . append ( { \"Sid\" : f \" { k . upper () } Actions\" , \"Effect\" : \"Allow\" , \"Action\" : sorted ( v ), \"Resource\" : \"*\" , } ) LOG . warning ( \"NOTE: The generated IAM policy will contain <service>:* IAM Actions where a\" + \" coverage gap exists within the CloudFormation Resource Spec\" ) LOG . warning ( \"Provide feedback to the CloudFormation team via: \" + \"https://github.com/aws-cloudformation/cloudformation-coverage-roadmap \" ) return _policy Variables LOG Classes CFNPolicyGenerator class CFNPolicyGenerator ( config : taskcat . _config . Config , output_file : str ) View Source class CFNPolicyGenerator : def __init__ ( self , config : Config , output_file : str ) : self . _config = config self . _data_file_path = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/cfn_resource_iam_policy.json\" ) self . _output_file = output_file def generate_policy ( self ) : LOG . warning ( \"This is an ALPHA feature. Use with caution\" ) templates = [] for template in self . _config . get_templates (). values () : templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \"Resources\" ] . items () : resource_types . add ( _resource [ \"Type\" ] ) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ). resolve (), \"w\" , encoding = \"utf-8\" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True )) @staticmethod def _generate_placeholder ( resource_type_name ) : svc_name = resource_type_name . split ( \"::\" ) [ 1 ] . lower () _x = { \"create\" : [ f\"{svc_name}:*\" ] , \"read\" : [ f\"{svc_name}:*\" ] , \"update\" : [ f\"{svc_name}:*\" ] , \"delete\" : [ f\"{svc_name}:*\" ] , } return _x def _policy_from_resource_types ( self , resource_types : List [ str ] ) : with open ( self . _data_file_path , encoding = \"utf-8\" ) as _f : data = json . load ( _f ) _policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : []} _statements : dict = { \"create\" : set (), \"read\" : set (), \"update\" : set (), \"delete\" : set (), } for resource in resource_types : for k , v in data . get ( resource , self . _generate_placeholder ( resource ) ). items () : for action in v : _statements [ k ] . add ( action ) for k , v in _statements . items () : _policy [ \"Statement\" ] . append ( { \"Sid\" : f \"{k.upper()}Actions\" , \"Effect\" : \"Allow\" , \"Action\" : sorted ( v ), \"Resource\" : \"*\" , } ) LOG . warning ( \"NOTE: The generated IAM policy will contain <service>:* IAM Actions where a\" + \" coverage gap exists within the CloudFormation Resource Spec\" ) LOG . warning ( \"Provide feedback to the CloudFormation team via: \" + \"https://github.com/aws-cloudformation/cloudformation-coverage-roadmap \" ) return _policy Methods generate_policy def generate_policy ( self ) View Source def generate_policy ( self ) : LOG . warning ( \" This is an ALPHA feature. Use with caution \" ) templates = [] for template in self . _config . get_templates () . values () : templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \" Resources \" ]. items () : resource_types . add ( _resource [ \" Type \" ] ) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ) . resolve () , \" w \" , encoding = \" utf-8 \" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True ))","title":"Policy"},{"location":"reference/taskcat/iam_policy/policy/#module-taskcatiam_policypolicy","text":"None None View Source import json import logging from pathlib import Path from typing import List import pkg_resources from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class CFNPolicyGenerator : def __init__ ( self , config : Config , output_file : str ): self . _config = config self . _data_file_path = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/cfn_resource_iam_policy.json\" ) self . _output_file = output_file def generate_policy ( self ): LOG . warning ( \"This is an ALPHA feature. Use with caution\" ) templates = [] for template in self . _config . get_templates () . values (): templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \"Resources\" ] . items (): resource_types . add ( _resource [ \"Type\" ]) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ) . resolve (), \"w\" , encoding = \"utf-8\" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True )) @staticmethod def _generate_placeholder ( resource_type_name ): svc_name = resource_type_name . split ( \"::\" )[ 1 ] . lower () _x = { \"create\" : [ f \" { svc_name } :*\" ], \"read\" : [ f \" { svc_name } :*\" ], \"update\" : [ f \" { svc_name } :*\" ], \"delete\" : [ f \" { svc_name } :*\" ], } return _x def _policy_from_resource_types ( self , resource_types : List [ str ]): with open ( self . _data_file_path , encoding = \"utf-8\" ) as _f : data = json . load ( _f ) _policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : []} _statements : dict = { \"create\" : set (), \"read\" : set (), \"update\" : set (), \"delete\" : set (), } for resource in resource_types : for k , v in data . get ( resource , self . _generate_placeholder ( resource ) ) . items (): for action in v : _statements [ k ] . add ( action ) for k , v in _statements . items (): _policy [ \"Statement\" ] . append ( { \"Sid\" : f \" { k . upper () } Actions\" , \"Effect\" : \"Allow\" , \"Action\" : sorted ( v ), \"Resource\" : \"*\" , } ) LOG . warning ( \"NOTE: The generated IAM policy will contain <service>:* IAM Actions where a\" + \" coverage gap exists within the CloudFormation Resource Spec\" ) LOG . warning ( \"Provide feedback to the CloudFormation team via: \" + \"https://github.com/aws-cloudformation/cloudformation-coverage-roadmap \" ) return _policy","title":"Module taskcat.iam_policy.policy"},{"location":"reference/taskcat/iam_policy/policy/#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/iam_policy/policy/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/iam_policy/policy/#cfnpolicygenerator","text":"class CFNPolicyGenerator ( config : taskcat . _config . Config , output_file : str ) View Source class CFNPolicyGenerator : def __init__ ( self , config : Config , output_file : str ) : self . _config = config self . _data_file_path = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/cfn_resource_iam_policy.json\" ) self . _output_file = output_file def generate_policy ( self ) : LOG . warning ( \"This is an ALPHA feature. Use with caution\" ) templates = [] for template in self . _config . get_templates (). values () : templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \"Resources\" ] . items () : resource_types . add ( _resource [ \"Type\" ] ) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ). resolve (), \"w\" , encoding = \"utf-8\" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True )) @staticmethod def _generate_placeholder ( resource_type_name ) : svc_name = resource_type_name . split ( \"::\" ) [ 1 ] . lower () _x = { \"create\" : [ f\"{svc_name}:*\" ] , \"read\" : [ f\"{svc_name}:*\" ] , \"update\" : [ f\"{svc_name}:*\" ] , \"delete\" : [ f\"{svc_name}:*\" ] , } return _x def _policy_from_resource_types ( self , resource_types : List [ str ] ) : with open ( self . _data_file_path , encoding = \"utf-8\" ) as _f : data = json . load ( _f ) _policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : []} _statements : dict = { \"create\" : set (), \"read\" : set (), \"update\" : set (), \"delete\" : set (), } for resource in resource_types : for k , v in data . get ( resource , self . _generate_placeholder ( resource ) ). items () : for action in v : _statements [ k ] . add ( action ) for k , v in _statements . items () : _policy [ \"Statement\" ] . append ( { \"Sid\" : f \"{k.upper()}Actions\" , \"Effect\" : \"Allow\" , \"Action\" : sorted ( v ), \"Resource\" : \"*\" , } ) LOG . warning ( \"NOTE: The generated IAM policy will contain <service>:* IAM Actions where a\" + \" coverage gap exists within the CloudFormation Resource Spec\" ) LOG . warning ( \"Provide feedback to the CloudFormation team via: \" + \"https://github.com/aws-cloudformation/cloudformation-coverage-roadmap \" ) return _policy","title":"CFNPolicyGenerator"},{"location":"reference/taskcat/iam_policy/policy/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/iam_policy/policy/#generate_policy","text":"def generate_policy ( self ) View Source def generate_policy ( self ) : LOG . warning ( \" This is an ALPHA feature. Use with caution \" ) templates = [] for template in self . _config . get_templates () . values () : templates . append ( template ) templates += list ( template . descendents ) resource_types = set () for template in templates : for _resource_name , _resource in template . template [ \" Resources \" ]. items () : resource_types . add ( _resource [ \" Type \" ] ) policy = self . _policy_from_resource_types ( list ( resource_types )) with open ( Path ( self . _output_file ) . resolve () , \" w \" , encoding = \" utf-8 \" ) as _f : _f . write ( json . dumps ( policy , indent = 4 , sort_keys = True ))","title":"generate_policy"},{"location":"reference/taskcat/iam_policy/tools/","text":"Module taskcat.iam_policy.tools None None View Source import json import boto3 CFN = boto3 . client ( \"cloudformation\" ) def _get_all_resource_types (): _t = [] paginator = CFN . get_paginator ( \"list_types\" ) for page in paginator . paginate ( Visibility = \"PUBLIC\" , ProvisioningType = \"FULLY_MUTABLE\" , Type = \"RESOURCE\" , Filters = { \"Category\" : \"AWS_TYPES\" }, ): for _r in page [ \"TypeSummaries\" ]: _t . append ( _r [ \"TypeArn\" ]) return _t def _get_schema_for_resource_type ( resource_type_arn ): resp = CFN . describe_type ( Arn = resource_type_arn ) return json . loads ( resp [ \"Schema\" ]) def _transform_to_abbreviated_format ( schema ): result = { schema [ \"typeName\" ]: {}} for method in [ \"create\" , \"read\" , \"update\" , \"delete\" ]: transformed = [] for _z in schema [ \"handlers\" ][ method ][ \"permissions\" ]: if not _z : continue try : _x , _y = _z . split ( \":\" ) transformed . append ( f \" { _x . lower () } : { _y } \" ) except ValueError : transformed . append ( _z ) result [ schema [ \"typeName\" ]][ method ] = transformed return result Variables CFN","title":"Tools"},{"location":"reference/taskcat/iam_policy/tools/#module-taskcatiam_policytools","text":"None None View Source import json import boto3 CFN = boto3 . client ( \"cloudformation\" ) def _get_all_resource_types (): _t = [] paginator = CFN . get_paginator ( \"list_types\" ) for page in paginator . paginate ( Visibility = \"PUBLIC\" , ProvisioningType = \"FULLY_MUTABLE\" , Type = \"RESOURCE\" , Filters = { \"Category\" : \"AWS_TYPES\" }, ): for _r in page [ \"TypeSummaries\" ]: _t . append ( _r [ \"TypeArn\" ]) return _t def _get_schema_for_resource_type ( resource_type_arn ): resp = CFN . describe_type ( Arn = resource_type_arn ) return json . loads ( resp [ \"Schema\" ]) def _transform_to_abbreviated_format ( schema ): result = { schema [ \"typeName\" ]: {}} for method in [ \"create\" , \"read\" , \"update\" , \"delete\" ]: transformed = [] for _z in schema [ \"handlers\" ][ method ][ \"permissions\" ]: if not _z : continue try : _x , _y = _z . split ( \":\" ) transformed . append ( f \" { _x . lower () } : { _y } \" ) except ValueError : transformed . append ( _z ) result [ schema [ \"typeName\" ]][ method ] = transformed return result","title":"Module taskcat.iam_policy.tools"},{"location":"reference/taskcat/iam_policy/tools/#variables","text":"CFN","title":"Variables"},{"location":"reference/taskcat/testing/","text":"Module taskcat.testing None None View Source from ._cfn_test import CFNTest # noqa: F401 from ._lint_test import LintTest # noqa: F401 from ._unit_test import UnitTest # noqa: F401 __all__ = [ \"CFNTest\" ] Sub-modules taskcat.testing.base_test Classes CFNTest class CFNTest ( config : taskcat . _config . Config , printer : Union [ taskcat . _tui . TerminalPrinter , NoneType ] = None , test_names : str = 'ALL' , regions : str = 'ALL' , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , _extra_tags : list = None ) View Source class CFNTest ( BaseTest ): # pylint: disable=too-many-instance-attributes \"\"\" Tests Cloudformation template by making sure the stack can properly deploy in the specified regions. \"\"\" def __init__ ( self , config : Config , printer : Union [ TerminalPrinter , None ] = None , test_names : str = \"ALL\" , regions : str = \"ALL\" , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , _extra_tags : list = None , ): \"\"\"The constructor creates a test from the given Config object. Args: config (Config): A pre-configured Taskcat Config instance. printer (Union[TerminalPrinter, None], optional): A printer object that will handle Test output. Defaults to TerminalPrinter. test_names (str, optional): A comma separated list of tests to run. Defaults to \"ALL\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". skip_upload (bool, optional): Use templates in an existing cloudformation bucket. Defaults to False. lint_disable (bool, optional): Disable linting with cfn-lint. Defaults to False. no_delete (bool, optional): Don't delete stacks after test is complete. Defaults to False. keep_failed (bool, optional): Don't delete failed stacks. Defaults to False. dont_wait_for_delete (bool, optional): Exits immediately after calling stack_delete. Defaults to True. \"\"\" # noqa: B950 super () . __init__ ( config ) self . test_definition : Stacker self . test_names = test_names self . regions = regions self . skip_upload = skip_upload self . lint_disable = lint_disable self . no_delete = no_delete self . keep_failed = keep_failed self . dont_wait_for_delete = dont_wait_for_delete self . _extra_tags = _extra_tags if _extra_tags else [] if printer is None : self . printer = TerminalPrinter ( minimalist = True ) else : self . printer = printer def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root , [] ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , tags = self . _extra_tags , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks def clean_up ( self ) -> None : # noqa: C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ): LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ]) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ({ \"status\" : \"CREATE_COMPLETE\" }) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO: summarise stack statusses (did they complete/delete ok) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ]) == 0 ): deleted : ListType [ str ] = [] for test in buckets . values (): for bucket in test . values (): if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK. status = self . test_definition . status () if len ( status [ \"FAILED\" ]) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) def report ( self , output_directory : str = \"./taskcat_outputs\" , ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path ( output_directory ) . resolve () report_path . mkdir ( exist_ok = True ) cfn_logs = _CfnLogTools () cfn_logs . createcfnlogs ( self . test_definition , report_path ) ReportBuilder ( self . test_definition , report_path / \"index.html\" ) . generate_report () Ancestors (in MRO) taskcat.testing.base_test.BaseTest taskcat.testing._abstract_test.Test abc.ABC Static methods from_dict def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) from_file def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) Instance variables config passed result Methods clean_up def clean_up ( self ) -> None Deletes the Test related resources in AWS. Raises: Type Description TaskCatException If one or more stacks failed to create. View Source def clean_up ( self ) -> None : # noqa : C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ) : LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ] ) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ( { \"status\" : \"CREATE_COMPLETE\" } ) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO : summarise stack statusses ( did they complete / delete ok ) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ] ) == 0 ) : deleted : ListType [ str ] = [] for test in buckets . values () : for bucket in test . values () : if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK . status = self . test_definition . status () if len ( status [ \"FAILED\" ] ) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) report def report ( self , output_directory : str = './taskcat_outputs' ) Generates a report of the status of Cloudformation stacks. Parameters: Name Type Description Default output_directory str The directory to save the report in. Defaults to \"./taskcat_outputs\". \"./taskcat_outputs\" View Source def report( self, output_directory: str = \"./taskcat_outputs\", ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path(output_directory).resolve() report_path.mkdir(exist_ok=True) cfn_logs = _CfnLogTools() cfn_logs.createcfnlogs(self.test_definition, report_path) ReportBuilder( self.test_definition, report_path / \"index.html\" ).generate_report() run def run ( self ) -> None Deploys the required Test resources in AWS. Raises: Type Description TaskCatException If skip_upload is set without specifying s3_bucket in config. TaskCatException If linting fails with errors. View Source def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root , [] ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , tags = self . _extra_tags , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks","title":"Index"},{"location":"reference/taskcat/testing/#module-taskcattesting","text":"None None View Source from ._cfn_test import CFNTest # noqa: F401 from ._lint_test import LintTest # noqa: F401 from ._unit_test import UnitTest # noqa: F401 __all__ = [ \"CFNTest\" ]","title":"Module taskcat.testing"},{"location":"reference/taskcat/testing/#sub-modules","text":"taskcat.testing.base_test","title":"Sub-modules"},{"location":"reference/taskcat/testing/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/testing/#cfntest","text":"class CFNTest ( config : taskcat . _config . Config , printer : Union [ taskcat . _tui . TerminalPrinter , NoneType ] = None , test_names : str = 'ALL' , regions : str = 'ALL' , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , _extra_tags : list = None ) View Source class CFNTest ( BaseTest ): # pylint: disable=too-many-instance-attributes \"\"\" Tests Cloudformation template by making sure the stack can properly deploy in the specified regions. \"\"\" def __init__ ( self , config : Config , printer : Union [ TerminalPrinter , None ] = None , test_names : str = \"ALL\" , regions : str = \"ALL\" , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , _extra_tags : list = None , ): \"\"\"The constructor creates a test from the given Config object. Args: config (Config): A pre-configured Taskcat Config instance. printer (Union[TerminalPrinter, None], optional): A printer object that will handle Test output. Defaults to TerminalPrinter. test_names (str, optional): A comma separated list of tests to run. Defaults to \"ALL\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". skip_upload (bool, optional): Use templates in an existing cloudformation bucket. Defaults to False. lint_disable (bool, optional): Disable linting with cfn-lint. Defaults to False. no_delete (bool, optional): Don't delete stacks after test is complete. Defaults to False. keep_failed (bool, optional): Don't delete failed stacks. Defaults to False. dont_wait_for_delete (bool, optional): Exits immediately after calling stack_delete. Defaults to True. \"\"\" # noqa: B950 super () . __init__ ( config ) self . test_definition : Stacker self . test_names = test_names self . regions = regions self . skip_upload = skip_upload self . lint_disable = lint_disable self . no_delete = no_delete self . keep_failed = keep_failed self . dont_wait_for_delete = dont_wait_for_delete self . _extra_tags = _extra_tags if _extra_tags else [] if printer is None : self . printer = TerminalPrinter ( minimalist = True ) else : self . printer = printer def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root , [] ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , tags = self . _extra_tags , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks def clean_up ( self ) -> None : # noqa: C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ): LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ]) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ({ \"status\" : \"CREATE_COMPLETE\" }) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO: summarise stack statusses (did they complete/delete ok) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ]) == 0 ): deleted : ListType [ str ] = [] for test in buckets . values (): for bucket in test . values (): if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK. status = self . test_definition . status () if len ( status [ \"FAILED\" ]) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) def report ( self , output_directory : str = \"./taskcat_outputs\" , ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path ( output_directory ) . resolve () report_path . mkdir ( exist_ok = True ) cfn_logs = _CfnLogTools () cfn_logs . createcfnlogs ( self . test_definition , report_path ) ReportBuilder ( self . test_definition , report_path / \"index.html\" ) . generate_report ()","title":"CFNTest"},{"location":"reference/taskcat/testing/#ancestors-in-mro","text":"taskcat.testing.base_test.BaseTest taskcat.testing._abstract_test.Test abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/testing/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/testing/#from_dict","text":"def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"from_dict"},{"location":"reference/taskcat/testing/#from_file","text":"def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config )","title":"from_file"},{"location":"reference/taskcat/testing/#instance-variables","text":"config passed result","title":"Instance variables"},{"location":"reference/taskcat/testing/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/testing/#clean_up","text":"def clean_up ( self ) -> None Deletes the Test related resources in AWS. Raises: Type Description TaskCatException If one or more stacks failed to create. View Source def clean_up ( self ) -> None : # noqa : C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ) : LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ] ) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ( { \"status\" : \"CREATE_COMPLETE\" } ) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO : summarise stack statusses ( did they complete / delete ok ) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ] ) == 0 ) : deleted : ListType [ str ] = [] for test in buckets . values () : for bucket in test . values () : if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK . status = self . test_definition . status () if len ( status [ \"FAILED\" ] ) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' )","title":"clean_up"},{"location":"reference/taskcat/testing/#report","text":"def report ( self , output_directory : str = './taskcat_outputs' ) Generates a report of the status of Cloudformation stacks. Parameters: Name Type Description Default output_directory str The directory to save the report in. Defaults to \"./taskcat_outputs\". \"./taskcat_outputs\" View Source def report( self, output_directory: str = \"./taskcat_outputs\", ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path(output_directory).resolve() report_path.mkdir(exist_ok=True) cfn_logs = _CfnLogTools() cfn_logs.createcfnlogs(self.test_definition, report_path) ReportBuilder( self.test_definition, report_path / \"index.html\" ).generate_report()","title":"report"},{"location":"reference/taskcat/testing/#run","text":"def run ( self ) -> None Deploys the required Test resources in AWS. Raises: Type Description TaskCatException If skip_upload is set without specifying s3_bucket in config. TaskCatException If linting fails with errors. View Source def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root , [] ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , tags = self . _extra_tags , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks","title":"run"},{"location":"reference/taskcat/testing/base_test/","text":"Module taskcat.testing.base_test None None View Source # pylint: disable=line-too-long import uuid from pathlib import Path from typing import Any , Dict , Type , TypeVar from taskcat._cli_core import GLOBAL_ARGS from taskcat._config import Config from ._abstract_test import Test T = TypeVar ( \"T\" , bound = \"BaseTest\" ) # pylint: disable=invalid-name class BaseTest ( Test ): \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ): self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception. Might be needed since # child objects rely on the configs uid. self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ): try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ): # we could optionally call self.report() on exiting. self . clean_up () @classmethod def from_file ( cls : Type [ T ], project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". input_file (str, optional): The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO: detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ], input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ { \"source\" : \"Manual\" , \"config\" : input_config }, { \"source\" : \"CliArgument\" , \"config\" : args }, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) def _build_args ( enable_sig_v2 , regions , default_profile ): args : Dict [ str , Any ] = {} if enable_sig_v2 : args [ \"project\" ] = { \"s3_enable_sig_v2\" : enable_sig_v2 } if regions != \"ALL\" : if \"project\" not in args : args [ \"project\" ] = {} args [ \"project\" ][ \"regions\" ] = regions . split ( \",\" ) if default_profile : _auth_dict = { \"default\" : default_profile } if not args . get ( \"project\" ): args [ \"project\" ] = { \"auth\" : _auth_dict } else : args [ \"project\" ][ \"auth\" ] = _auth_dict return args Variables T Classes BaseTest class BaseTest ( config : taskcat . _config . Config ) View Source class BaseTest ( Test ) : \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ) : self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception . Might be needed since # child objects rely on the configs uid . self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ) : try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ) : # we could optionally call self . report () on exiting . self . clean_up () @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) Ancestors (in MRO) taskcat.testing._abstract_test.Test abc.ABC Descendants taskcat.testing._cfn_test.CFNTest taskcat.testing._lint_test.LintTest taskcat.testing._unit_test.UnitTest Static methods from_dict def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) from_file def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) Instance variables config passed result Methods clean_up def clean_up ( self ) -> None Clean up after the Test. View Source @abstractmethod def clean_up ( self ) -> None : \"\"\"Clean up after the Test.\"\"\" run def run ( self ) -> None Run the Test. View Source @abstractmethod def run ( self ) -> None : \"\"\"Run the Test.\"\"\"","title":"Base Test"},{"location":"reference/taskcat/testing/base_test/#module-taskcattestingbase_test","text":"None None View Source # pylint: disable=line-too-long import uuid from pathlib import Path from typing import Any , Dict , Type , TypeVar from taskcat._cli_core import GLOBAL_ARGS from taskcat._config import Config from ._abstract_test import Test T = TypeVar ( \"T\" , bound = \"BaseTest\" ) # pylint: disable=invalid-name class BaseTest ( Test ): \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ): self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception. Might be needed since # child objects rely on the configs uid. self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ): try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ): # we could optionally call self.report() on exiting. self . clean_up () @classmethod def from_file ( cls : Type [ T ], project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". input_file (str, optional): The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO: detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ], input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ { \"source\" : \"Manual\" , \"config\" : input_config }, { \"source\" : \"CliArgument\" , \"config\" : args }, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) def _build_args ( enable_sig_v2 , regions , default_profile ): args : Dict [ str , Any ] = {} if enable_sig_v2 : args [ \"project\" ] = { \"s3_enable_sig_v2\" : enable_sig_v2 } if regions != \"ALL\" : if \"project\" not in args : args [ \"project\" ] = {} args [ \"project\" ][ \"regions\" ] = regions . split ( \",\" ) if default_profile : _auth_dict = { \"default\" : default_profile } if not args . get ( \"project\" ): args [ \"project\" ] = { \"auth\" : _auth_dict } else : args [ \"project\" ][ \"auth\" ] = _auth_dict return args","title":"Module taskcat.testing.base_test"},{"location":"reference/taskcat/testing/base_test/#variables","text":"T","title":"Variables"},{"location":"reference/taskcat/testing/base_test/#classes","text":"","title":"Classes"},{"location":"reference/taskcat/testing/base_test/#basetest","text":"class BaseTest ( config : taskcat . _config . Config ) View Source class BaseTest ( Test ) : \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ) : self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception . Might be needed since # child objects rely on the configs uid . self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ) : try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ) : # we could optionally call self . report () on exiting . self . clean_up () @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"BaseTest"},{"location":"reference/taskcat/testing/base_test/#ancestors-in-mro","text":"taskcat.testing._abstract_test.Test abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/testing/base_test/#descendants","text":"taskcat.testing._cfn_test.CFNTest taskcat.testing._lint_test.LintTest taskcat.testing._unit_test.UnitTest","title":"Descendants"},{"location":"reference/taskcat/testing/base_test/#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/testing/base_test/#from_dict","text":"def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"from_dict"},{"location":"reference/taskcat/testing/base_test/#from_file","text":"def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config )","title":"from_file"},{"location":"reference/taskcat/testing/base_test/#instance-variables","text":"config passed result","title":"Instance variables"},{"location":"reference/taskcat/testing/base_test/#methods","text":"","title":"Methods"},{"location":"reference/taskcat/testing/base_test/#clean_up","text":"def clean_up ( self ) -> None Clean up after the Test. View Source @abstractmethod def clean_up ( self ) -> None : \"\"\"Clean up after the Test.\"\"\"","title":"clean_up"},{"location":"reference/taskcat/testing/base_test/#run","text":"def run ( self ) -> None Run the Test. View Source @abstractmethod def run ( self ) -> None : \"\"\"Run the Test.\"\"\"","title":"run"}]}