{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"taskcat Installation Usage What is taskcat? taskcat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. taskcat is implemented as a Python class that you import, instantiate, and run. taskcat was developed by the AWS QuickStart team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions Note: taskcat has changed significantly in the 0.9.0 release, for details see Migrating from v0.8.x Support Installation Currently only installation via pip is supported. Installation via docker coming soon. Requirements The host taskcat is run on requires access to an AWS account, this can be done by any of the following mechanisms: Environment variables Shared credential file (~/.aws/credentials) AWS config file (~/.aws/config) Assume Role provider Boto2 config file (/etc/boto.cfg and ~/.boto) Instance metadata service on an Amazon EC2 instance that has an IAM role configured. for more info see the boto3 credential configuration documentation . Note: docker is only required if building lambda functions using a Dockerfile Installing via pip3 pip3 install taskcat Installing via pip3 --user will install taskcat into homedir, useful if you get permissions errors with the regular method pip3 install taskcat --user Note: the user install dir is platform specific For Example: (On Mac: ~/Library/Python/3.x/bin/taskcat) For Example: (On Linux: ~/.local/bin) Warning: Be sure to add the python bin dir to your $PATH Windows taskcat on Windows is not supported . If you are running Windows 10 we recommend that you install Windows Subsystem for Linux (WSL) and then install taskcat inside the WSL environment using the steps above. Usage CLI The cli is self documenting by using --help . The most common use of taskcat is for executing function tests of CloudFormation templates. The command for this is: taskcat test run add --help to see the supported flags and arguments Python Taskcat can be imported into Python and used in the testing framework of your choice. from taskcat.testing import CFNTest test = CFNTest . from_file ( project_root = './template_dir' ) with test as stacks : # Calling 'with' or 'test.run()' will deploy the stacks. for stack in stacks : print ( f \"Testing { stack . name } \" ) bucket_name = \"\" for output in stack . outputs : if output . key == \"LogsBucketName\" : bucket_name = output . value break assert \"logs\" in bucket_name assert stack . region . name in bucket_name print ( f \"Created bucket: { bucket_name } \" ) The example used here is very simple, you would most likely leverage other python modules like boto3 to do more advanced testing. The CFNTest object can be passed the same arguments as taskcat test run . See the docs for more details. Config files taskcat has several configuration files which can be used to set behaviors in a flexible way. Global config ~/.taskcat.yml provides global settings that become defaults for all projects. general General configuration settings. auth AWS authentication section <AUTH_NAME> parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_regional_buckets Boolean flag to upload the project to a bucket generated in each region where it will be deployed tags Tags to apply to CloudFormation template <TAG_NAME> Project config <PROJECT_ROOT>/.taskcat.yml provides project specific configuration. project Project specific configuration section auth AWS authentication section <AUTH_NAME> az_blacklist List of Availability Zones ID's to exclude when generating availability zones build_submodules Build Lambda zips recursively for submodules, set to false to disable lambda_source_path Path relative to the project root containing Lambda zip files, default is 'lambda_functions/source' lambda_zip_path Path relative to the project root to place Lambda zip files, default is 'lambda_functions/zips' name Project name, used as s3 key prefix when uploading objects owner email address for project owner (not used at present) package_lambda Package Lambda functions into zips before uploading to s3, set to false to disable parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> regions List of AWS regions s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_enable_sig_v2 Enable (deprecated) sigv2 access to auto-generated buckets s3_object_acl ACL for uploaded s3 objects, defaults to 'private' tags Tags to apply to CloudFormation template <TAG_NAME> template path to template file relative to the project config file path tests auth AWS authentication section <AUTH_NAME> az_blacklist List of Availability Zones ID's to exclude when generating availability zones parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> regions List of AWS regions s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated tags Tags to apply to CloudFormation template <TAG_NAME> template path to template file relative to the project config file path At minimum it must provide a project name, list of regions, template name and one test. Minimal example: project : name : my-cfn-project regions : - us-west-2 - eu-north-1 tests : default : template : ./templates/my-template.yaml Complete example with comments: tests/data/config_full_example/.taskcat.yml Parameter overrides a parameter override file can be created in <PROJECT_ROOT>/.taskcat_overrides.yml . Parameter Keys/Values specified in this file take precedence over values defined in all other configuration files. For example: KeyPair : my-overriden-keypair VpcId : vpc-1234abcd Warning: it is recommended to add .taskcat_overrides.yml to .gitignore to ensure it is not accidentally checked into source control Precedence With the exception of the parameters section, more specific config with the same key takes precedence. The rationale behind having parameters function this way is so that values can be overridden at a system level outside of a project, that is likely committed to source control. parameters that define account specific things like VPC details, Key Pairs, or secrets like API keys can be defined per host outside of source control. for example, consider this global config: ~/.taskcat.yml general : s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair given a simple project config: project : name : my-project regions : - us-east-2 tests : default : template : ./template.yaml the effective test configuration would become: tests : default : template : ./template.yaml s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair if any item is re-defined in a project it takes precedence over the global value. Anything defined in a test takes precedence over what is defined in the project or global configuration. with the exception of the parameters section which works in reverse. For example, using the same global config as above, given this project config: project : name : my-project regions : - us-east-2 s3_bucket : my-project-s3-bucket tests : default : template : ./template.yaml parameters : KeyPair : my-test-ec2-keypair would result in this effective test configuration: tests : default : template : ./template.yaml s3_bucket : my-project-s3-bucket parameters : KeyPair : my-global-ec2-keypair Notice that s3_bucket took the most specific value and KeyPair the most general. Migrating from 0.8.x taskcat 0.9.0 is a major re-write of the project and the opportunity was taken to modernise the cli interface update the config file format based on learnings from the previous releases. CLI interface taskcat adopts a similar cli command structure to git with a taskcat command subcommand --flag style. The cli is also designed to be simplest if run from the root of a project. Let's have a look at equivalent command to run a test: v0.8.x taskcat -c ./quickstart-aws-vpc/ci/taskcat.yml in v0.9.x you can cd into the project root for a very simple cli experience: cd ./quickstart-aws-vpc taskcat test run or run it from anywhere by providing the path to the project root taskcat test run -p ./quickstart-aws-vpc Non-standard credentials Taskcat leverages the credential mechanisms of the AWS CLI, with the exception of environment variables. To integrate advanced credential handling (such as AWS SSO), please see issue #596 for an example Configuration files The configuration files required for taskcat have changed, to ease migration, if taskcat is run and legacy config files are found, they are converted and written to new file locations. For more information on the new format, see the config file docs . GitHub: PyPi:","title":"Home"},{"location":"index.html#taskcat","text":"Installation Usage","title":"taskcat"},{"location":"index.html#what-is-taskcat","text":"taskcat is a tool that tests AWS CloudFormation templates. It deploys your AWS CloudFormation template in multiple AWS Regions and generates a report with a pass/fail grade for each region. You can specify the regions and number of Availability Zones you want to include in the test, and pass in parameter values from your AWS CloudFormation template. taskcat is implemented as a Python class that you import, instantiate, and run. taskcat was developed by the AWS QuickStart team to test AWS CloudFormation templates that automatically deploy workloads on AWS. We\u2019re pleased to make the tool available to all developers who want to validate their custom AWS CloudFormation templates across AWS Regions Note: taskcat has changed significantly in the 0.9.0 release, for details see Migrating from v0.8.x","title":"What is taskcat?"},{"location":"index.html#support","text":"","title":"Support"},{"location":"index.html#installation","text":"Currently only installation via pip is supported. Installation via docker coming soon.","title":"Installation"},{"location":"index.html#requirements","text":"The host taskcat is run on requires access to an AWS account, this can be done by any of the following mechanisms: Environment variables Shared credential file (~/.aws/credentials) AWS config file (~/.aws/config) Assume Role provider Boto2 config file (/etc/boto.cfg and ~/.boto) Instance metadata service on an Amazon EC2 instance that has an IAM role configured. for more info see the boto3 credential configuration documentation . Note: docker is only required if building lambda functions using a Dockerfile","title":"Requirements"},{"location":"index.html#installing-via-pip3","text":"pip3 install taskcat","title":"Installing via pip3"},{"location":"index.html#installing-via-pip3-user","text":"will install taskcat into homedir, useful if you get permissions errors with the regular method pip3 install taskcat --user Note: the user install dir is platform specific For Example: (On Mac: ~/Library/Python/3.x/bin/taskcat) For Example: (On Linux: ~/.local/bin) Warning: Be sure to add the python bin dir to your $PATH","title":"Installing via pip3 --user"},{"location":"index.html#windows","text":"taskcat on Windows is not supported . If you are running Windows 10 we recommend that you install Windows Subsystem for Linux (WSL) and then install taskcat inside the WSL environment using the steps above.","title":"Windows"},{"location":"index.html#usage","text":"","title":"Usage"},{"location":"index.html#cli","text":"The cli is self documenting by using --help . The most common use of taskcat is for executing function tests of CloudFormation templates. The command for this is: taskcat test run add --help to see the supported flags and arguments","title":"CLI"},{"location":"index.html#python","text":"Taskcat can be imported into Python and used in the testing framework of your choice. from taskcat.testing import CFNTest test = CFNTest . from_file ( project_root = './template_dir' ) with test as stacks : # Calling 'with' or 'test.run()' will deploy the stacks. for stack in stacks : print ( f \"Testing { stack . name } \" ) bucket_name = \"\" for output in stack . outputs : if output . key == \"LogsBucketName\" : bucket_name = output . value break assert \"logs\" in bucket_name assert stack . region . name in bucket_name print ( f \"Created bucket: { bucket_name } \" ) The example used here is very simple, you would most likely leverage other python modules like boto3 to do more advanced testing. The CFNTest object can be passed the same arguments as taskcat test run . See the docs for more details.","title":"Python"},{"location":"index.html#config-files","text":"taskcat has several configuration files which can be used to set behaviors in a flexible way.","title":"Config files"},{"location":"index.html#global-config","text":"~/.taskcat.yml provides global settings that become defaults for all projects. general General configuration settings. auth AWS authentication section <AUTH_NAME> parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_regional_buckets Boolean flag to upload the project to a bucket generated in each region where it will be deployed tags Tags to apply to CloudFormation template <TAG_NAME>","title":"Global config"},{"location":"index.html#project-config","text":"<PROJECT_ROOT>/.taskcat.yml provides project specific configuration. project Project specific configuration section auth AWS authentication section <AUTH_NAME> az_blacklist List of Availability Zones ID's to exclude when generating availability zones build_submodules Build Lambda zips recursively for submodules, set to false to disable lambda_source_path Path relative to the project root containing Lambda zip files, default is 'lambda_functions/source' lambda_zip_path Path relative to the project root to place Lambda zip files, default is 'lambda_functions/zips' name Project name, used as s3 key prefix when uploading objects owner email address for project owner (not used at present) package_lambda Package Lambda functions into zips before uploading to s3, set to false to disable parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> regions List of AWS regions s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_enable_sig_v2 Enable (deprecated) sigv2 access to auto-generated buckets s3_object_acl ACL for uploaded s3 objects, defaults to 'private' tags Tags to apply to CloudFormation template <TAG_NAME> template path to template file relative to the project config file path tests auth AWS authentication section <AUTH_NAME> az_blacklist List of Availability Zones ID's to exclude when generating availability zones parameters Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> regions List of AWS regions s3_bucket Name of S3 bucket to upload project to, if left out a bucket will be auto-generated tags Tags to apply to CloudFormation template <TAG_NAME> template path to template file relative to the project config file path At minimum it must provide a project name, list of regions, template name and one test. Minimal example: project : name : my-cfn-project regions : - us-west-2 - eu-north-1 tests : default : template : ./templates/my-template.yaml Complete example with comments: tests/data/config_full_example/.taskcat.yml","title":"Project config"},{"location":"index.html#parameter-overrides","text":"a parameter override file can be created in <PROJECT_ROOT>/.taskcat_overrides.yml . Parameter Keys/Values specified in this file take precedence over values defined in all other configuration files. For example: KeyPair : my-overriden-keypair VpcId : vpc-1234abcd Warning: it is recommended to add .taskcat_overrides.yml to .gitignore to ensure it is not accidentally checked into source control","title":"Parameter overrides"},{"location":"index.html#precedence","text":"With the exception of the parameters section, more specific config with the same key takes precedence. The rationale behind having parameters function this way is so that values can be overridden at a system level outside of a project, that is likely committed to source control. parameters that define account specific things like VPC details, Key Pairs, or secrets like API keys can be defined per host outside of source control. for example, consider this global config: ~/.taskcat.yml general : s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair given a simple project config: project : name : my-project regions : - us-east-2 tests : default : template : ./template.yaml the effective test configuration would become: tests : default : template : ./template.yaml s3_bucket : my-globally-defined-bucket parameters : KeyPair : my-global-ec2-keypair if any item is re-defined in a project it takes precedence over the global value. Anything defined in a test takes precedence over what is defined in the project or global configuration. with the exception of the parameters section which works in reverse. For example, using the same global config as above, given this project config: project : name : my-project regions : - us-east-2 s3_bucket : my-project-s3-bucket tests : default : template : ./template.yaml parameters : KeyPair : my-test-ec2-keypair would result in this effective test configuration: tests : default : template : ./template.yaml s3_bucket : my-project-s3-bucket parameters : KeyPair : my-global-ec2-keypair Notice that s3_bucket took the most specific value and KeyPair the most general.","title":"Precedence"},{"location":"index.html#migrating-from-08x","text":"taskcat 0.9.0 is a major re-write of the project and the opportunity was taken to modernise the cli interface update the config file format based on learnings from the previous releases.","title":"Migrating from 0.8.x"},{"location":"index.html#cli-interface","text":"taskcat adopts a similar cli command structure to git with a taskcat command subcommand --flag style. The cli is also designed to be simplest if run from the root of a project. Let's have a look at equivalent command to run a test: v0.8.x taskcat -c ./quickstart-aws-vpc/ci/taskcat.yml in v0.9.x you can cd into the project root for a very simple cli experience: cd ./quickstart-aws-vpc taskcat test run or run it from anywhere by providing the path to the project root taskcat test run -p ./quickstart-aws-vpc","title":"CLI interface"},{"location":"index.html#non-standard-credentials","text":"Taskcat leverages the credential mechanisms of the AWS CLI, with the exception of environment variables. To integrate advanced credential handling (such as AWS SSO), please see issue #596 for an example","title":"Non-standard credentials"},{"location":"index.html#configuration-files","text":"The configuration files required for taskcat have changed, to ease migration, if taskcat is run and legacy config files are found, they are converted and written to new file locations. For more information on the new format, see the config file docs . GitHub: PyPi:","title":"Configuration files"},{"location":"CODE_OF_CONDUCT.html","text":"Code of Conduct 1. Purpose A primary goal of TaskCat is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof). This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in TaskCat to help us create safe and positive experiences for everyone. 2. Open Source Citizenship A supplemental goal of this Code of Conduct is to increase open source citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community. Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know. 3. Expected Behavior The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential. Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations. 4. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people\u2019s personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Inappropriate physical contact. You should have someone\u2019s consent before touching them. Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. Sustained disruption of community events, including talks and presentations. 5. Consequences of Unacceptable Behavior Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event). 6. Reporting Guidelines If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. quickstart@amazon.com. Link to reporting guidelines: codeofconduct@amazon.com Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress. 7. Addressing Grievances If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify AWS Quickstart with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Policy 8. Scope We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business. This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members. 9. Contact info quickstart@amazon.com 10. License and attribution This Code of Conduct is distributed under a Creative Commons Attribution-ShareAlike license . Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy . Retrieved on November 22, 2016 from http://citizencodeofconduct.org/","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT.html#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT.html#1-purpose","text":"A primary goal of TaskCat is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof). This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior. We invite all those who participate in TaskCat to help us create safe and positive experiences for everyone.","title":"1. Purpose"},{"location":"CODE_OF_CONDUCT.html#2-open-source-citizenship","text":"A supplemental goal of this Code of Conduct is to increase open source citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community. Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know.","title":"2. Open Source Citizenship"},{"location":"CODE_OF_CONDUCT.html#3-expected-behavior","text":"The following behaviors are expected and requested of all community members: Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community. Exercise consideration and respect in your speech and actions. Attempt collaboration before conflict. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential. Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations.","title":"3. Expected Behavior"},{"location":"CODE_OF_CONDUCT.html#4-unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable within our community: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people\u2019s personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Inappropriate physical contact. You should have someone\u2019s consent before touching them. Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. Sustained disruption of community events, including talks and presentations.","title":"4. Unacceptable Behavior"},{"location":"CODE_OF_CONDUCT.html#5-consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event).","title":"5. Consequences of Unacceptable Behavior"},{"location":"CODE_OF_CONDUCT.html#6-reporting-guidelines","text":"If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. quickstart@amazon.com. Link to reporting guidelines: codeofconduct@amazon.com Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress.","title":"6. Reporting Guidelines"},{"location":"CODE_OF_CONDUCT.html#7-addressing-grievances","text":"If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify AWS Quickstart with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies. Policy","title":"7. Addressing Grievances"},{"location":"CODE_OF_CONDUCT.html#8-scope","text":"We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues\u2013online and in-person\u2013as well as in all one-on-one communications pertaining to community business. This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members.","title":"8. Scope"},{"location":"CODE_OF_CONDUCT.html#9-contact-info","text":"quickstart@amazon.com","title":"9. Contact info"},{"location":"CODE_OF_CONDUCT.html#10-license-and-attribution","text":"This Code of Conduct is distributed under a Creative Commons Attribution-ShareAlike license . Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy . Retrieved on November 22, 2016 from http://citizencodeofconduct.org/","title":"10. License and attribution"},{"location":"CONTRIBUTING.html","text":"Contributing Guidelines Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Documentation Links: Module Documentation User Guide Reporting Bugs/Feature Requests We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests (Pull request template provided) Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: All changes are staged into the develop branch (Send PR to the develop branch) You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Licensing We may ask you to affirm the Apache 2.0 agreement for larger changes.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING.html#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Documentation Links: Module Documentation User Guide","title":"Contributing Guidelines"},{"location":"CONTRIBUTING.html#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"CONTRIBUTING.html#contributing-via-pull-requests-pull-request-template-provided","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: All changes are staged into the develop branch (Send PR to the develop branch) You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests (Pull request template provided)"},{"location":"CONTRIBUTING.html#licensing","text":"We may ask you to affirm the Apache 2.0 agreement for larger changes.","title":"Licensing"},{"location":"FAQ.html","text":"TaskCat FAQ FAQ CommonErrors Error: botocore . exceptions . ClientError : An error occurred ( IllegalLocationConstraintException ) when calling the CreateBucket operation : The unspecified location constraint is incompatible for the region specific endpoint this request was sent to . Solution: Set your default region to us-east-1 For boto profile set the default to us-east-1 [profile default] output = json region = us-east-1 Critial failure with version all version below 2018.416.143234 Error: Traceback ( most recent call last ): File \"/var/lib/jenkins/.local/bin/taskcat\" , line 58 , in < module > main () File \"/var/lib/jenkins/.local/bin/taskcat\" , line 22 , in main tcat_instance . welcome ( 'taskcat' ) File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/taskcat/stacker.py\" , line 2192 , in welcome self . checkforupdate () File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/taskcat/stacker.py\" , line 2173 , in checkforupdate if version in current_version : TypeError : argument of type 'NoneType' is not iterable Due to infrastructure changes in https://pypi.org version check will fail for older versions :-( please update to latest version Solution: (Get latest version) To upgrade pip version [ pip install --upgrade taskcat] To upgrade docker version [ docker pull taskcat/taskcat ]","title":"FAQ"},{"location":"FAQ.html#taskcat-faq","text":"","title":"TaskCat FAQ"},{"location":"FAQ.html#faq","text":"","title":"FAQ"},{"location":"FAQ.html#commonerrors","text":"Error: botocore . exceptions . ClientError : An error occurred ( IllegalLocationConstraintException ) when calling the CreateBucket operation : The unspecified location constraint is incompatible for the region specific endpoint this request was sent to . Solution: Set your default region to us-east-1 For boto profile set the default to us-east-1 [profile default] output = json region = us-east-1","title":"CommonErrors"},{"location":"FAQ.html#critial-failure-with-version-all-version-below-2018416143234","text":"Error: Traceback ( most recent call last ): File \"/var/lib/jenkins/.local/bin/taskcat\" , line 58 , in < module > main () File \"/var/lib/jenkins/.local/bin/taskcat\" , line 22 , in main tcat_instance . welcome ( 'taskcat' ) File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/taskcat/stacker.py\" , line 2192 , in welcome self . checkforupdate () File \"/var/lib/jenkins/.local/lib/python3.6/site-packages/taskcat/stacker.py\" , line 2173 , in checkforupdate if version in current_version : TypeError : argument of type 'NoneType' is not iterable Due to infrastructure changes in https://pypi.org version check will fail for older versions :-( please update to latest version Solution: (Get latest version) To upgrade pip version [ pip install --upgrade taskcat] To upgrade docker version [ docker pull taskcat/taskcat ]","title":"Critial failure with version all version below 2018.416.143234"},{"location":"PULL_REQUEST_TEMPLATE.html","text":"Overview Brief description of what this PR does, and why it is needed (use case)? Testing/Steps taken to ensure quality How did you validate the changes in this PR? Notes Optional. Caveats, Alternatives, Other relevant information. Testing Instructions How to test this PR Start after checking out this branch (bulleted) * Include test case, and expected output","title":"PULL REQUEST TEMPLATE"},{"location":"PULL_REQUEST_TEMPLATE.html#overview","text":"Brief description of what this PR does, and why it is needed (use case)?","title":"Overview"},{"location":"PULL_REQUEST_TEMPLATE.html#testingsteps-taken-to-ensure-quality","text":"How did you validate the changes in this PR?","title":"Testing/Steps taken to ensure quality"},{"location":"PULL_REQUEST_TEMPLATE.html#notes","text":"Optional. Caveats, Alternatives, Other relevant information.","title":"Notes"},{"location":"PULL_REQUEST_TEMPLATE.html#testing-instructions","text":"How to test this PR Start after checking out this branch (bulleted) * Include test case, and expected output","title":"Testing Instructions"},{"location":"README.AMIUPDATER.html","text":"AMIUpdater README General Usage. amiupdater <flags> </path/to/template_directory|template_file> For a current list of options, see.. amiupdater -h Leveraging the Upstream Config File Upstream Mappings By default, AMIUpdater uses a config file bundled with taskcat . This config file is populated with common AMI Mappings, such as Amazon Linux AMI and Ubuntu Server 18.04 . To see all of the mappings available, check out the config file To utilize these upstream mappings, simply leverage them in your templates. Note: The AMI IDs are here for example purposes. When first configuring the Mapping, you can filll them with arbitrary data. JSON { (...) \"Mappings\" : { \"AWSAMIRegionMap\" : { \"ap-northeast-1\" : { \"AMZNLINUXHVM\" : \"ami-00a5245b4816c38e6\" , \"CENTOS7HVM\" : \"ami-8e8847f1\" , \"US1404HVM\" : \"ami-0be9269b44d4b26c1\" , \"US1604HVM\" : \"ami-0d5e82481c5fd4ad5\" , \"SLES15HVM\" : \"ami-09161bc9964f46a98\" }, \"ap-northeast-2\" : { \"AMZNLINUXHVM\" : \"ami-00dc207f8ba6dc919\" , \"CENTOS7HVM\" : \"ami-bf9c36d1\" , \"US1404HVM\" : \"ami-017332df4b882edd2\" , \"US1604HVM\" : \"ami-0507b772e2c9b8c15\" , \"SLES15HVM\" : \"ami-04ecb44b7d8e8d354\" }, \"ap-south-1\" : { \"AMZNLINUXHVM\" : \"ami-0ad42f4f66f6c1cc9\" , \"CENTOS7HVM\" : \"ami-1780a878\" , \"US1404HVM\" : \"ami-09dcf5653a185f5df\" , \"US1604HVM\" : \"ami-0c8810f694cbe10ba\" , \"SLES15HVM\" : \"ami-025d8258d76079367\" } (...) } } } } YAML Mappings : AWSAMIRegionMap : ap-northeast-1 : AMZNLINUXHVM : ami-00a5245b4816c38e6, CENTOS7HVM : ami-8e8847f1, US1404HVM : ami-0be9269b44d4b26c1, US1604HVM : ami-0d5e82481c5fd4ad5, SLES15HVM : ami-09161bc9964f46a98 ap-northeast-2 : AMZNLINUXHVM : ami-00dc207f8ba6dc919, CENTOS7HVM : ami-bf9c36d1, US1404HVM : ami-017332df4b882edd2, US1604HVM : ami-0507b772e2c9b8c15, SLES15HVM : ami-04ecb44b7d8e8d354 ap-south-1 : AMZNLINUXHVM : ami-0ad42f4f66f6c1cc9, CENTOS7HVM : ami-1780a878, US1404HVM : ami-09dcf5653a185f5df, US1604HVM : ami-0c8810f694cbe10ba, SLES15HVM : ami-025d8258d76079367 Defining your own AMI Mappings Custom Config File Functionally the same as the upstream config file, a local config file can be created and used in deployment pipelines. For a full list of filters, available, please see the AWS EC2 API Documentation . # Owner-id must be in quotes # Whereas, all other filters do not need quotes, # because they are not in a number format global : AMIs : CUSTOM_MAPPING_1 : name : my_super_awesome_name-* owner-id : 1234567890 CUSTOM_MAPPING_2 : name : my_super_other_awesome_name ???? * owner-id : 1234567890 architecture : arm64 Template Inline Config JSON \"Metadata\" : { \"AWSAMIRegionMap\" :{ \"Filters\" :{ \"<MAPPING_NAME>\" :{ \"name\" : \"my awesome AMI NAME\" , \"owner-id\" : \"01234567890\" } } } YAML Metadata : AWSAMIRegionMap : Filters : <MAPPING_NAME> : name : my awesome AMI NAME owner-id : 01234567890","title":"*AMIUpdater README*"},{"location":"README.AMIUPDATER.html#amiupdater-readme","text":"","title":"AMIUpdater README"},{"location":"README.AMIUPDATER.html#general-usage","text":"amiupdater <flags> </path/to/template_directory|template_file> For a current list of options, see.. amiupdater -h","title":"General Usage."},{"location":"README.AMIUPDATER.html#leveraging-the-upstream-config-file","text":"","title":"Leveraging the Upstream Config File"},{"location":"README.AMIUPDATER.html#upstream-mappings","text":"By default, AMIUpdater uses a config file bundled with taskcat . This config file is populated with common AMI Mappings, such as Amazon Linux AMI and Ubuntu Server 18.04 . To see all of the mappings available, check out the config file To utilize these upstream mappings, simply leverage them in your templates. Note: The AMI IDs are here for example purposes. When first configuring the Mapping, you can filll them with arbitrary data. JSON { (...) \"Mappings\" : { \"AWSAMIRegionMap\" : { \"ap-northeast-1\" : { \"AMZNLINUXHVM\" : \"ami-00a5245b4816c38e6\" , \"CENTOS7HVM\" : \"ami-8e8847f1\" , \"US1404HVM\" : \"ami-0be9269b44d4b26c1\" , \"US1604HVM\" : \"ami-0d5e82481c5fd4ad5\" , \"SLES15HVM\" : \"ami-09161bc9964f46a98\" }, \"ap-northeast-2\" : { \"AMZNLINUXHVM\" : \"ami-00dc207f8ba6dc919\" , \"CENTOS7HVM\" : \"ami-bf9c36d1\" , \"US1404HVM\" : \"ami-017332df4b882edd2\" , \"US1604HVM\" : \"ami-0507b772e2c9b8c15\" , \"SLES15HVM\" : \"ami-04ecb44b7d8e8d354\" }, \"ap-south-1\" : { \"AMZNLINUXHVM\" : \"ami-0ad42f4f66f6c1cc9\" , \"CENTOS7HVM\" : \"ami-1780a878\" , \"US1404HVM\" : \"ami-09dcf5653a185f5df\" , \"US1604HVM\" : \"ami-0c8810f694cbe10ba\" , \"SLES15HVM\" : \"ami-025d8258d76079367\" } (...) } } } } YAML Mappings : AWSAMIRegionMap : ap-northeast-1 : AMZNLINUXHVM : ami-00a5245b4816c38e6, CENTOS7HVM : ami-8e8847f1, US1404HVM : ami-0be9269b44d4b26c1, US1604HVM : ami-0d5e82481c5fd4ad5, SLES15HVM : ami-09161bc9964f46a98 ap-northeast-2 : AMZNLINUXHVM : ami-00dc207f8ba6dc919, CENTOS7HVM : ami-bf9c36d1, US1404HVM : ami-017332df4b882edd2, US1604HVM : ami-0507b772e2c9b8c15, SLES15HVM : ami-04ecb44b7d8e8d354 ap-south-1 : AMZNLINUXHVM : ami-0ad42f4f66f6c1cc9, CENTOS7HVM : ami-1780a878, US1404HVM : ami-09dcf5653a185f5df, US1604HVM : ami-0c8810f694cbe10ba, SLES15HVM : ami-025d8258d76079367","title":"Upstream Mappings"},{"location":"README.AMIUPDATER.html#defining-your-own-ami-mappings","text":"","title":"Defining your own AMI Mappings"},{"location":"README.AMIUPDATER.html#custom-config-file","text":"Functionally the same as the upstream config file, a local config file can be created and used in deployment pipelines. For a full list of filters, available, please see the AWS EC2 API Documentation . # Owner-id must be in quotes # Whereas, all other filters do not need quotes, # because they are not in a number format global : AMIs : CUSTOM_MAPPING_1 : name : my_super_awesome_name-* owner-id : 1234567890 CUSTOM_MAPPING_2 : name : my_super_other_awesome_name ???? * owner-id : 1234567890 architecture : arm64","title":"Custom Config File"},{"location":"README.AMIUPDATER.html#template-inline-config","text":"JSON \"Metadata\" : { \"AWSAMIRegionMap\" :{ \"Filters\" :{ \"<MAPPING_NAME>\" :{ \"name\" : \"my awesome AMI NAME\" , \"owner-id\" : \"01234567890\" } } } YAML Metadata : AWSAMIRegionMap : Filters : <MAPPING_NAME> : name : my awesome AMI NAME owner-id : 01234567890","title":"Template Inline Config"},{"location":"docs/schema/taskcat_schema.html","text":"general type: object General configuration settings. auth type: object AWS authentication section <AUTH_NAME> type: object parameters type: object Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> type: object posthooks type: array hooks to execute after executing tests prehooks type: array hooks to execute prior to executing tests regions type: array List of AWS regions s3_bucket type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_regional_buckets type: boolean Enable regional auto-buckets. tags type: object Tags to apply to CloudFormation template <TAG_NAME> type: object project type: object Project specific configuration section auth type: object AWS authentication section <AUTH_NAME> type: object az_blacklist type: array List of Availablilty Zones ID's to exclude when generating availability zones build_submodules type: boolean Build Lambda zips recursively for submodules, set to false to disable lambda_source_path type: string Path relative to the project root containing Lambda zip files, default is 'lambda_functions/source' lambda_zip_path type: string Path relative to the project root to place Lambda zip files, default is 'lambda_functions/zips' name type: string Project name, used as s3 key prefix when uploading objects owner type: string email address for project owner (not used at present) package_lambda type: boolean Package Lambda functions into zips before uploading to s3, set to false to disable parameters type: object Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> type: object posthooks type: array hooks to execute after executing tests prehooks type: array hooks to execute prior to executing tests regions type: array List of AWS regions role_name type: string Role name to use when launching CFN Stacks. s3_bucket type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_enable_sig_v2 type: boolean Enable (deprecated) sigv2 access to auto-generated buckets s3_object_acl type: string ACL for uploaded s3 objects, defaults to 'private' s3_regional_buckets type: boolean Enable regional auto-buckets. shorten_stack_name type: boolean Shorten stack names generated for tests, set to true to enable tags type: object Tags to apply to CloudFormation template <TAG_NAME> type: object template type: string path to template file relative to the project config file path tests type: object auth type: object AWS authentication section <AUTH_NAME> type: object az_blacklist type: array List of Availablilty Zones ID's to exclude when generating availability zones parameters type: object Parameter key-values to pass to CloudFormation, parameters provided in global config take precedence <PARAMETER_NAME> type: object posthooks type: array hooks to execute after executing tests prehooks type: array hooks to execute prior to executing tests regions type: array List of AWS regions role_name type: string Role name to use when launching CFN Stacks. s3_bucket type: string Name of S3 bucket to upload project to, if left out a bucket will be auto-generated s3_regional_buckets type: boolean Enable regional auto-buckets. stack_name type: string Cloudformation Stack Name stack_name_prefix type: string Prefix to apply to generated CFN Stack Name stack_name_suffix type: string Suffix to apply to generated CFN Stack Name tags type: object Tags to apply to CloudFormation template <TAG_NAME> type: object template type: string path to template file relative to the project config file path","title":"Schema"},{"location":"reference/_amiupdater.html","text":"Module _amiupdater None None View Source import dataclasses import datetime import logging import re from dataclasses import dataclass , field from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List , Set import pkg_resources import yaml # pylint: disable=wrong-import-order import dateutil.parser # pylint: disable=wrong-import-order from taskcat._cfn.template import Template as TCTemplate from taskcat._client_factory import Boto3Cache from taskcat._common_utils import deep_get , neglect_submodule_templates from taskcat._dataclasses import RegionObj from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) REGION_REGEX = re . compile ( \"((eu|ap|us|af|me|ca|cn|sa)-|(us-gov-))\" \"(north(east|west)?|south(east|west)?|central|east|west)-[0-9]\" , re . IGNORECASE , ) class Config : raw_dict : dict = { \"global\" : { \"AMIs\" : {}}} codenames : Set [ Dict [ str , str ]] = set () @classmethod def load ( cls , file_name , configtype = None ): with open ( file_name , \"r\" ) as _f : try : cls . raw_dict = yaml . safe_load ( _f ) except yaml . YAMLError as e : LOG . error ( f \"[ { file_name } ] - YAML Syntax Error!\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) try : for _x in cls . raw_dict . get ( \"global\" ) . get ( \"AMIs\" ) . keys (): cls . codenames . add ( _x ) except Exception as e : LOG . error ( f \" { configtype } config file [ { file_name } ]\" f \"is not structured properly!\" ) LOG . error ( f \" { e } \" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) @classmethod def update_filter ( cls , code_name ): cls . raw_dict [ \"global\" ][ \"AMIs\" ] . update ( code_name ) @classmethod def get_filter ( cls , code_name ): _x = deep_get ( cls . raw_dict , f \"global/AMIs/ { code_name } \" , {}) return { str ( k ): [ str ( v )] if isinstance ( v , str ) else list ( v ) for k , v in _x . items () } @dataclass class EC2FilterValue : # pylint: disable=invalid-name Name : str Values : List [ str ] @dataclass class APIResultsData : codename : str ami_id : str creation_date : int region : str custom_comparisons : bool = True def __lt__ ( self , other ): # See Codenames.parse_api_results for notes on why this is here. if self . custom_comparisons : return self . creation_date < other . creation_date return object . __lt__ ( self , other ) def __gt__ ( self , other ): # See Codenames.parse_api_results for notes on why this is here. if self . custom_comparisons : return self . creation_date > other . creation_date return object . __gt__ ( self , other ) @dataclass class RegionalCodename : # pylint: disable=invalid-name region : str cn : str new_ami : str = \"\" filters : list = field ( default_factory = list ) _creation_dt : datetime . datetime = field ( default_factory = datetime . datetime . now ) def __hash__ ( self ): return hash ( self . region + self . cn + self . new_ami + str ( self . filters )) class Template : def __init__ ( self , underlying : TCTemplate ): self . codenames : Set [ Dict [ str , str ]] = set () self . mapping_path : str = \"Mappings/AWSAMIRegionMap\" self . metadata_path : str = \"Metadata/AWSAMIRegionMap/Filters\" self . region_codename_lineno : Dict [ str , Dict [ str , int ]] = {} self . region_names : Set [ str ] = set () self . underlying : TCTemplate = underlying self . _ls = self . underlying . linesplit _template_regions = deep_get ( self . underlying . template , self . mapping_path , {}) for region_name , region_data in _template_regions . items (): if region_name == \"AMI\" : continue self . region_names . add ( region_name ) for codename , cnvalue in region_data . items (): key = f \" { codename } / { region_name } \" line_no = codename . start_mark . line if cnvalue == \"\" : if '\"\"' in self . _ls [ line_no ]: cnvalue = '\"\"' elif \"''\" in self . _ls [ line_no ]: cnvalue = \"''\" self . region_codename_lineno [ key ] = { \"line\" : line_no , \"old\" : cnvalue } def set_codename_ami ( self , cname , region , new_ami ): if region not in self . region_names : return False key = f \" { cname } / { region } \" try : line_no = self . region_codename_lineno [ key ][ \"line\" ] old_ami = self . region_codename_lineno [ key ][ \"old\" ] if old_ami == new_ami : return False except KeyError : return False if old_ami == '\"\"' : new_ami = f '\" { new_ami } \"' new_record = re . sub ( old_ami , new_ami , self . _ls [ line_no ]) self . _ls [ line_no ] = new_record return True def write ( self ): self . underlying . raw_template = \" \\n \" . join ( self . _ls ) self . underlying . write () class AMIUpdaterFatalException ( TaskCatException ): \"\"\"Raised when AMIUpdater experiences a fatal error\"\"\" def __init__ ( self , message = None ): # pylint: disable=super-with-arguments super ( AMIUpdaterFatalException , self ) . __init__ ( message ) self . message = message class AMIUpdaterCommitNeededException ( TaskCatException ): def __init__ ( self , message = None ): # pylint: disable=super-with-arguments super ( AMIUpdaterCommitNeededException , self ) . __init__ ( message ) self . message = message def _construct_filters ( cname : str , config : Config ) -> List [ EC2FilterValue ]: formatted_filters : List [ EC2FilterValue ] = [] fetched_filters = config . get_filter ( cname ) formatted_filters = [ EC2FilterValue ( k , v ) for k , v in fetched_filters . items ()] if formatted_filters : formatted_filters . append ( EC2FilterValue ( \"state\" , [ \"available\" ])) return formatted_filters def build_codenames ( tobj : Template , config : Config ) -> List [ RegionalCodename ]: \"\"\"Builds regional codename objects\"\"\" built_cn = [] filters = deep_get ( tobj . underlying . template , tobj . metadata_path , {}) mappings = deep_get ( tobj . underlying . template , tobj . mapping_path , {}) for cname , cfilters in filters . items (): config . update_filter ({ cname : cfilters }) for region , cndata in mappings . items (): _missing_filters : Set [ str ] = set () if region == \"AMI\" : continue if not REGION_REGEX . search ( region ): LOG . error ( f \"[ { region } ] is not a valid region. Please check your template!\" ) raise AMIUpdaterFatalException for cnname in cndata . keys (): _filters = _construct_filters ( cnname , config ) if not _filters : if cnname not in _missing_filters : _missing_filters . add ( cnname ) LOG . warning ( f \"No query parameters were found for: { cnname . upper () } .\" , \"(Results for this codename are not possible.\" , ) continue region_cn = RegionalCodename ( region = region , cn = cnname , filters = _filters ) built_cn . append ( region_cn ) return built_cn def _per_codename_amifetch ( region_dict , regional_cn ): new_filters = [] for _filter in regional_cn . filters : new_filters . append ( dataclasses . asdict ( _filter )) _r = region_dict . get ( regional_cn . region ) image_results = [] if _r : image_results = _r . client ( \"ec2\" ) . describe_images ( Filters = new_filters )[ \"Images\" ] return { \"region\" : regional_cn . region , \"cn\" : regional_cn . cn , \"api_results\" : image_results , } def query_codenames ( codename_list : Set [ RegionalCodename ], region_dict : Dict [ str , RegionObj ] ): \"\"\"Fetches AMI IDs from AWS\"\"\" if len ( codename_list ) == 0 : raise AMIUpdaterFatalException ( \"No AMI filters were found. Nothing to fetch from the EC2 API.\" ) for region in list ( region_dict . keys ()): _ = region_dict [ region ] . client ( \"ec2\" ) pool = ThreadPool ( len ( region_dict )) _p = partial ( _per_codename_amifetch , region_dict ) response = pool . map ( _p , codename_list ) return response def _image_timestamp ( raw_ts ): return int ( dateutil . parser . parse ( raw_ts ) . timestamp ()) def reduce_api_results ( raw_results ): unsorted_results = [] missing_results = [] final_results = [] result_state = {} for query_result in raw_results : if query_result [ \"api_results\" ]: cn_api_results_data = [ APIResultsData ( query_result [ \"cn\" ], x [ \"ImageId\" ], _image_timestamp ( x [ \"CreationDate\" ]), query_result [ \"region\" ], ) for x in query_result [ \"api_results\" ] ] unsorted_results = cn_api_results_data + unsorted_results else : missing_results . append ( query_result ) if missing_results : LOG . warning ( \"No results were available for the following CODENAME / Region combination\" ) for missing_result in missing_results : LOG . warning ( f \"- { missing_result [ 'cn' ] } in { missing_result [ 'region' ] } \" ) sorted_results = sorted ( unsorted_results , reverse = True ) for _r in sorted_results : found_key = f \" { _r . region } - { _r . codename } \" already_found = result_state . get ( found_key , False ) if already_found : continue result_state [ found_key ] = True final_results . append ( _r ) return final_results class AMIUpdater : upstream_config_file = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/amiupdater.cfg.yml\" ) upstream_config_file_url = ( \"https://raw.githubusercontent.com/aws-quickstart/\" \"taskcat/main/cfg/amiupdater.cfg.yml\" ) def __init__ ( self , config , user_config_file = None , use_upstream_mappings = True ): if use_upstream_mappings : Config . load ( self . upstream_config_file , configtype = \"Upstream\" ) if user_config_file : Config . load ( user_config_file , configtype = \"User\" ) # TODO: Needed? self . config = config self . boto3_cache = Boto3Cache () self . template_list = self . _determine_templates () self . regions = self . _get_regions () def _get_regions ( self ): profile = ( self . config . config . general . auth . get ( \"default\" , \"default\" ) if self . config . config . general . auth else \"default\" ) default_region = self . boto3_cache . get_default_region ( profile ) regions = [ _r [ \"RegionName\" ] for _r in self . boto3_cache . client ( \"ec2\" , profile , default_region ) . describe_regions ()[ \"Regions\" ] ] regions = self . get_regions_for_profile ( profile , regions ) if self . config . config . general . auth : for region , profile in self . config . config . general . auth . items (): regions . update ( self . get_regions_for_profile ( profile , [ region ])) return regions def get_regions_for_profile ( self , profile , _regions ): regions = {} for _r in _regions : regions [ _r ] = RegionObj ( name = _r , account_id = self . boto3_cache . account_id ( profile ), partition = self . boto3_cache . partition ( profile ), profile = profile , _boto3_cache = self . boto3_cache , taskcat_id = self . config . uid , _role_name = None , ) return regions def _determine_templates ( self ): _up = self . config . get_templates () unprocessed_templates = list ( _up . values ()) finalized_templates = neglect_submodule_templates ( project_root = self . config . project_root , template_list = unprocessed_templates ) return finalized_templates def _determine_templates_regions ( self ): templates = [] for tc_template in self . template_list : _t = Template ( underlying = tc_template ) templates . append ( _t ) return templates def update_amis ( self ): codenames = set () LOG . info ( \"Determining templates and supported regions\" ) templates = self . _determine_templates_regions () LOG . info ( \"Determining regional search params for each AMI\" ) # Flush out codenames. for template in templates : template_cn = build_codenames ( template , Config ) for tcn in template_cn : codenames . add ( tcn ) # Retrieve API Results. LOG . info ( \"Retrieving results from the EC2 API\" ) results = query_codenames ( codenames , self . regions ) LOG . info ( \"Determining the latest AMI for each Codename/Region\" ) updated_api_results = reduce_api_results ( results ) # Figure out a way to sort dictionary by key-value (timestmap) _write_template = False for template in templates : for result in updated_api_results : changed = template . set_codename_ami ( result . codename , result . region , result . ami_id ) if changed : _write_template = True if _write_template : template . write () if _write_template : LOG . info ( \"Templates updated\" ) raise AMIUpdaterCommitNeededException LOG . info ( \"No AMI's needed updates.\" ) Variables LOG REGION_REGEX Functions build_codenames def build_codenames ( tobj : _amiupdater . Template , config : _amiupdater . Config ) -> List [ _amiupdater . RegionalCodename ] Builds regional codename objects View Source def build_codenames ( tobj : Template , config : Config ) -> List [ RegionalCodename ] : \"\"\"Builds regional codename objects\"\"\" built_cn = [] filters = deep_get ( tobj . underlying . template , tobj . metadata_path , {} ) mappings = deep_get ( tobj . underlying . template , tobj . mapping_path , {} ) for cname , cfilters in filters . items () : config . update_filter ( { cname : cfilters } ) for region , cndata in mappings . items () : _missing_filters : Set [ str ] = set () if region == \"AMI\" : continue if not REGION_REGEX . search ( region ) : LOG . error ( f \"[{region}] is not a valid region. Please check your template!\" ) raise AMIUpdaterFatalException for cnname in cndata . keys () : _filters = _construct_filters ( cnname , config ) if not _filters : if cnname not in _missing_filters : _missing_filters . add ( cnname ) LOG . warning ( f \"No query parameters were found for: {cnname.upper()}.\" , \"(Results for this codename are not possible.\" , ) continue region_cn = RegionalCodename ( region = region , cn = cnname , filters = _filters ) built_cn . append ( region_cn ) return built_cn query_codenames def query_codenames ( codename_list : Set [ _amiupdater . RegionalCodename ], region_dict : Dict [ str , taskcat . _dataclasses . RegionObj ] ) Fetches AMI IDs from AWS View Source def query_codenames ( codename_list : Set [ RegionalCodename ] , region_dict : Dict [ str, RegionObj ] ) : \"\"\"Fetches AMI IDs from AWS\"\"\" if len ( codename_list ) == 0 : raise AMIUpdaterFatalException ( \"No AMI filters were found. Nothing to fetch from the EC2 API.\" ) for region in list ( region_dict . keys ()) : _ = region_dict [ region ] . client ( \"ec2\" ) pool = ThreadPool ( len ( region_dict )) _p = partial ( _per_codename_amifetch , region_dict ) response = pool . map ( _p , codename_list ) return response reduce_api_results def reduce_api_results ( raw_results ) View Source def reduce_api_results ( raw_results ) : unsorted_results = [] missing_results = [] final_results = [] result_state = {} for query_result in raw_results : if query_result [ \"api_results\" ] : cn_api_results_data = [ APIResultsData( query_result[\"cn\" ] , x [ \"ImageId\" ] , _image_timestamp ( x [ \"CreationDate\" ] ), query_result [ \"region\" ] , ) for x in query_result [ \"api_results\" ] ] unsorted_results = cn_api_results_data + unsorted_results else : missing_results . append ( query_result ) if missing_results : LOG . warning ( \"No results were available for the following CODENAME / Region combination\" ) for missing_result in missing_results : LOG . warning ( f \"- {missing_result['cn']} in {missing_result['region']}\" ) sorted_results = sorted ( unsorted_results , reverse = True ) for _r in sorted_results : found_key = f \"{_r.region}-{_r.codename}\" already_found = result_state . get ( found_key , False ) if already_found : continue result_state [ found_key ] = True final_results . append ( _r ) return final_results Classes AMIUpdater class AMIUpdater ( config , user_config_file = None , use_upstream_mappings = True ) Class variables upstream_config_file upstream_config_file_url Methods get_regions_for_profile def get_regions_for_profile ( self , profile , _regions ) View Source def get_regions_for_profile ( self , profile , _regions ) : regions = {} for _r in _regions : regions [ _r ] = RegionObj ( name = _r , account_id = self . boto3_cache . account_id ( profile ), partition = self . boto3_cache . partition ( profile ), profile = profile , _boto3_cache = self . boto3_cache , taskcat_id = self . config . uid , _role_name = None , ) return regions update_amis def update_amis ( self ) View Source def update_amis ( self ) : codenames = set () LOG . info ( \"Determining templates and supported regions\" ) templates = self . _determine_templates_regions () LOG . info ( \"Determining regional search params for each AMI\" ) # Flush out codenames . for template in templates: template_cn = build_codenames ( template , Config ) for tcn in template_cn: codenames . add ( tcn ) # Retrieve API Results . LOG . info ( \"Retrieving results from the EC2 API\" ) results = query_codenames ( codenames , self . regions ) LOG . info ( \"Determining the latest AMI for each Codename/Region\" ) updated_api_results = reduce_api_results ( results ) # Figure out a way to sort dictionary by key - value ( timestmap ) _write_template = False for template in templates: for result in updated_api_results: changed = template . set_codename_ami ( result . codename , result . region , result . ami_id ) if changed: _write_template = True if _write_template: template . write () if _write_template: LOG . info ( \"Templates updated\" ) raise AMIUpdaterCommitNeededException LOG . info ( \"No AMI's needed updates.\" ) AMIUpdaterCommitNeededException class AMIUpdaterCommitNeededException ( message = None ) Ancestors (in MRO) taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. AMIUpdaterFatalException class AMIUpdaterFatalException ( message = None ) Ancestors (in MRO) taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. APIResultsData class APIResultsData ( codename : str , ami_id : str , creation_date : int , region : str , custom_comparisons : bool = True ) Class variables custom_comparisons Config class Config ( / , * args , ** kwargs ) Class variables codenames raw_dict Static methods get_filter def get_filter ( code_name ) View Source @classmethod def get_filter ( cls , code_name ) : _x = deep_get ( cls . raw_dict , f \"global/AMIs/{code_name}\" , {} ) return { str ( k ) : [ str(v) ] if isinstance ( v , str ) else list ( v ) for k , v in _x . items () } load def load ( file_name , configtype = None ) View Source @ classmethod def load ( cls , file_name , configtype = None ): with open ( file_name , \"r\" ) as _f : try : cls . raw_dict = yaml . safe_load ( _f ) except yaml . YAMLError as e : LOG . error ( f \"[{file_name}] - YAML Syntax Error!\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) try : for _x in cls . raw_dict . get ( \"global\" ) . get ( \"AMIs\" ) . keys (): cls . codenames . add ( _x ) except Exception as e : LOG . error ( f \"{configtype} config file [{file_name}]\" f \"is not structured properly!\" ) LOG . error ( f \"{e}\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) update_filter def update_filter ( code_name ) View Source @classmethod def update_filter ( cls , code_name ) : cls . raw_dict [ \"global\" ][ \"AMIs\" ] . update ( code_name ) EC2FilterValue class EC2FilterValue ( Name : str , Values : List [ str ] ) RegionalCodename class RegionalCodename ( region : str , cn : str , new_ami : str = '' , filters : list = < factory > , _creation_dt : datetime . datetime = < factory > ) Class variables new_ami Template class Template ( underlying : taskcat . _cfn . template . Template ) Methods set_codename_ami def set_codename_ami ( self , cname , region , new_ami ) View Source def set_codename_ami ( self , cname , region , new_ami ) : if region not in self . region_names : return False key = f \"{cname}/{region}\" try : line_no = self . region_codename_lineno [ key ][ \"line\" ] old_ami = self . region_codename_lineno [ key ][ \"old\" ] if old_ami == new_ami : return False except KeyError : return False if old_ami == '\"\"' : new_ami = f '\"{new_ami}\"' new_record = re . sub ( old_ami , new_ami , self . _ls [ line_no ] ) self . _ls [ line_no ] = new_record return True write def write ( self ) View Source def write(self): self.underlying.raw_template = \"\\n\".join(self._ls) self.underlying.write()","title":" Amiupdater"},{"location":"reference/_amiupdater.html#module-_amiupdater","text":"None None View Source import dataclasses import datetime import logging import re from dataclasses import dataclass , field from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List , Set import pkg_resources import yaml # pylint: disable=wrong-import-order import dateutil.parser # pylint: disable=wrong-import-order from taskcat._cfn.template import Template as TCTemplate from taskcat._client_factory import Boto3Cache from taskcat._common_utils import deep_get , neglect_submodule_templates from taskcat._dataclasses import RegionObj from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) REGION_REGEX = re . compile ( \"((eu|ap|us|af|me|ca|cn|sa)-|(us-gov-))\" \"(north(east|west)?|south(east|west)?|central|east|west)-[0-9]\" , re . IGNORECASE , ) class Config : raw_dict : dict = { \"global\" : { \"AMIs\" : {}}} codenames : Set [ Dict [ str , str ]] = set () @classmethod def load ( cls , file_name , configtype = None ): with open ( file_name , \"r\" ) as _f : try : cls . raw_dict = yaml . safe_load ( _f ) except yaml . YAMLError as e : LOG . error ( f \"[ { file_name } ] - YAML Syntax Error!\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) try : for _x in cls . raw_dict . get ( \"global\" ) . get ( \"AMIs\" ) . keys (): cls . codenames . add ( _x ) except Exception as e : LOG . error ( f \" { configtype } config file [ { file_name } ]\" f \"is not structured properly!\" ) LOG . error ( f \" { e } \" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) @classmethod def update_filter ( cls , code_name ): cls . raw_dict [ \"global\" ][ \"AMIs\" ] . update ( code_name ) @classmethod def get_filter ( cls , code_name ): _x = deep_get ( cls . raw_dict , f \"global/AMIs/ { code_name } \" , {}) return { str ( k ): [ str ( v )] if isinstance ( v , str ) else list ( v ) for k , v in _x . items () } @dataclass class EC2FilterValue : # pylint: disable=invalid-name Name : str Values : List [ str ] @dataclass class APIResultsData : codename : str ami_id : str creation_date : int region : str custom_comparisons : bool = True def __lt__ ( self , other ): # See Codenames.parse_api_results for notes on why this is here. if self . custom_comparisons : return self . creation_date < other . creation_date return object . __lt__ ( self , other ) def __gt__ ( self , other ): # See Codenames.parse_api_results for notes on why this is here. if self . custom_comparisons : return self . creation_date > other . creation_date return object . __gt__ ( self , other ) @dataclass class RegionalCodename : # pylint: disable=invalid-name region : str cn : str new_ami : str = \"\" filters : list = field ( default_factory = list ) _creation_dt : datetime . datetime = field ( default_factory = datetime . datetime . now ) def __hash__ ( self ): return hash ( self . region + self . cn + self . new_ami + str ( self . filters )) class Template : def __init__ ( self , underlying : TCTemplate ): self . codenames : Set [ Dict [ str , str ]] = set () self . mapping_path : str = \"Mappings/AWSAMIRegionMap\" self . metadata_path : str = \"Metadata/AWSAMIRegionMap/Filters\" self . region_codename_lineno : Dict [ str , Dict [ str , int ]] = {} self . region_names : Set [ str ] = set () self . underlying : TCTemplate = underlying self . _ls = self . underlying . linesplit _template_regions = deep_get ( self . underlying . template , self . mapping_path , {}) for region_name , region_data in _template_regions . items (): if region_name == \"AMI\" : continue self . region_names . add ( region_name ) for codename , cnvalue in region_data . items (): key = f \" { codename } / { region_name } \" line_no = codename . start_mark . line if cnvalue == \"\" : if '\"\"' in self . _ls [ line_no ]: cnvalue = '\"\"' elif \"''\" in self . _ls [ line_no ]: cnvalue = \"''\" self . region_codename_lineno [ key ] = { \"line\" : line_no , \"old\" : cnvalue } def set_codename_ami ( self , cname , region , new_ami ): if region not in self . region_names : return False key = f \" { cname } / { region } \" try : line_no = self . region_codename_lineno [ key ][ \"line\" ] old_ami = self . region_codename_lineno [ key ][ \"old\" ] if old_ami == new_ami : return False except KeyError : return False if old_ami == '\"\"' : new_ami = f '\" { new_ami } \"' new_record = re . sub ( old_ami , new_ami , self . _ls [ line_no ]) self . _ls [ line_no ] = new_record return True def write ( self ): self . underlying . raw_template = \" \\n \" . join ( self . _ls ) self . underlying . write () class AMIUpdaterFatalException ( TaskCatException ): \"\"\"Raised when AMIUpdater experiences a fatal error\"\"\" def __init__ ( self , message = None ): # pylint: disable=super-with-arguments super ( AMIUpdaterFatalException , self ) . __init__ ( message ) self . message = message class AMIUpdaterCommitNeededException ( TaskCatException ): def __init__ ( self , message = None ): # pylint: disable=super-with-arguments super ( AMIUpdaterCommitNeededException , self ) . __init__ ( message ) self . message = message def _construct_filters ( cname : str , config : Config ) -> List [ EC2FilterValue ]: formatted_filters : List [ EC2FilterValue ] = [] fetched_filters = config . get_filter ( cname ) formatted_filters = [ EC2FilterValue ( k , v ) for k , v in fetched_filters . items ()] if formatted_filters : formatted_filters . append ( EC2FilterValue ( \"state\" , [ \"available\" ])) return formatted_filters def build_codenames ( tobj : Template , config : Config ) -> List [ RegionalCodename ]: \"\"\"Builds regional codename objects\"\"\" built_cn = [] filters = deep_get ( tobj . underlying . template , tobj . metadata_path , {}) mappings = deep_get ( tobj . underlying . template , tobj . mapping_path , {}) for cname , cfilters in filters . items (): config . update_filter ({ cname : cfilters }) for region , cndata in mappings . items (): _missing_filters : Set [ str ] = set () if region == \"AMI\" : continue if not REGION_REGEX . search ( region ): LOG . error ( f \"[ { region } ] is not a valid region. Please check your template!\" ) raise AMIUpdaterFatalException for cnname in cndata . keys (): _filters = _construct_filters ( cnname , config ) if not _filters : if cnname not in _missing_filters : _missing_filters . add ( cnname ) LOG . warning ( f \"No query parameters were found for: { cnname . upper () } .\" , \"(Results for this codename are not possible.\" , ) continue region_cn = RegionalCodename ( region = region , cn = cnname , filters = _filters ) built_cn . append ( region_cn ) return built_cn def _per_codename_amifetch ( region_dict , regional_cn ): new_filters = [] for _filter in regional_cn . filters : new_filters . append ( dataclasses . asdict ( _filter )) _r = region_dict . get ( regional_cn . region ) image_results = [] if _r : image_results = _r . client ( \"ec2\" ) . describe_images ( Filters = new_filters )[ \"Images\" ] return { \"region\" : regional_cn . region , \"cn\" : regional_cn . cn , \"api_results\" : image_results , } def query_codenames ( codename_list : Set [ RegionalCodename ], region_dict : Dict [ str , RegionObj ] ): \"\"\"Fetches AMI IDs from AWS\"\"\" if len ( codename_list ) == 0 : raise AMIUpdaterFatalException ( \"No AMI filters were found. Nothing to fetch from the EC2 API.\" ) for region in list ( region_dict . keys ()): _ = region_dict [ region ] . client ( \"ec2\" ) pool = ThreadPool ( len ( region_dict )) _p = partial ( _per_codename_amifetch , region_dict ) response = pool . map ( _p , codename_list ) return response def _image_timestamp ( raw_ts ): return int ( dateutil . parser . parse ( raw_ts ) . timestamp ()) def reduce_api_results ( raw_results ): unsorted_results = [] missing_results = [] final_results = [] result_state = {} for query_result in raw_results : if query_result [ \"api_results\" ]: cn_api_results_data = [ APIResultsData ( query_result [ \"cn\" ], x [ \"ImageId\" ], _image_timestamp ( x [ \"CreationDate\" ]), query_result [ \"region\" ], ) for x in query_result [ \"api_results\" ] ] unsorted_results = cn_api_results_data + unsorted_results else : missing_results . append ( query_result ) if missing_results : LOG . warning ( \"No results were available for the following CODENAME / Region combination\" ) for missing_result in missing_results : LOG . warning ( f \"- { missing_result [ 'cn' ] } in { missing_result [ 'region' ] } \" ) sorted_results = sorted ( unsorted_results , reverse = True ) for _r in sorted_results : found_key = f \" { _r . region } - { _r . codename } \" already_found = result_state . get ( found_key , False ) if already_found : continue result_state [ found_key ] = True final_results . append ( _r ) return final_results class AMIUpdater : upstream_config_file = pkg_resources . resource_filename ( \"taskcat\" , \"/cfg/amiupdater.cfg.yml\" ) upstream_config_file_url = ( \"https://raw.githubusercontent.com/aws-quickstart/\" \"taskcat/main/cfg/amiupdater.cfg.yml\" ) def __init__ ( self , config , user_config_file = None , use_upstream_mappings = True ): if use_upstream_mappings : Config . load ( self . upstream_config_file , configtype = \"Upstream\" ) if user_config_file : Config . load ( user_config_file , configtype = \"User\" ) # TODO: Needed? self . config = config self . boto3_cache = Boto3Cache () self . template_list = self . _determine_templates () self . regions = self . _get_regions () def _get_regions ( self ): profile = ( self . config . config . general . auth . get ( \"default\" , \"default\" ) if self . config . config . general . auth else \"default\" ) default_region = self . boto3_cache . get_default_region ( profile ) regions = [ _r [ \"RegionName\" ] for _r in self . boto3_cache . client ( \"ec2\" , profile , default_region ) . describe_regions ()[ \"Regions\" ] ] regions = self . get_regions_for_profile ( profile , regions ) if self . config . config . general . auth : for region , profile in self . config . config . general . auth . items (): regions . update ( self . get_regions_for_profile ( profile , [ region ])) return regions def get_regions_for_profile ( self , profile , _regions ): regions = {} for _r in _regions : regions [ _r ] = RegionObj ( name = _r , account_id = self . boto3_cache . account_id ( profile ), partition = self . boto3_cache . partition ( profile ), profile = profile , _boto3_cache = self . boto3_cache , taskcat_id = self . config . uid , _role_name = None , ) return regions def _determine_templates ( self ): _up = self . config . get_templates () unprocessed_templates = list ( _up . values ()) finalized_templates = neglect_submodule_templates ( project_root = self . config . project_root , template_list = unprocessed_templates ) return finalized_templates def _determine_templates_regions ( self ): templates = [] for tc_template in self . template_list : _t = Template ( underlying = tc_template ) templates . append ( _t ) return templates def update_amis ( self ): codenames = set () LOG . info ( \"Determining templates and supported regions\" ) templates = self . _determine_templates_regions () LOG . info ( \"Determining regional search params for each AMI\" ) # Flush out codenames. for template in templates : template_cn = build_codenames ( template , Config ) for tcn in template_cn : codenames . add ( tcn ) # Retrieve API Results. LOG . info ( \"Retrieving results from the EC2 API\" ) results = query_codenames ( codenames , self . regions ) LOG . info ( \"Determining the latest AMI for each Codename/Region\" ) updated_api_results = reduce_api_results ( results ) # Figure out a way to sort dictionary by key-value (timestmap) _write_template = False for template in templates : for result in updated_api_results : changed = template . set_codename_ami ( result . codename , result . region , result . ami_id ) if changed : _write_template = True if _write_template : template . write () if _write_template : LOG . info ( \"Templates updated\" ) raise AMIUpdaterCommitNeededException LOG . info ( \"No AMI's needed updates.\" )","title":"Module _amiupdater"},{"location":"reference/_amiupdater.html#variables","text":"LOG REGION_REGEX","title":"Variables"},{"location":"reference/_amiupdater.html#functions","text":"","title":"Functions"},{"location":"reference/_amiupdater.html#build_codenames","text":"def build_codenames ( tobj : _amiupdater . Template , config : _amiupdater . Config ) -> List [ _amiupdater . RegionalCodename ] Builds regional codename objects View Source def build_codenames ( tobj : Template , config : Config ) -> List [ RegionalCodename ] : \"\"\"Builds regional codename objects\"\"\" built_cn = [] filters = deep_get ( tobj . underlying . template , tobj . metadata_path , {} ) mappings = deep_get ( tobj . underlying . template , tobj . mapping_path , {} ) for cname , cfilters in filters . items () : config . update_filter ( { cname : cfilters } ) for region , cndata in mappings . items () : _missing_filters : Set [ str ] = set () if region == \"AMI\" : continue if not REGION_REGEX . search ( region ) : LOG . error ( f \"[{region}] is not a valid region. Please check your template!\" ) raise AMIUpdaterFatalException for cnname in cndata . keys () : _filters = _construct_filters ( cnname , config ) if not _filters : if cnname not in _missing_filters : _missing_filters . add ( cnname ) LOG . warning ( f \"No query parameters were found for: {cnname.upper()}.\" , \"(Results for this codename are not possible.\" , ) continue region_cn = RegionalCodename ( region = region , cn = cnname , filters = _filters ) built_cn . append ( region_cn ) return built_cn","title":"build_codenames"},{"location":"reference/_amiupdater.html#query_codenames","text":"def query_codenames ( codename_list : Set [ _amiupdater . RegionalCodename ], region_dict : Dict [ str , taskcat . _dataclasses . RegionObj ] ) Fetches AMI IDs from AWS View Source def query_codenames ( codename_list : Set [ RegionalCodename ] , region_dict : Dict [ str, RegionObj ] ) : \"\"\"Fetches AMI IDs from AWS\"\"\" if len ( codename_list ) == 0 : raise AMIUpdaterFatalException ( \"No AMI filters were found. Nothing to fetch from the EC2 API.\" ) for region in list ( region_dict . keys ()) : _ = region_dict [ region ] . client ( \"ec2\" ) pool = ThreadPool ( len ( region_dict )) _p = partial ( _per_codename_amifetch , region_dict ) response = pool . map ( _p , codename_list ) return response","title":"query_codenames"},{"location":"reference/_amiupdater.html#reduce_api_results","text":"def reduce_api_results ( raw_results ) View Source def reduce_api_results ( raw_results ) : unsorted_results = [] missing_results = [] final_results = [] result_state = {} for query_result in raw_results : if query_result [ \"api_results\" ] : cn_api_results_data = [ APIResultsData( query_result[\"cn\" ] , x [ \"ImageId\" ] , _image_timestamp ( x [ \"CreationDate\" ] ), query_result [ \"region\" ] , ) for x in query_result [ \"api_results\" ] ] unsorted_results = cn_api_results_data + unsorted_results else : missing_results . append ( query_result ) if missing_results : LOG . warning ( \"No results were available for the following CODENAME / Region combination\" ) for missing_result in missing_results : LOG . warning ( f \"- {missing_result['cn']} in {missing_result['region']}\" ) sorted_results = sorted ( unsorted_results , reverse = True ) for _r in sorted_results : found_key = f \"{_r.region}-{_r.codename}\" already_found = result_state . get ( found_key , False ) if already_found : continue result_state [ found_key ] = True final_results . append ( _r ) return final_results","title":"reduce_api_results"},{"location":"reference/_amiupdater.html#classes","text":"","title":"Classes"},{"location":"reference/_amiupdater.html#amiupdater","text":"class AMIUpdater ( config , user_config_file = None , use_upstream_mappings = True )","title":"AMIUpdater"},{"location":"reference/_amiupdater.html#class-variables","text":"upstream_config_file upstream_config_file_url","title":"Class variables"},{"location":"reference/_amiupdater.html#methods","text":"","title":"Methods"},{"location":"reference/_amiupdater.html#get_regions_for_profile","text":"def get_regions_for_profile ( self , profile , _regions ) View Source def get_regions_for_profile ( self , profile , _regions ) : regions = {} for _r in _regions : regions [ _r ] = RegionObj ( name = _r , account_id = self . boto3_cache . account_id ( profile ), partition = self . boto3_cache . partition ( profile ), profile = profile , _boto3_cache = self . boto3_cache , taskcat_id = self . config . uid , _role_name = None , ) return regions","title":"get_regions_for_profile"},{"location":"reference/_amiupdater.html#update_amis","text":"def update_amis ( self ) View Source def update_amis ( self ) : codenames = set () LOG . info ( \"Determining templates and supported regions\" ) templates = self . _determine_templates_regions () LOG . info ( \"Determining regional search params for each AMI\" ) # Flush out codenames . for template in templates: template_cn = build_codenames ( template , Config ) for tcn in template_cn: codenames . add ( tcn ) # Retrieve API Results . LOG . info ( \"Retrieving results from the EC2 API\" ) results = query_codenames ( codenames , self . regions ) LOG . info ( \"Determining the latest AMI for each Codename/Region\" ) updated_api_results = reduce_api_results ( results ) # Figure out a way to sort dictionary by key - value ( timestmap ) _write_template = False for template in templates: for result in updated_api_results: changed = template . set_codename_ami ( result . codename , result . region , result . ami_id ) if changed: _write_template = True if _write_template: template . write () if _write_template: LOG . info ( \"Templates updated\" ) raise AMIUpdaterCommitNeededException LOG . info ( \"No AMI's needed updates.\" )","title":"update_amis"},{"location":"reference/_amiupdater.html#amiupdatercommitneededexception","text":"class AMIUpdaterCommitNeededException ( message = None )","title":"AMIUpdaterCommitNeededException"},{"location":"reference/_amiupdater.html#ancestors-in-mro","text":"taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/_amiupdater.html#class-variables_1","text":"args","title":"Class variables"},{"location":"reference/_amiupdater.html#methods_1","text":"","title":"Methods"},{"location":"reference/_amiupdater.html#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/_amiupdater.html#amiupdaterfatalexception","text":"class AMIUpdaterFatalException ( message = None )","title":"AMIUpdaterFatalException"},{"location":"reference/_amiupdater.html#ancestors-in-mro_1","text":"taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/_amiupdater.html#class-variables_2","text":"args","title":"Class variables"},{"location":"reference/_amiupdater.html#methods_2","text":"","title":"Methods"},{"location":"reference/_amiupdater.html#with_traceback_1","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/_amiupdater.html#apiresultsdata","text":"class APIResultsData ( codename : str , ami_id : str , creation_date : int , region : str , custom_comparisons : bool = True )","title":"APIResultsData"},{"location":"reference/_amiupdater.html#class-variables_3","text":"custom_comparisons","title":"Class variables"},{"location":"reference/_amiupdater.html#config","text":"class Config ( / , * args , ** kwargs )","title":"Config"},{"location":"reference/_amiupdater.html#class-variables_4","text":"codenames raw_dict","title":"Class variables"},{"location":"reference/_amiupdater.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_amiupdater.html#get_filter","text":"def get_filter ( code_name ) View Source @classmethod def get_filter ( cls , code_name ) : _x = deep_get ( cls . raw_dict , f \"global/AMIs/{code_name}\" , {} ) return { str ( k ) : [ str(v) ] if isinstance ( v , str ) else list ( v ) for k , v in _x . items () }","title":"get_filter"},{"location":"reference/_amiupdater.html#load","text":"def load ( file_name , configtype = None ) View Source @ classmethod def load ( cls , file_name , configtype = None ): with open ( file_name , \"r\" ) as _f : try : cls . raw_dict = yaml . safe_load ( _f ) except yaml . YAMLError as e : LOG . error ( f \"[{file_name}] - YAML Syntax Error!\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e )) try : for _x in cls . raw_dict . get ( \"global\" ) . get ( \"AMIs\" ) . keys (): cls . codenames . add ( _x ) except Exception as e : LOG . error ( f \"{configtype} config file [{file_name}]\" f \"is not structured properly!\" ) LOG . error ( f \"{e}\" ) # pylint: disable=raise-missing-from raise AMIUpdaterFatalException ( str ( e ))","title":"load"},{"location":"reference/_amiupdater.html#update_filter","text":"def update_filter ( code_name ) View Source @classmethod def update_filter ( cls , code_name ) : cls . raw_dict [ \"global\" ][ \"AMIs\" ] . update ( code_name )","title":"update_filter"},{"location":"reference/_amiupdater.html#ec2filtervalue","text":"class EC2FilterValue ( Name : str , Values : List [ str ] )","title":"EC2FilterValue"},{"location":"reference/_amiupdater.html#regionalcodename","text":"class RegionalCodename ( region : str , cn : str , new_ami : str = '' , filters : list = < factory > , _creation_dt : datetime . datetime = < factory > )","title":"RegionalCodename"},{"location":"reference/_amiupdater.html#class-variables_5","text":"new_ami","title":"Class variables"},{"location":"reference/_amiupdater.html#template","text":"class Template ( underlying : taskcat . _cfn . template . Template )","title":"Template"},{"location":"reference/_amiupdater.html#methods_3","text":"","title":"Methods"},{"location":"reference/_amiupdater.html#set_codename_ami","text":"def set_codename_ami ( self , cname , region , new_ami ) View Source def set_codename_ami ( self , cname , region , new_ami ) : if region not in self . region_names : return False key = f \"{cname}/{region}\" try : line_no = self . region_codename_lineno [ key ][ \"line\" ] old_ami = self . region_codename_lineno [ key ][ \"old\" ] if old_ami == new_ami : return False except KeyError : return False if old_ami == '\"\"' : new_ami = f '\"{new_ami}\"' new_record = re . sub ( old_ami , new_ami , self . _ls [ line_no ] ) self . _ls [ line_no ] = new_record return True","title":"set_codename_ami"},{"location":"reference/_amiupdater.html#write","text":"def write ( self ) View Source def write(self): self.underlying.raw_template = \"\\n\".join(self._ls) self.underlying.write()","title":"write"},{"location":"reference/_cli_core.html","text":"Module _cli_core None None View Source # This should ultimately become it's own module, as it has great general purpose # utility. import argparse import importlib import inspect import logging import sys import types from typing import List from taskcat._common_utils import exit_with_code LOG = logging . getLogger ( __name__ ) class CustomParser ( argparse . ArgumentParser ): def error ( self , message ): sys . stderr . write ( 'error: %s \\n ' % message ) self . print_help () sys . exit ( 2 ) def _get_log_level ( args , exit_func = exit_with_code ): log_level = \"INFO\" if ( \"-d\" in args or \"--debug\" in args ) and ( \"-q\" in args or \"--quiet\" in args ): exit_func ( 1 , \"--debug and --quiet cannot be specified simultaneously\" ) if \"-d\" in args or \"--debug\" in args : log_level = \"DEBUG\" if \"-q\" in args or \"--quiet\" in args : log_level = \"ERROR\" return log_level class SetVerbosity ( argparse . Action ): def __call__ ( self , parser , namespace , values , option_string = None ): LOG . setLevel ( _get_log_level ([ option_string ])) class GlobalArgs : ARGS = [ [ [ \"-q\" , \"--quiet\" ], { \"action\" : SetVerbosity , \"nargs\" : 0 , \"help\" : \"reduce output to the minimum\" , \"dest\" : \"_quiet\" , }, ], [ [ \"-d\" , \"--debug\" ], { \"action\" : SetVerbosity , \"nargs\" : 0 , \"help\" : \"adds debug output and tracebacks\" , \"dest\" : \"_debug\" , }, ], [[ \"--profile\" ], { \"help\" : \"set the default profile used.\" , \"dest\" : \"_profile\" }], ] def __init__ ( self ): self . _profile = \"default\" @property def profile ( self ): return self . _profile @profile . setter def profile ( self , profile ): self . _profile = profile GLOBAL_ARGS = GlobalArgs () class CliCore : USAGE = \" {prog}{global_opts}{command}{command_opts}{subcommand}{subcommand_opts} \" longform_required : List = [] @classmethod def longform_param_required ( cls , param_name ): def wrapper ( command_func ): formatted_param = param_name . lower () . replace ( \"_\" , \"-\" ) qualname = command_func . __qualname__ . replace ( \".__init__\" , \"\" ) cls . longform_required . append ( f \" { qualname } . { formatted_param } \" ) return command_func return wrapper def __init__ ( self , prog_name , module_package , description , version = None , args = None ): self . name = prog_name self . module_package = module_package self . _modules = self . _get_plugin_modules () self . args = { \"global\" : args if args is not None else [], \"commands\" : {}} self . _build_args () self . command_parser = None self . subcommand_parsers = {} self . parser = self . _build_parser ( description , version ) self . parsed_args = [] def _build_args ( self ): for name , module in self . _modules . items (): params = self . _get_params ( module ) self . args [ \"commands\" ][ name ] = { \"args\" : params , \"subcommands\" : {}} for method_name , method_function in self . _get_class_methods ( module ): if not method_name . startswith ( \"_\" ): params = self . _get_params ( method_function ) self . args [ \"commands\" ][ name ][ \"subcommands\" ][ method_name ] = params @staticmethod def _get_class_methods ( module ): methods = inspect . getmembers ( module , predicate = inspect . isfunction ) return [ method for method in methods if not method [ 0 ] . startswith ( \"_\" )] def _get_params ( self , item ): params = [] for param in inspect . signature ( item ) . parameters . values (): if param . name == \"self\" or param . name . startswith ( \"_\" ): continue required = param . default == param . empty default = param . default if not required else None val_type = param . annotation if param . annotation in [ str , int , bool ] else str action = \"store_true\" if val_type == bool else \"store\" param_help = CliCore . _get_param_help ( item , param . name ) name = param . name . lower () kwargs = { \"action\" : action , \"help\" : param_help } if not required : name = name . replace ( \"_\" , \"-\" ) kwargs . update ( { \"required\" : required , \"default\" : default , \"dest\" : param . name } ) if action == \"store\" : kwargs . update ({ \"type\" : val_type }) if required : params . append ([[ name ], kwargs ]) else : if f \" { item . __qualname__ } . { name } \" in self . longform_required : params . append ([[ f \"-- { name } \" ], kwargs ]) else : params . append ([[ f \"- { name [ 0 ] } \" , f \"-- { name } \" ], kwargs ]) return params @staticmethod def _get_param_help ( item , param ): help_str = \"\" docstring = ( item . __doc__ if isinstance ( item , types . FunctionType ) else item . __init__ . __doc__ ) if docstring is None : return help_str for line in docstring . split ( \" \\n \" ): if line . strip () . startswith ( f \":param { param } :\" ): help_str = line . strip ()[ len ( f \":param { param } :\" ) :] . strip () break return help_str @staticmethod def _get_help ( item ): help_str = \"\" if item . __doc__ is None : return help_str for line in item . __doc__ . split ( \" \\n \" ): if not line . strip () . startswith ( \":\" ): help_str += line . strip () return help_str . strip () def _get_command_help ( self , commands ): help_str = \"\" for name , mod in commands . items (): mod_help = self . _get_help ( mod ) if not mod_help : help_str += f \" { name } \\n \" else : help_str += f \" { name } - { mod_help } \\n \" return help_str . strip () def _add_subparser ( self , usage , description , mod , parser , args ): sub_parser = parser . add_parser ( mod , usage = usage , description = description , formatter_class = argparse . RawDescriptionHelpFormatter , ) self . _add_arguments ( args , sub_parser ) return sub_parser @staticmethod def _add_arguments ( input_args , parser ): for args , kwargs in input_args : parser . add_argument ( * args , ** kwargs ) @staticmethod def _add_sub ( parser , ** kwargs ): if sys . version_info [ 1 ] != 6 or \"required\" not in kwargs : return parser . add_subparsers ( ** kwargs ) required = kwargs [ \"required\" ] kwargs . pop ( \"required\" ) sub = parser . add_subparsers ( ** kwargs ) sub . required = required return sub def _build_parser ( self , description , version ): parser = CustomParser ( description = description , usage = self . _build_usage (), formatter_class = argparse . RawDescriptionHelpFormatter , ) if version : parser . add_argument ( \"-v\" , \"--version\" , action = \"version\" , version = version ) # Add global arguments self . _add_arguments ( self . args [ \"global\" ], parser ) description = self . _get_command_help ( self . _modules ) command_parser = self . _add_sub ( parser = parser , title = \"commands\" , description = description , required = True , metavar = \"\" , dest = \"_command\" , ) self . command_parser = command_parser for mod in self . _modules : usage = self . _build_usage ({ \"command\" : mod }) description = self . _get_help ( self . _modules [ mod ]) mod_parser = self . _add_subparser ( usage , description , mod , command_parser , self . args [ \"commands\" ][ mod ][ \"args\" ], ) self . subcommand_parsers [ mod ] = mod_parser # add subcommand parser if subcommands exist subcommands = self . args [ \"commands\" ][ mod ][ \"subcommands\" ] if subcommands : class_methods = { m [ 0 ]: m [ 1 ] for m in self . _get_class_methods ( self . _modules [ mod ]) } description = self . _get_command_help ( class_methods ) subcommand_parser = self . _add_sub ( parser = mod_parser , title = \"subcommands\" , description = description , required = True , metavar = \"\" , dest = \"_subcommand\" , ) for subcommand_name , subcommand_args in subcommands . items (): usage = self . _build_usage ({ \"subcommand\" : subcommand_name }) description = self . _get_help ( class_methods [ subcommand_name ]) self . _add_subparser ( usage , description , subcommand_name , subcommand_parser , subcommand_args , ) return parser def _build_usage ( self , args = None ): args = args if args is not None else {} args [ \"prog\" ] = self . name if \"command\" not in args : args [ \"command\" ] = \"<command>\" if \"subcommand\" not in args : args [ \"subcommand\" ] = \"[subcommand]\" if \"global_opts\" not in args : args [ \"global_opts\" ] = \"[args]\" if \"command_opts\" not in args : args [ \"command_opts\" ] = \"[args]\" if \"subcommand_opts\" not in args : args [ \"subcommand_opts\" ] = \"[args]\" for key , val in args . items (): if val and not val . endswith ( \" \" ): args [ key ] = f \" { val } \" return self . USAGE . format ( ** args ) def _get_plugin_modules ( self ): # pylint: disable=invalid-name members = inspect . getmembers ( self . module_package , predicate = inspect . isclass ) member_name_class = [] for name , cls in members : if hasattr ( cls , \"CLINAME\" ): name = cls . CLINAME member_name_class . append (( name , cls )) x = { name . lower (): cls for name , cls in member_name_class } return x @staticmethod def _import_plugin_module ( class_name , module_name ): return getattr ( importlib . import_module ( module_name ), class_name ) def parse ( self , args = None ): if not args : args = [] self . parsed_args = self . parser . parse_args ( args ) return self . parsed_args def run ( self ): args = self . parsed_args . __dict__ command = self . _modules [ args [ \"_command\" ]] subcommand = \"\" if \"_subcommand\" in args : subcommand = args [ \"_subcommand\" ] args = { k : v for k , v in args . items () if not k . startswith ( \"_\" )} if not subcommand : return command ( ** args ) return getattr ( command (), subcommand )( ** args ) Variables GLOBAL_ARGS LOG Classes CliCore class CliCore ( prog_name , module_package , description , version = None , args = None ) Class variables USAGE longform_required Static methods longform_param_required def longform_param_required ( param_name ) View Source @classmethod def longform_param_required ( cls , param_name ) : def wrapper ( command_func ) : formatted_param = param_name . lower (). replace ( \"_\" , \"-\" ) qualname = command_func . __qualname__ . replace ( \".__init__\" , \"\" ) cls . longform_required . append ( f \"{qualname}.{formatted_param}\" ) return command_func return wrapper Methods parse def parse ( self , args = None ) View Source def parse ( self , args = None ) : if not args : args = [] self . parsed_args = self . parser . parse_args ( args ) return self . parsed_args run def run ( self ) View Source def run ( self ) : args = self . parsed_args . __dict__ command = self . _modules [ args [ \" _command \" ]] subcommand = \"\" if \" _subcommand \" in args : subcommand = args [ \" _subcommand \" ] args = { k : v for k , v in args . items () if not k . startswith ( \" _ \" ) } if not subcommand : return command ( ** args ) return getattr ( command () , subcommand )( ** args ) CustomParser class CustomParser ( prog = None , usage = None , description = None , epilog = None , parents = [], formatter_class =< class ' argparse . HelpFormatter '>, prefix_chars = '-' , fromfile_prefix_chars = None , argument_default = None , conflict_handler = 'error' , add_help = True , allow_abbrev = True , exit_on_error = True ) Ancestors (in MRO) argparse.ArgumentParser argparse._AttributeHolder argparse._ActionsContainer Methods add_argument def add_argument ( self , * args , ** kwargs ) add_argument(dest, ..., name=value, ...) add_argument(option_string, option_string, ..., name=value, ...) View Source def add_argument ( self , * args , ** kwargs ): \"\"\" add_argument(dest, ..., name=value, ...) add_argument(option_string, option_string, ..., name=value, ...) \"\"\" # if no positional args are supplied or only one is supplied and # it doesn't look like an option string, parse a positional # argument chars = self . prefix_chars if not args or len ( args ) == 1 and args [ 0 ][ 0 ] not in chars : if args and 'dest' in kwargs : raise ValueError ( 'dest supplied twice for positional argument' ) kwargs = self . _get_positional_kwargs ( * args , ** kwargs ) # otherwise, we're adding an optional argument else : kwargs = self . _get_optional_kwargs ( * args , ** kwargs ) # if no default was supplied, use the parser-level default if 'default' not in kwargs : dest = kwargs [ 'dest' ] if dest in self . _defaults : kwargs [ 'default' ] = self . _defaults [ dest ] elif self . argument_default is not None : kwargs [ 'default' ] = self . argument_default # create the action object, and add it to the parser action_class = self . _pop_action_class ( kwargs ) if not callable ( action_class ): raise ValueError ( 'unknown action \" %s \"' % ( action_class ,)) action = action_class ( ** kwargs ) # raise an error if the action type is not callable type_func = self . _registry_get ( 'type' , action . type , action . type ) if not callable ( type_func ): raise ValueError ( ' %r is not callable' % ( type_func ,)) if type_func is FileType : raise ValueError ( ' %r is a FileType class object, instance of it' ' must be passed' % ( type_func ,)) # raise an error if the metavar does not match the type if hasattr ( self , \"_get_formatter\" ): try : self . _get_formatter () . _format_args ( action , None ) except TypeError : raise ValueError ( \"length of metavar tuple does not match nargs\" ) return self . _add_action ( action ) add_argument_group def add_argument_group ( self , * args , ** kwargs ) View Source def add_argument_group ( self , * args , ** kwargs ) : group = _ArgumentGroup ( self , * args , ** kwargs ) self . _action_groups . append ( group ) return group add_mutually_exclusive_group def add_mutually_exclusive_group ( self , ** kwargs ) View Source def add_mutually_exclusive_group ( self , ** kwargs ) : group = _MutuallyExclusiveGroup ( self , ** kwargs ) self . _mutually_exclusive_groups . append ( group ) return group add_subparsers def add_subparsers ( self , ** kwargs ) View Source def add_subparsers ( self , ** kwargs ) : if self . _subparsers is not None : self . error ( _ ( ' cannot have multiple subparser arguments ' )) # add the parser class to the arguments if it ' s not present kwargs . setdefault ( ' parser_class ' , type ( self )) if ' title ' in kwargs or ' description ' in kwargs : title = _ ( kwargs . pop ( ' title ' , ' subcommands ' )) description = _ ( kwargs . pop ( ' description ' , None )) self . _subparsers = self . add_argument_group ( title , description ) else : self . _subparsers = self . _positionals # prog defaults to the usage message of this parser , skipping # optional arguments and with no \" usage: \" prefix if kwargs . get ( ' prog ' ) is None : formatter = self . _get_formatter () positionals = self . _get_positional_actions () groups = self . _mutually_exclusive_groups formatter . add_usage ( self . usage , positionals , groups , '' ) kwargs [ ' prog ' ] = formatter . format_help () . strip () # create the parsers action and add it to the positionals list parsers_class = self . _pop_action_class ( kwargs , ' parsers ' ) action = parsers_class ( option_strings = [], ** kwargs ) self . _subparsers . _add_action ( action ) # return the created parsers action return action convert_arg_line_to_args def convert_arg_line_to_args ( self , arg_line ) View Source def convert_arg_line_to_args ( self , arg_line ) : return [ arg_line ] error def error ( self , message ) View Source def error ( self , message ) : sys . stderr . write ( ' error: %s \\n ' % message ) self . print_help () sys . exit ( 2 ) exit def exit ( self , status = 0 , message = None ) View Source def exit ( self , status = 0 , message = None ) : if message : self . _print_message ( message , _sys . stderr ) _sys . exit ( status ) format_help def format_help ( self ) View Source def format_help ( self ) : formatter = self . _get_formatter () # usage formatter . add_usage ( self . usage , self . _actions , self . _mutually_exclusive_groups ) # description formatter . add_text ( self . description ) # positionals , optionals and user - defined groups for action_group in self . _action_groups : formatter . start_section ( action_group . title ) formatter . add_text ( action_group . description ) formatter . add_arguments ( action_group . _group_actions ) formatter . end_section () # epilog formatter . add_text ( self . epilog ) # determine help from format above return formatter . format_help () format_usage def format_usage ( self ) View Source def format_usage ( self ) : formatter = self . _get_formatter () formatter . add_usage ( self . usage , self . _actions , self . _mutually_exclusive_groups ) return formatter . format_help () get_default def get_default ( self , dest ) View Source def get_default ( self , dest ) : for action in self . _actions : if action . dest == dest and action . default is not None : return action . default return self . _defaults . get ( dest , None ) parse_args def parse_args ( self , args = None , namespace = None ) View Source def parse_args ( self , args = None , namespace = None ) : args , argv = self . parse_known_args ( args , namespace ) if argv : msg = _ ( ' unrecognized arguments: %s ' ) self . error ( msg % ' ' . join ( argv )) return args parse_intermixed_args def parse_intermixed_args ( self , args = None , namespace = None ) View Source def parse_intermixed_args ( self , args = None , namespace = None ) : args , argv = self . parse_known_intermixed_args ( args , namespace ) if argv : msg = _ ( ' unrecognized arguments: %s ' ) self . error ( msg % ' ' . join ( argv )) return args parse_known_args def parse_known_args ( self , args = None , namespace = None ) View Source def parse_known_args ( self , args = None , namespace = None ) : if args is None : # args default to the system args args = _sys . argv [ 1: ] else : # make sure that args are mutable args = list ( args ) # default Namespace built from parser defaults if namespace is None : namespace = Namespace () # add any action defaults that aren 't present for action in self._actions: if action.dest is not SUPPRESS: if not hasattr(namespace, action.dest): if action.default is not SUPPRESS: setattr(namespace, action.dest, action.default) # add any parser defaults that aren' t present for dest in self . _defaults : if not hasattr ( namespace , dest ) : setattr ( namespace , dest , self . _defaults [ dest ] ) # parse the arguments and exit if there are any errors if self . exit_on_error : try : namespace , args = self . _parse_known_args ( args , namespace ) except ArgumentError : err = _sys . exc_info () [ 1 ] self . error ( str ( err )) else : namespace , args = self . _parse_known_args ( args , namespace ) if hasattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) : args . extend ( getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR )) delattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) return namespace , args parse_known_intermixed_args def parse_known_intermixed_args ( self , args = None , namespace = None ) View Source def parse_known_intermixed_args ( self , args = None , namespace = None ) : # returns a namespace and list of extras # # positional can be freely intermixed with optionals . optionals are # first parsed with all positional arguments deactivated . The ' extras ' # are then parsed . If the parser definition is incompatible with the # intermixed assumptions ( e . g . use of REMAINDER , subparsers ) a # TypeError is raised . # # positionals are ' deactivated ' by setting nargs and default to # SUPPRESS . This blocks the addition of that positional to the # namespace positionals = self . _get_positional_actions () a = [ action for action in positionals if action . nargs in [ PARSER , REMAINDER ]] if a : raise TypeError ( ' parse_intermixed_args: positional arg ' ' with nargs=%s ' % a [ 0 ]. nargs ) if [ action . dest for group in self . _mutually_exclusive_groups for action in group . _group_actions if action in positionals ]: raise TypeError ( ' parse_intermixed_args: positional in ' ' mutuallyExclusiveGroup ' ) try : save_usage = self . usage try : if self . usage is None : # capture the full usage for use in error messages self . usage = self . format_usage () [ 7 :] for action in positionals : # deactivate positionals action . save_nargs = action . nargs # action . nargs = 0 action . nargs = SUPPRESS action . save_default = action . default action . default = SUPPRESS namespace , remaining_args = self . parse_known_args ( args , namespace ) for action in positionals : # remove the empty positional values from namespace if ( hasattr ( namespace , action . dest ) and getattr ( namespace , action . dest ) == [] ) : from warnings import warn warn ( ' Do not expect %s in %s ' % ( action . dest , namespace )) delattr ( namespace , action . dest ) finally : # restore nargs and usage before exiting for action in positionals : action . nargs = action . save_nargs action . default = action . save_default optionals = self . _get_optional_actions () try : # parse positionals . optionals aren ' t normally required, but # they could be , so make sure they aren ' t. for action in optionals : action . save_required = action . required action . required = False for group in self . _mutually_exclusive_groups : group . save_required = group . required group . required = False namespace , extras = self . parse_known_args ( remaining_args , namespace ) finally : # restore parser values before exiting for action in optionals : action . required = action . save_required for group in self . _mutually_exclusive_groups : group . required = group . save_required finally : self . usage = save_usage return namespace , extras print_help def print_help ( self , file = None ) View Source def print_help ( self , file = None ) : if file is None : file = _sys . stdout self . _print_message ( self . format_help () , file ) print_usage def print_usage ( self , file = None ) View Source def print_usage ( self , file = None ) : if file is None : file = _sys . stdout self . _print_message ( self . format_usage () , file ) register def register ( self , registry_name , value , object ) View Source def register ( self , registry_name , value , object ) : registry = self . _registries . setdefault ( registry_name , {} ) registry [ value ] = object set_defaults def set_defaults ( self , ** kwargs ) View Source def set_defaults ( self , ** kwargs ) : self . _defaults . update ( kwargs ) # if these defaults match any existing arguments , replace # the previous default on the object with the new one for action in self . _actions : if action . dest in kwargs : action . default = kwargs [ action . dest ] GlobalArgs class GlobalArgs ( ) Class variables ARGS Instance variables profile SetVerbosity class SetVerbosity ( option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None ) Ancestors (in MRO) argparse.Action argparse._AttributeHolder Methods format_usage def format_usage ( self ) View Source def format_usage ( self ) : return self . option_strings [ 0 ]","title":" Cli Core"},{"location":"reference/_cli_core.html#module-_cli_core","text":"None None View Source # This should ultimately become it's own module, as it has great general purpose # utility. import argparse import importlib import inspect import logging import sys import types from typing import List from taskcat._common_utils import exit_with_code LOG = logging . getLogger ( __name__ ) class CustomParser ( argparse . ArgumentParser ): def error ( self , message ): sys . stderr . write ( 'error: %s \\n ' % message ) self . print_help () sys . exit ( 2 ) def _get_log_level ( args , exit_func = exit_with_code ): log_level = \"INFO\" if ( \"-d\" in args or \"--debug\" in args ) and ( \"-q\" in args or \"--quiet\" in args ): exit_func ( 1 , \"--debug and --quiet cannot be specified simultaneously\" ) if \"-d\" in args or \"--debug\" in args : log_level = \"DEBUG\" if \"-q\" in args or \"--quiet\" in args : log_level = \"ERROR\" return log_level class SetVerbosity ( argparse . Action ): def __call__ ( self , parser , namespace , values , option_string = None ): LOG . setLevel ( _get_log_level ([ option_string ])) class GlobalArgs : ARGS = [ [ [ \"-q\" , \"--quiet\" ], { \"action\" : SetVerbosity , \"nargs\" : 0 , \"help\" : \"reduce output to the minimum\" , \"dest\" : \"_quiet\" , }, ], [ [ \"-d\" , \"--debug\" ], { \"action\" : SetVerbosity , \"nargs\" : 0 , \"help\" : \"adds debug output and tracebacks\" , \"dest\" : \"_debug\" , }, ], [[ \"--profile\" ], { \"help\" : \"set the default profile used.\" , \"dest\" : \"_profile\" }], ] def __init__ ( self ): self . _profile = \"default\" @property def profile ( self ): return self . _profile @profile . setter def profile ( self , profile ): self . _profile = profile GLOBAL_ARGS = GlobalArgs () class CliCore : USAGE = \" {prog}{global_opts}{command}{command_opts}{subcommand}{subcommand_opts} \" longform_required : List = [] @classmethod def longform_param_required ( cls , param_name ): def wrapper ( command_func ): formatted_param = param_name . lower () . replace ( \"_\" , \"-\" ) qualname = command_func . __qualname__ . replace ( \".__init__\" , \"\" ) cls . longform_required . append ( f \" { qualname } . { formatted_param } \" ) return command_func return wrapper def __init__ ( self , prog_name , module_package , description , version = None , args = None ): self . name = prog_name self . module_package = module_package self . _modules = self . _get_plugin_modules () self . args = { \"global\" : args if args is not None else [], \"commands\" : {}} self . _build_args () self . command_parser = None self . subcommand_parsers = {} self . parser = self . _build_parser ( description , version ) self . parsed_args = [] def _build_args ( self ): for name , module in self . _modules . items (): params = self . _get_params ( module ) self . args [ \"commands\" ][ name ] = { \"args\" : params , \"subcommands\" : {}} for method_name , method_function in self . _get_class_methods ( module ): if not method_name . startswith ( \"_\" ): params = self . _get_params ( method_function ) self . args [ \"commands\" ][ name ][ \"subcommands\" ][ method_name ] = params @staticmethod def _get_class_methods ( module ): methods = inspect . getmembers ( module , predicate = inspect . isfunction ) return [ method for method in methods if not method [ 0 ] . startswith ( \"_\" )] def _get_params ( self , item ): params = [] for param in inspect . signature ( item ) . parameters . values (): if param . name == \"self\" or param . name . startswith ( \"_\" ): continue required = param . default == param . empty default = param . default if not required else None val_type = param . annotation if param . annotation in [ str , int , bool ] else str action = \"store_true\" if val_type == bool else \"store\" param_help = CliCore . _get_param_help ( item , param . name ) name = param . name . lower () kwargs = { \"action\" : action , \"help\" : param_help } if not required : name = name . replace ( \"_\" , \"-\" ) kwargs . update ( { \"required\" : required , \"default\" : default , \"dest\" : param . name } ) if action == \"store\" : kwargs . update ({ \"type\" : val_type }) if required : params . append ([[ name ], kwargs ]) else : if f \" { item . __qualname__ } . { name } \" in self . longform_required : params . append ([[ f \"-- { name } \" ], kwargs ]) else : params . append ([[ f \"- { name [ 0 ] } \" , f \"-- { name } \" ], kwargs ]) return params @staticmethod def _get_param_help ( item , param ): help_str = \"\" docstring = ( item . __doc__ if isinstance ( item , types . FunctionType ) else item . __init__ . __doc__ ) if docstring is None : return help_str for line in docstring . split ( \" \\n \" ): if line . strip () . startswith ( f \":param { param } :\" ): help_str = line . strip ()[ len ( f \":param { param } :\" ) :] . strip () break return help_str @staticmethod def _get_help ( item ): help_str = \"\" if item . __doc__ is None : return help_str for line in item . __doc__ . split ( \" \\n \" ): if not line . strip () . startswith ( \":\" ): help_str += line . strip () return help_str . strip () def _get_command_help ( self , commands ): help_str = \"\" for name , mod in commands . items (): mod_help = self . _get_help ( mod ) if not mod_help : help_str += f \" { name } \\n \" else : help_str += f \" { name } - { mod_help } \\n \" return help_str . strip () def _add_subparser ( self , usage , description , mod , parser , args ): sub_parser = parser . add_parser ( mod , usage = usage , description = description , formatter_class = argparse . RawDescriptionHelpFormatter , ) self . _add_arguments ( args , sub_parser ) return sub_parser @staticmethod def _add_arguments ( input_args , parser ): for args , kwargs in input_args : parser . add_argument ( * args , ** kwargs ) @staticmethod def _add_sub ( parser , ** kwargs ): if sys . version_info [ 1 ] != 6 or \"required\" not in kwargs : return parser . add_subparsers ( ** kwargs ) required = kwargs [ \"required\" ] kwargs . pop ( \"required\" ) sub = parser . add_subparsers ( ** kwargs ) sub . required = required return sub def _build_parser ( self , description , version ): parser = CustomParser ( description = description , usage = self . _build_usage (), formatter_class = argparse . RawDescriptionHelpFormatter , ) if version : parser . add_argument ( \"-v\" , \"--version\" , action = \"version\" , version = version ) # Add global arguments self . _add_arguments ( self . args [ \"global\" ], parser ) description = self . _get_command_help ( self . _modules ) command_parser = self . _add_sub ( parser = parser , title = \"commands\" , description = description , required = True , metavar = \"\" , dest = \"_command\" , ) self . command_parser = command_parser for mod in self . _modules : usage = self . _build_usage ({ \"command\" : mod }) description = self . _get_help ( self . _modules [ mod ]) mod_parser = self . _add_subparser ( usage , description , mod , command_parser , self . args [ \"commands\" ][ mod ][ \"args\" ], ) self . subcommand_parsers [ mod ] = mod_parser # add subcommand parser if subcommands exist subcommands = self . args [ \"commands\" ][ mod ][ \"subcommands\" ] if subcommands : class_methods = { m [ 0 ]: m [ 1 ] for m in self . _get_class_methods ( self . _modules [ mod ]) } description = self . _get_command_help ( class_methods ) subcommand_parser = self . _add_sub ( parser = mod_parser , title = \"subcommands\" , description = description , required = True , metavar = \"\" , dest = \"_subcommand\" , ) for subcommand_name , subcommand_args in subcommands . items (): usage = self . _build_usage ({ \"subcommand\" : subcommand_name }) description = self . _get_help ( class_methods [ subcommand_name ]) self . _add_subparser ( usage , description , subcommand_name , subcommand_parser , subcommand_args , ) return parser def _build_usage ( self , args = None ): args = args if args is not None else {} args [ \"prog\" ] = self . name if \"command\" not in args : args [ \"command\" ] = \"<command>\" if \"subcommand\" not in args : args [ \"subcommand\" ] = \"[subcommand]\" if \"global_opts\" not in args : args [ \"global_opts\" ] = \"[args]\" if \"command_opts\" not in args : args [ \"command_opts\" ] = \"[args]\" if \"subcommand_opts\" not in args : args [ \"subcommand_opts\" ] = \"[args]\" for key , val in args . items (): if val and not val . endswith ( \" \" ): args [ key ] = f \" { val } \" return self . USAGE . format ( ** args ) def _get_plugin_modules ( self ): # pylint: disable=invalid-name members = inspect . getmembers ( self . module_package , predicate = inspect . isclass ) member_name_class = [] for name , cls in members : if hasattr ( cls , \"CLINAME\" ): name = cls . CLINAME member_name_class . append (( name , cls )) x = { name . lower (): cls for name , cls in member_name_class } return x @staticmethod def _import_plugin_module ( class_name , module_name ): return getattr ( importlib . import_module ( module_name ), class_name ) def parse ( self , args = None ): if not args : args = [] self . parsed_args = self . parser . parse_args ( args ) return self . parsed_args def run ( self ): args = self . parsed_args . __dict__ command = self . _modules [ args [ \"_command\" ]] subcommand = \"\" if \"_subcommand\" in args : subcommand = args [ \"_subcommand\" ] args = { k : v for k , v in args . items () if not k . startswith ( \"_\" )} if not subcommand : return command ( ** args ) return getattr ( command (), subcommand )( ** args )","title":"Module _cli_core"},{"location":"reference/_cli_core.html#variables","text":"GLOBAL_ARGS LOG","title":"Variables"},{"location":"reference/_cli_core.html#classes","text":"","title":"Classes"},{"location":"reference/_cli_core.html#clicore","text":"class CliCore ( prog_name , module_package , description , version = None , args = None )","title":"CliCore"},{"location":"reference/_cli_core.html#class-variables","text":"USAGE longform_required","title":"Class variables"},{"location":"reference/_cli_core.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_cli_core.html#longform_param_required","text":"def longform_param_required ( param_name ) View Source @classmethod def longform_param_required ( cls , param_name ) : def wrapper ( command_func ) : formatted_param = param_name . lower (). replace ( \"_\" , \"-\" ) qualname = command_func . __qualname__ . replace ( \".__init__\" , \"\" ) cls . longform_required . append ( f \"{qualname}.{formatted_param}\" ) return command_func return wrapper","title":"longform_param_required"},{"location":"reference/_cli_core.html#methods","text":"","title":"Methods"},{"location":"reference/_cli_core.html#parse","text":"def parse ( self , args = None ) View Source def parse ( self , args = None ) : if not args : args = [] self . parsed_args = self . parser . parse_args ( args ) return self . parsed_args","title":"parse"},{"location":"reference/_cli_core.html#run","text":"def run ( self ) View Source def run ( self ) : args = self . parsed_args . __dict__ command = self . _modules [ args [ \" _command \" ]] subcommand = \"\" if \" _subcommand \" in args : subcommand = args [ \" _subcommand \" ] args = { k : v for k , v in args . items () if not k . startswith ( \" _ \" ) } if not subcommand : return command ( ** args ) return getattr ( command () , subcommand )( ** args )","title":"run"},{"location":"reference/_cli_core.html#customparser","text":"class CustomParser ( prog = None , usage = None , description = None , epilog = None , parents = [], formatter_class =< class ' argparse . HelpFormatter '>, prefix_chars = '-' , fromfile_prefix_chars = None , argument_default = None , conflict_handler = 'error' , add_help = True , allow_abbrev = True , exit_on_error = True )","title":"CustomParser"},{"location":"reference/_cli_core.html#ancestors-in-mro","text":"argparse.ArgumentParser argparse._AttributeHolder argparse._ActionsContainer","title":"Ancestors (in MRO)"},{"location":"reference/_cli_core.html#methods_1","text":"","title":"Methods"},{"location":"reference/_cli_core.html#add_argument","text":"def add_argument ( self , * args , ** kwargs ) add_argument(dest, ..., name=value, ...) add_argument(option_string, option_string, ..., name=value, ...) View Source def add_argument ( self , * args , ** kwargs ): \"\"\" add_argument(dest, ..., name=value, ...) add_argument(option_string, option_string, ..., name=value, ...) \"\"\" # if no positional args are supplied or only one is supplied and # it doesn't look like an option string, parse a positional # argument chars = self . prefix_chars if not args or len ( args ) == 1 and args [ 0 ][ 0 ] not in chars : if args and 'dest' in kwargs : raise ValueError ( 'dest supplied twice for positional argument' ) kwargs = self . _get_positional_kwargs ( * args , ** kwargs ) # otherwise, we're adding an optional argument else : kwargs = self . _get_optional_kwargs ( * args , ** kwargs ) # if no default was supplied, use the parser-level default if 'default' not in kwargs : dest = kwargs [ 'dest' ] if dest in self . _defaults : kwargs [ 'default' ] = self . _defaults [ dest ] elif self . argument_default is not None : kwargs [ 'default' ] = self . argument_default # create the action object, and add it to the parser action_class = self . _pop_action_class ( kwargs ) if not callable ( action_class ): raise ValueError ( 'unknown action \" %s \"' % ( action_class ,)) action = action_class ( ** kwargs ) # raise an error if the action type is not callable type_func = self . _registry_get ( 'type' , action . type , action . type ) if not callable ( type_func ): raise ValueError ( ' %r is not callable' % ( type_func ,)) if type_func is FileType : raise ValueError ( ' %r is a FileType class object, instance of it' ' must be passed' % ( type_func ,)) # raise an error if the metavar does not match the type if hasattr ( self , \"_get_formatter\" ): try : self . _get_formatter () . _format_args ( action , None ) except TypeError : raise ValueError ( \"length of metavar tuple does not match nargs\" ) return self . _add_action ( action )","title":"add_argument"},{"location":"reference/_cli_core.html#add_argument_group","text":"def add_argument_group ( self , * args , ** kwargs ) View Source def add_argument_group ( self , * args , ** kwargs ) : group = _ArgumentGroup ( self , * args , ** kwargs ) self . _action_groups . append ( group ) return group","title":"add_argument_group"},{"location":"reference/_cli_core.html#add_mutually_exclusive_group","text":"def add_mutually_exclusive_group ( self , ** kwargs ) View Source def add_mutually_exclusive_group ( self , ** kwargs ) : group = _MutuallyExclusiveGroup ( self , ** kwargs ) self . _mutually_exclusive_groups . append ( group ) return group","title":"add_mutually_exclusive_group"},{"location":"reference/_cli_core.html#add_subparsers","text":"def add_subparsers ( self , ** kwargs ) View Source def add_subparsers ( self , ** kwargs ) : if self . _subparsers is not None : self . error ( _ ( ' cannot have multiple subparser arguments ' )) # add the parser class to the arguments if it ' s not present kwargs . setdefault ( ' parser_class ' , type ( self )) if ' title ' in kwargs or ' description ' in kwargs : title = _ ( kwargs . pop ( ' title ' , ' subcommands ' )) description = _ ( kwargs . pop ( ' description ' , None )) self . _subparsers = self . add_argument_group ( title , description ) else : self . _subparsers = self . _positionals # prog defaults to the usage message of this parser , skipping # optional arguments and with no \" usage: \" prefix if kwargs . get ( ' prog ' ) is None : formatter = self . _get_formatter () positionals = self . _get_positional_actions () groups = self . _mutually_exclusive_groups formatter . add_usage ( self . usage , positionals , groups , '' ) kwargs [ ' prog ' ] = formatter . format_help () . strip () # create the parsers action and add it to the positionals list parsers_class = self . _pop_action_class ( kwargs , ' parsers ' ) action = parsers_class ( option_strings = [], ** kwargs ) self . _subparsers . _add_action ( action ) # return the created parsers action return action","title":"add_subparsers"},{"location":"reference/_cli_core.html#convert_arg_line_to_args","text":"def convert_arg_line_to_args ( self , arg_line ) View Source def convert_arg_line_to_args ( self , arg_line ) : return [ arg_line ]","title":"convert_arg_line_to_args"},{"location":"reference/_cli_core.html#error","text":"def error ( self , message ) View Source def error ( self , message ) : sys . stderr . write ( ' error: %s \\n ' % message ) self . print_help () sys . exit ( 2 )","title":"error"},{"location":"reference/_cli_core.html#exit","text":"def exit ( self , status = 0 , message = None ) View Source def exit ( self , status = 0 , message = None ) : if message : self . _print_message ( message , _sys . stderr ) _sys . exit ( status )","title":"exit"},{"location":"reference/_cli_core.html#format_help","text":"def format_help ( self ) View Source def format_help ( self ) : formatter = self . _get_formatter () # usage formatter . add_usage ( self . usage , self . _actions , self . _mutually_exclusive_groups ) # description formatter . add_text ( self . description ) # positionals , optionals and user - defined groups for action_group in self . _action_groups : formatter . start_section ( action_group . title ) formatter . add_text ( action_group . description ) formatter . add_arguments ( action_group . _group_actions ) formatter . end_section () # epilog formatter . add_text ( self . epilog ) # determine help from format above return formatter . format_help ()","title":"format_help"},{"location":"reference/_cli_core.html#format_usage","text":"def format_usage ( self ) View Source def format_usage ( self ) : formatter = self . _get_formatter () formatter . add_usage ( self . usage , self . _actions , self . _mutually_exclusive_groups ) return formatter . format_help ()","title":"format_usage"},{"location":"reference/_cli_core.html#get_default","text":"def get_default ( self , dest ) View Source def get_default ( self , dest ) : for action in self . _actions : if action . dest == dest and action . default is not None : return action . default return self . _defaults . get ( dest , None )","title":"get_default"},{"location":"reference/_cli_core.html#parse_args","text":"def parse_args ( self , args = None , namespace = None ) View Source def parse_args ( self , args = None , namespace = None ) : args , argv = self . parse_known_args ( args , namespace ) if argv : msg = _ ( ' unrecognized arguments: %s ' ) self . error ( msg % ' ' . join ( argv )) return args","title":"parse_args"},{"location":"reference/_cli_core.html#parse_intermixed_args","text":"def parse_intermixed_args ( self , args = None , namespace = None ) View Source def parse_intermixed_args ( self , args = None , namespace = None ) : args , argv = self . parse_known_intermixed_args ( args , namespace ) if argv : msg = _ ( ' unrecognized arguments: %s ' ) self . error ( msg % ' ' . join ( argv )) return args","title":"parse_intermixed_args"},{"location":"reference/_cli_core.html#parse_known_args","text":"def parse_known_args ( self , args = None , namespace = None ) View Source def parse_known_args ( self , args = None , namespace = None ) : if args is None : # args default to the system args args = _sys . argv [ 1: ] else : # make sure that args are mutable args = list ( args ) # default Namespace built from parser defaults if namespace is None : namespace = Namespace () # add any action defaults that aren 't present for action in self._actions: if action.dest is not SUPPRESS: if not hasattr(namespace, action.dest): if action.default is not SUPPRESS: setattr(namespace, action.dest, action.default) # add any parser defaults that aren' t present for dest in self . _defaults : if not hasattr ( namespace , dest ) : setattr ( namespace , dest , self . _defaults [ dest ] ) # parse the arguments and exit if there are any errors if self . exit_on_error : try : namespace , args = self . _parse_known_args ( args , namespace ) except ArgumentError : err = _sys . exc_info () [ 1 ] self . error ( str ( err )) else : namespace , args = self . _parse_known_args ( args , namespace ) if hasattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) : args . extend ( getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR )) delattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) return namespace , args","title":"parse_known_args"},{"location":"reference/_cli_core.html#parse_known_intermixed_args","text":"def parse_known_intermixed_args ( self , args = None , namespace = None ) View Source def parse_known_intermixed_args ( self , args = None , namespace = None ) : # returns a namespace and list of extras # # positional can be freely intermixed with optionals . optionals are # first parsed with all positional arguments deactivated . The ' extras ' # are then parsed . If the parser definition is incompatible with the # intermixed assumptions ( e . g . use of REMAINDER , subparsers ) a # TypeError is raised . # # positionals are ' deactivated ' by setting nargs and default to # SUPPRESS . This blocks the addition of that positional to the # namespace positionals = self . _get_positional_actions () a = [ action for action in positionals if action . nargs in [ PARSER , REMAINDER ]] if a : raise TypeError ( ' parse_intermixed_args: positional arg ' ' with nargs=%s ' % a [ 0 ]. nargs ) if [ action . dest for group in self . _mutually_exclusive_groups for action in group . _group_actions if action in positionals ]: raise TypeError ( ' parse_intermixed_args: positional in ' ' mutuallyExclusiveGroup ' ) try : save_usage = self . usage try : if self . usage is None : # capture the full usage for use in error messages self . usage = self . format_usage () [ 7 :] for action in positionals : # deactivate positionals action . save_nargs = action . nargs # action . nargs = 0 action . nargs = SUPPRESS action . save_default = action . default action . default = SUPPRESS namespace , remaining_args = self . parse_known_args ( args , namespace ) for action in positionals : # remove the empty positional values from namespace if ( hasattr ( namespace , action . dest ) and getattr ( namespace , action . dest ) == [] ) : from warnings import warn warn ( ' Do not expect %s in %s ' % ( action . dest , namespace )) delattr ( namespace , action . dest ) finally : # restore nargs and usage before exiting for action in positionals : action . nargs = action . save_nargs action . default = action . save_default optionals = self . _get_optional_actions () try : # parse positionals . optionals aren ' t normally required, but # they could be , so make sure they aren ' t. for action in optionals : action . save_required = action . required action . required = False for group in self . _mutually_exclusive_groups : group . save_required = group . required group . required = False namespace , extras = self . parse_known_args ( remaining_args , namespace ) finally : # restore parser values before exiting for action in optionals : action . required = action . save_required for group in self . _mutually_exclusive_groups : group . required = group . save_required finally : self . usage = save_usage return namespace , extras","title":"parse_known_intermixed_args"},{"location":"reference/_cli_core.html#print_help","text":"def print_help ( self , file = None ) View Source def print_help ( self , file = None ) : if file is None : file = _sys . stdout self . _print_message ( self . format_help () , file )","title":"print_help"},{"location":"reference/_cli_core.html#print_usage","text":"def print_usage ( self , file = None ) View Source def print_usage ( self , file = None ) : if file is None : file = _sys . stdout self . _print_message ( self . format_usage () , file )","title":"print_usage"},{"location":"reference/_cli_core.html#register","text":"def register ( self , registry_name , value , object ) View Source def register ( self , registry_name , value , object ) : registry = self . _registries . setdefault ( registry_name , {} ) registry [ value ] = object","title":"register"},{"location":"reference/_cli_core.html#set_defaults","text":"def set_defaults ( self , ** kwargs ) View Source def set_defaults ( self , ** kwargs ) : self . _defaults . update ( kwargs ) # if these defaults match any existing arguments , replace # the previous default on the object with the new one for action in self . _actions : if action . dest in kwargs : action . default = kwargs [ action . dest ]","title":"set_defaults"},{"location":"reference/_cli_core.html#globalargs","text":"class GlobalArgs ( )","title":"GlobalArgs"},{"location":"reference/_cli_core.html#class-variables_1","text":"ARGS","title":"Class variables"},{"location":"reference/_cli_core.html#instance-variables","text":"profile","title":"Instance variables"},{"location":"reference/_cli_core.html#setverbosity","text":"class SetVerbosity ( option_strings , dest , nargs = None , const = None , default = None , type = None , choices = None , required = False , help = None , metavar = None )","title":"SetVerbosity"},{"location":"reference/_cli_core.html#ancestors-in-mro_1","text":"argparse.Action argparse._AttributeHolder","title":"Ancestors (in MRO)"},{"location":"reference/_cli_core.html#methods_2","text":"","title":"Methods"},{"location":"reference/_cli_core.html#format_usage_1","text":"def format_usage ( self ) View Source def format_usage ( self ) : return self . option_strings [ 0 ]","title":"format_usage"},{"location":"reference/_client_factory.html","text":"Module _client_factory None None View Source import logging import operator from functools import reduce from time import sleep from typing import Any , Dict , List import boto3 import botocore . loaders as boto_loader import botocore . regions as boto_regions from botocore . config import Config as BotoConfig from botocore . exceptions import ClientError , NoCredentialsError , ProfileNotFound from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) REGIONAL_ENDPOINT_SERVICES = [ \"sts\" ] class Boto3Cache : RETRIES = 10 BACKOFF = 2 DELAY = 0.1 CLIENT_THROTTLE_RETRIES = 20 def __ init__ ( self , _ boto3 = boto3 ) : self . _ boto3 = _ boto3 self . _ session_cache: Dict [ str , Dict [ str , boto3 . Session ]] = {} self . _ client_cache: Dict [ str , Dict [ str , Dict [ str , boto3 . client ]]] = {} self . _ resource_cache: Dict [ str , Dict [ str , Dict [ str , boto3 . resource ]]] = {} self . _ account_info: Dict [ str , Dict [ str , str ]] = {} self . _ lock_cache_update = False def session ( self , profile : str = \"default\" , region : str = None ) -> boto3 . Session : region = self . _ get_region ( region , profile ) try : session = self . _ cache_lookup ( self . _ session_cache , [ profile , region ], self . _ boto3 . Session , [], { \"region_name\" : region , \"profile_name\" : profile }, ) except ProfileNotFound : if profile ! = \"default\" : raise session = self . _ boto3 . Session ( region_name = region ) self . _ cache_set ( self . _ session_cache , [ profile , region ], session ) return session def client ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . client : region = self . _ get_region ( region , profile ) session = self . session ( profile , region ) kwargs = { \"config\" : BotoConfig ( retries= { \"max_attempts\" : 20 })} if service in REGIONAL_ENDPOINT_SERVICES : kwargs . update ({ \"endpoint_url\" : self . _ get_endpoint_url ( service , region )}) return self . _ cache_lookup ( self . _ client_cache , [ profile , region , service ], session . client , [ service ], kwargs , ) def resource ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . resource : region = self . _ get_region ( region , profile ) session = self . session ( profile , region ) return self . _ cache_lookup ( self . _ resource_cache , [ profile , region , service ], session . resource , [ service ], ) def partition ( self , profile : str = \"default\" ) -> str : return self . _ cache_lookup ( self . _ account_info , [ profile ], self . _ get_account_info , [ profile ] )[ \"partition\" ] def account_id ( self , profile : str = \"default\" ) -> str : return self . _ cache_lookup ( self . _ account_info , [ profile ], self . _ get_account_info , [ profile ] )[ \"account_id\" ] def _ get_account_info ( self , profile ) : partition , region = self . _ get_partition ( profile ) session = self . session ( profile , region ) sts_client = session . client ( \"sts\" , region_name = region ) try : account_id = sts_client . get_caller_identity ()[ \"Account\" ] except ClientError as e : if e . response [ \"Error\" ][ \"Code\" ] == \"AccessDenied\" : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) raise except NoCredentialsError as e : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) except ProfileNotFound as e : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) return { \"partition\" : partition , \"account_id\" : account_id } def _ make_parent_keys ( self , cache : dict , keys : list ) : if keys : if not cache . get ( keys [ 0 ]) : cache [ keys [ 0 ]] = {} self . _ make_parent_keys ( cache [ keys [ 0 ]], keys [ 1 : ]) def _ cache_lookup ( self , cache , key_list , create_func , args = None , kwargs = None ) : try : value = self . _ cache_get ( cache , key_list ) except KeyError : args = [] if not args else args kwargs = {} if not kwargs else kwargs value = self . _ get_with_retry ( create_func , args , kwargs ) self . _ cache_set ( cache , key_list , value ) return value def _ get_with_retry ( self , create_func , args , kwargs ) : retries = self . RETRIES delay = self . DELAY while retries : try : return create_func ( * args , **kwargs ) except KeyError as e : if str ( e ) not in [ \"'credential_provider'\" , \"'endpoint_resolver'\" ] : raise backoff = ( self . RETRIES - retries + delay ) * self . BACKOFF sleep ( backoff ) @staticmethod def _ get_endpoint_url ( service , region ) : data = boto_loader . create_loader (). load_data ( \"endpoints\" ) endpoint_data = boto_regions . EndpointResolver ( data ). construct_endpoint ( service , region ) if not endpoint_data: raise TaskCatException ( f \"unable to resolve endpoint for {service} in {region}\" ) return f \"https://{service}.{region}.{endpoint_data['dnsSuffix']}\" @staticmethod def _ cache_get ( cache : dict , key_list: List [ str ]) : return reduce ( operator . getitem , key_list , cache ) def _ cache_set ( self , cache : dict , key_list: list , value : Any ) : self . _ make_parent_keys ( cache , key_list [:- 1 ]) self . _ cache_get ( cache , key_list [:- 1 ])[ key_list [ - 1 ]] = value def _ get_region ( self , region , profile ) : if not region : region = self . get_default_region ( profile ) return region def _ get_partition ( self , profile ) : partition_regions = [ ( \"aws\" , \"us-east-1\" ), ( \"aws-cn\" , \"cn-north-1\" ), ( \"aws-us-gov\" , \"us-gov-west-1\" ), ] for partition , region in partition_regions: try : self . session ( profile , region ). client ( \"sts\" , region_name = region ). get_caller_identity () return ( partition , region ) except ClientError as e : if \"InvalidClientTokenId\" in str ( e ) : continue raise raise ValueError ( \"cannot find suitable AWS partition\" ) def get_default_region ( self , profile_name= \"default\" ) -> str : try : region = self . _ boto3 . session . Session ( profile_name = profile_name ). region_name except ProfileNotFound : if profile_name ! = \"default\" : raise region = self . _ boto3 . session . Session (). region_name if not region : _ , region = self . _ get_partition ( profile_name ) LOG . warning ( \"Region not set in credential chain, defaulting to {}\" . format ( region ) ) return region Variables LOG REGIONAL_ENDPOINT_SERVICES Classes Boto3Cache class Boto3Cache ( _boto3 =< module 'boto3' from '/Users/tonynv/.pyenv/versions/3.9.1/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/boto3/__init__.py' > ) Class variables BACKOFF CLIENT_THROTTLE_RETRIES DELAY RETRIES Methods account_id def account_id ( self , profile : str = 'default' ) -> str View Source def account_id ( self , profile : str = \"default\" ) -> str : return self . _cache_lookup ( self . _account_info , [ profile ] , self . _get_account_info , [ profile ] ) [ \"account_id\" ] client def client ( self , service : str , profile : str = 'default' , region : str = None ) -> < function client at 0x103f3b4c0 > View Source def client ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . client : region = self . _get_region ( region , profile ) session = self . session ( profile , region ) kwargs = { \"config\" : BotoConfig ( retries = { \"max_attempts\" : 20 } ) } if service in REGIONAL_ENDPOINT_SERVICES : kwargs . update ( { \"endpoint_url\" : self . _get_endpoint_url ( service , region ) } ) return self . _cache_lookup ( self . _client_cache , [ profile, region, service ] , session . client , [ service ] , kwargs , ) get_default_region def get_default_region ( self , profile_name = 'default' ) -> str View Source def get_default_region ( self , profile_name = \"default\" ) -> str: try: region = self . _boto3 . session . Session ( profile_name = profile_name ). region_name except ProfileNotFound: if profile_name != \"default\" : raise region = self . _boto3 . session . Session (). region_name if not region: _ , region = self . _get_partition ( profile_name ) LOG . warning ( \"Region not set in credential chain, defaulting to {}\" . format ( region ) ) return region partition def partition ( self , profile : str = 'default' ) -> str View Source def partition ( self , profile : str = \"default\" ) -> str : return self . _cache_lookup ( self . _account_info , [ profile ] , self . _get_account_info , [ profile ] ) [ \"partition\" ] resource def resource ( self , service : str , profile : str = 'default' , region : str = None ) -> < function resource at 0x103f3b550 > View Source def resource ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . resource : region = self . _get_region ( region , profile ) session = self . session ( profile , region ) return self . _cache_lookup ( self . _resource_cache , [ profile, region, service ] , session . resource , [ service ] , ) session def session ( self , profile : str = 'default' , region : str = None ) -> boto3 . session . Session View Source def session ( self , profile: str = \"default\" , region: str = None ) -> boto3 . Session: region = self . _get_region ( region , profile ) try: session = self . _cache_lookup ( self . _session_cache , [ profile , region ], self . _boto3 . Session , [], { \"region_name\" : region , \"profile_name\" : profile }, ) except ProfileNotFound: if profile != \"default\" : raise session = self . _boto3 . Session ( region_name = region ) self . _cache_set ( self . _session_cache , [ profile , region ], session ) return session","title":" Client Factory"},{"location":"reference/_client_factory.html#module-_client_factory","text":"None None View Source import logging import operator from functools import reduce from time import sleep from typing import Any , Dict , List import boto3 import botocore . loaders as boto_loader import botocore . regions as boto_regions from botocore . config import Config as BotoConfig from botocore . exceptions import ClientError , NoCredentialsError , ProfileNotFound from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) REGIONAL_ENDPOINT_SERVICES = [ \"sts\" ] class Boto3Cache : RETRIES = 10 BACKOFF = 2 DELAY = 0.1 CLIENT_THROTTLE_RETRIES = 20 def __ init__ ( self , _ boto3 = boto3 ) : self . _ boto3 = _ boto3 self . _ session_cache: Dict [ str , Dict [ str , boto3 . Session ]] = {} self . _ client_cache: Dict [ str , Dict [ str , Dict [ str , boto3 . client ]]] = {} self . _ resource_cache: Dict [ str , Dict [ str , Dict [ str , boto3 . resource ]]] = {} self . _ account_info: Dict [ str , Dict [ str , str ]] = {} self . _ lock_cache_update = False def session ( self , profile : str = \"default\" , region : str = None ) -> boto3 . Session : region = self . _ get_region ( region , profile ) try : session = self . _ cache_lookup ( self . _ session_cache , [ profile , region ], self . _ boto3 . Session , [], { \"region_name\" : region , \"profile_name\" : profile }, ) except ProfileNotFound : if profile ! = \"default\" : raise session = self . _ boto3 . Session ( region_name = region ) self . _ cache_set ( self . _ session_cache , [ profile , region ], session ) return session def client ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . client : region = self . _ get_region ( region , profile ) session = self . session ( profile , region ) kwargs = { \"config\" : BotoConfig ( retries= { \"max_attempts\" : 20 })} if service in REGIONAL_ENDPOINT_SERVICES : kwargs . update ({ \"endpoint_url\" : self . _ get_endpoint_url ( service , region )}) return self . _ cache_lookup ( self . _ client_cache , [ profile , region , service ], session . client , [ service ], kwargs , ) def resource ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . resource : region = self . _ get_region ( region , profile ) session = self . session ( profile , region ) return self . _ cache_lookup ( self . _ resource_cache , [ profile , region , service ], session . resource , [ service ], ) def partition ( self , profile : str = \"default\" ) -> str : return self . _ cache_lookup ( self . _ account_info , [ profile ], self . _ get_account_info , [ profile ] )[ \"partition\" ] def account_id ( self , profile : str = \"default\" ) -> str : return self . _ cache_lookup ( self . _ account_info , [ profile ], self . _ get_account_info , [ profile ] )[ \"account_id\" ] def _ get_account_info ( self , profile ) : partition , region = self . _ get_partition ( profile ) session = self . session ( profile , region ) sts_client = session . client ( \"sts\" , region_name = region ) try : account_id = sts_client . get_caller_identity ()[ \"Account\" ] except ClientError as e : if e . response [ \"Error\" ][ \"Code\" ] == \"AccessDenied\" : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) raise except NoCredentialsError as e : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) except ProfileNotFound as e : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Not able to fetch account number from {region} using profile \" f \"{profile}. {str(e)}\" ) return { \"partition\" : partition , \"account_id\" : account_id } def _ make_parent_keys ( self , cache : dict , keys : list ) : if keys : if not cache . get ( keys [ 0 ]) : cache [ keys [ 0 ]] = {} self . _ make_parent_keys ( cache [ keys [ 0 ]], keys [ 1 : ]) def _ cache_lookup ( self , cache , key_list , create_func , args = None , kwargs = None ) : try : value = self . _ cache_get ( cache , key_list ) except KeyError : args = [] if not args else args kwargs = {} if not kwargs else kwargs value = self . _ get_with_retry ( create_func , args , kwargs ) self . _ cache_set ( cache , key_list , value ) return value def _ get_with_retry ( self , create_func , args , kwargs ) : retries = self . RETRIES delay = self . DELAY while retries : try : return create_func ( * args , **kwargs ) except KeyError as e : if str ( e ) not in [ \"'credential_provider'\" , \"'endpoint_resolver'\" ] : raise backoff = ( self . RETRIES - retries + delay ) * self . BACKOFF sleep ( backoff ) @staticmethod def _ get_endpoint_url ( service , region ) : data = boto_loader . create_loader (). load_data ( \"endpoints\" ) endpoint_data = boto_regions . EndpointResolver ( data ). construct_endpoint ( service , region ) if not endpoint_data: raise TaskCatException ( f \"unable to resolve endpoint for {service} in {region}\" ) return f \"https://{service}.{region}.{endpoint_data['dnsSuffix']}\" @staticmethod def _ cache_get ( cache : dict , key_list: List [ str ]) : return reduce ( operator . getitem , key_list , cache ) def _ cache_set ( self , cache : dict , key_list: list , value : Any ) : self . _ make_parent_keys ( cache , key_list [:- 1 ]) self . _ cache_get ( cache , key_list [:- 1 ])[ key_list [ - 1 ]] = value def _ get_region ( self , region , profile ) : if not region : region = self . get_default_region ( profile ) return region def _ get_partition ( self , profile ) : partition_regions = [ ( \"aws\" , \"us-east-1\" ), ( \"aws-cn\" , \"cn-north-1\" ), ( \"aws-us-gov\" , \"us-gov-west-1\" ), ] for partition , region in partition_regions: try : self . session ( profile , region ). client ( \"sts\" , region_name = region ). get_caller_identity () return ( partition , region ) except ClientError as e : if \"InvalidClientTokenId\" in str ( e ) : continue raise raise ValueError ( \"cannot find suitable AWS partition\" ) def get_default_region ( self , profile_name= \"default\" ) -> str : try : region = self . _ boto3 . session . Session ( profile_name = profile_name ). region_name except ProfileNotFound : if profile_name ! = \"default\" : raise region = self . _ boto3 . session . Session (). region_name if not region : _ , region = self . _ get_partition ( profile_name ) LOG . warning ( \"Region not set in credential chain, defaulting to {}\" . format ( region ) ) return region","title":"Module _client_factory"},{"location":"reference/_client_factory.html#variables","text":"LOG REGIONAL_ENDPOINT_SERVICES","title":"Variables"},{"location":"reference/_client_factory.html#classes","text":"","title":"Classes"},{"location":"reference/_client_factory.html#boto3cache","text":"class Boto3Cache ( _boto3 =< module 'boto3' from '/Users/tonynv/.pyenv/versions/3.9.1/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/boto3/__init__.py' > )","title":"Boto3Cache"},{"location":"reference/_client_factory.html#class-variables","text":"BACKOFF CLIENT_THROTTLE_RETRIES DELAY RETRIES","title":"Class variables"},{"location":"reference/_client_factory.html#methods","text":"","title":"Methods"},{"location":"reference/_client_factory.html#account_id","text":"def account_id ( self , profile : str = 'default' ) -> str View Source def account_id ( self , profile : str = \"default\" ) -> str : return self . _cache_lookup ( self . _account_info , [ profile ] , self . _get_account_info , [ profile ] ) [ \"account_id\" ]","title":"account_id"},{"location":"reference/_client_factory.html#client","text":"def client ( self , service : str , profile : str = 'default' , region : str = None ) -> < function client at 0x103f3b4c0 > View Source def client ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . client : region = self . _get_region ( region , profile ) session = self . session ( profile , region ) kwargs = { \"config\" : BotoConfig ( retries = { \"max_attempts\" : 20 } ) } if service in REGIONAL_ENDPOINT_SERVICES : kwargs . update ( { \"endpoint_url\" : self . _get_endpoint_url ( service , region ) } ) return self . _cache_lookup ( self . _client_cache , [ profile, region, service ] , session . client , [ service ] , kwargs , )","title":"client"},{"location":"reference/_client_factory.html#get_default_region","text":"def get_default_region ( self , profile_name = 'default' ) -> str View Source def get_default_region ( self , profile_name = \"default\" ) -> str: try: region = self . _boto3 . session . Session ( profile_name = profile_name ). region_name except ProfileNotFound: if profile_name != \"default\" : raise region = self . _boto3 . session . Session (). region_name if not region: _ , region = self . _get_partition ( profile_name ) LOG . warning ( \"Region not set in credential chain, defaulting to {}\" . format ( region ) ) return region","title":"get_default_region"},{"location":"reference/_client_factory.html#partition","text":"def partition ( self , profile : str = 'default' ) -> str View Source def partition ( self , profile : str = \"default\" ) -> str : return self . _cache_lookup ( self . _account_info , [ profile ] , self . _get_account_info , [ profile ] ) [ \"partition\" ]","title":"partition"},{"location":"reference/_client_factory.html#resource","text":"def resource ( self , service : str , profile : str = 'default' , region : str = None ) -> < function resource at 0x103f3b550 > View Source def resource ( self , service : str , profile : str = \"default\" , region : str = None ) -> boto3 . resource : region = self . _get_region ( region , profile ) session = self . session ( profile , region ) return self . _cache_lookup ( self . _resource_cache , [ profile, region, service ] , session . resource , [ service ] , )","title":"resource"},{"location":"reference/_client_factory.html#session","text":"def session ( self , profile : str = 'default' , region : str = None ) -> boto3 . session . Session View Source def session ( self , profile: str = \"default\" , region: str = None ) -> boto3 . Session: region = self . _get_region ( region , profile ) try: session = self . _cache_lookup ( self . _session_cache , [ profile , region ], self . _boto3 . Session , [], { \"region_name\" : region , \"profile_name\" : profile }, ) except ProfileNotFound: if profile != \"default\" : raise session = self . _boto3 . Session ( region_name = region ) self . _cache_set ( self . _session_cache , [ profile , region ], session ) return session","title":"session"},{"location":"reference/_common_utils.html","text":"Module _common_utils None None View Source import collections import logging import os import re import sys from collections import OrderedDict from functools import reduce from pathlib import Path from time import sleep import requests import yaml from botocore.exceptions import ClientError from dulwich.config import ConfigFile , parse_submodules from taskcat.exceptions import TaskCatException from taskcat.regions_to_partitions import REGIONS LOG = logging . getLogger ( __name__ ) S3_PARTITION_MAP = { \"aws\" : \"amazonaws.com\" , \"aws-cn\" : \"amazonaws.com.cn\" , \"aws-us-gov\" : \"amazonaws.com\" , } FIRST_CAP_RE = re . compile ( \"(.)([A-Z][a-z]+)\" ) ALL_CAP_RE = re . compile ( \"([a-z0-9])([A-Z])\" ) def region_from_stack_id ( stack_id ): return stack_id . split ( \":\" )[ 3 ] def name_from_stack_id ( stack_id ): return stack_id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def s3_url_maker ( bucket , key , s3_client , autobucket = False ): retries = 10 while True : try : try : response = s3_client . get_bucket_location ( Bucket = bucket ) location = response [ \"LocationConstraint\" ] except ClientError as e : if e . response [ \"Error\" ][ \"Code\" ] != \"AccessDenied\" : raise resp = requests . get ( f \"https:// { bucket } .s3.amazonaws.com/ { key } \" ) location = resp . headers . get ( \"x-amz-bucket-region\" ) if not location : # pylint: disable=raise-missing-from raise TaskCatException ( f \"failed to discover region for bucket { bucket } \" ) break except s3_client . exceptions . NoSuchBucket : if not autobucket or retries < 1 : raise retries -= 1 sleep ( 5 ) # default case for us-east-1 which returns no location url = f \"https:// { bucket } .s3.us-east-1.amazonaws.com/ { key } \" if location : domain = get_s3_domain ( location ) url = f \"https:// { bucket } .s3. { location } . { domain } / { key } \" return url def get_s3_domain ( region ): try : return S3_PARTITION_MAP [ REGIONS [ region ]] except KeyError : # pylint: disable=raise-missing-from raise TaskCatException ( f \"cannot find the S3 hostname for region { region } \" ) def s3_bucket_name_from_url ( url ): return url . split ( \"//\" )[ 1 ] . split ( \".\" )[ 0 ] def s3_key_from_url ( url ): return \"/\" . join ( url . split ( \"//\" )[ 1 ] . split ( \"/\" )[ 1 :]) class CommonTools : def __init__ ( self , stack_name ): self . stack_name = stack_name @staticmethod def regxfind ( re_object , data_line ): \"\"\" Returns the matching string. :param re_object: Regex object :param data_line: String to be searched :return: Matching String if found, otherwise return 'Not-found' \"\"\" security_group = re_object . search ( data_line ) if security_group : return str ( security_group . group ()) return str ( \"Not-found\" ) def exit_with_code ( code , msg = \"\" ): if msg : LOG . error ( msg ) sys . exit ( code ) def make_dir ( path , ignore_exists = True ): path = os . path . abspath ( path ) if ignore_exists and os . path . isdir ( path ): return os . makedirs ( path ) def param_list_to_dict ( original_keys ): # Setup a list index dictionary. # - Used to give an Parameter => Index mapping for replacement. param_index = {} if not isinstance ( original_keys , list ): # pylint: disable=raise-missing-from raise TaskCatException ( 'Invalid parameter file, outermost json element must be a list (\"[]\")' ) for ( idx , param_dict ) in enumerate ( original_keys ): if not isinstance ( param_dict , dict ): # pylint: disable=raise-missing-from raise TaskCatException ( 'Invalid parameter %s parameters must be of type dict (\" {} \")' % param_dict ) if \"ParameterKey\" not in param_dict or \"ParameterValue\" not in param_dict : # pylint: disable=raise-missing-from raise TaskCatException ( f \"Invalid parameter { param_dict } all items must \" f \"have both ParameterKey and ParameterValue keys\" ) key = param_dict [ \"ParameterKey\" ] param_index [ key ] = idx return param_index def merge_dicts ( list_of_dicts ): merged_dict = {} for single_dict in list_of_dicts : merged_dict = { ** merged_dict , ** single_dict } return merged_dict def pascal_to_snake ( pascal ): sub = ALL_CAP_RE . sub ( r \"\\1_\\2\" , pascal ) return ALL_CAP_RE . sub ( r \"\\1_\\2\" , sub ) . lower () def merge_nested_dict ( old , new ): for k , v in new . items (): if isinstance ( old . get ( k ), dict ) and isinstance ( v , collections . Mapping ): merge_nested_dict ( old [ k ], v ) else : old [ k ] = v def ordered_dump ( data , stream = None , dumper = yaml . Dumper , ** kwds ): class OrderedDumper ( dumper ): # pylint: disable=too-many-ancestors pass def _dict_representer ( dumper , data ): return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items () ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , stream , OrderedDumper , ** kwds ) def deep_get ( dictionary , keys , default = None ): zulu = reduce ( lambda d , key : d . get ( key , default ) if isinstance ( d , dict ) else default , keys . split ( \"/\" ), dictionary , ) return zulu def neglect_submodule_templates ( project_root , template_list ): template_dict = {} # one template object per path. for template in template_list : template_dict [ template . template_path ] = template for template_descendent in template . descendents : template_dict [ template_descendent . template_path ] = template_descendent # Removing those within a submodule. submodule_path_prefixes = [] try : gitmodule_config = ConfigFile . from_path ( Path ( project_root / \".gitmodules\" )) except FileNotFoundError : return template_list for submodule_path , _ , _ in parse_submodules ( gitmodule_config ): submodule_path_prefixes . append ( Path ( project_root / submodule_path . decode ( \"utf-8\" )) ) finalized_templates = [] for template_obj in list ( template_dict . values ()): gitmodule_template = False for gm_path in submodule_path_prefixes : if gm_path in template_obj . template_path . parents : gitmodule_template = True if not gitmodule_template : finalized_templates . append ( template_obj ) return finalized_templates def determine_profile_for_region ( auth_dict , region ): profile = auth_dict . get ( region , auth_dict . get ( \"default\" , \"default\" )) return profile def fetch_ssm_parameter_value ( boto_client , parameter_path ): ssm = boto_client ( \"ssm\" ) response = ssm . get_parameter ( Name = parameter_path ) return response [ \"Parameter\" ][ \"Value\" ] def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ): secrets_manager = boto_client ( \"secretsmanager\" ) try : response = secrets_manager . get_secret_value ( SecretId = secret_arn )[ \"SecretString\" ] except Exception as e : # pylint: disable=raise-missing-from raise TaskCatException ( \"ARN: {} encountered an error: {} \" . format ( secret_arn , str ( e )) ) return response Variables ALL_CAP_RE FIRST_CAP_RE LOG REGIONS S3_PARTITION_MAP Functions deep_get def deep_get ( dictionary , keys , default = None ) View Source def deep_get ( dictionary , keys , default = None ) : zulu = reduce ( lambda d , key : d . get ( key , default ) if isinstance ( d , dict ) else default , keys . split ( \" / \" ) , dictionary , ) return zulu determine_profile_for_region def determine_profile_for_region ( auth_dict , region ) View Source def determine_profile_for_region ( auth_dict , region ) : profile = auth_dict . get ( region , auth_dict . get ( \"default\" , \"default\" )) return profile exit_with_code def exit_with_code ( code , msg = '' ) View Source def exit_with_code ( code , msg = \"\" ) : if msg : LOG . error ( msg ) sys . exit ( code ) fetch_secretsmanager_parameter_value def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ) View Source def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ) : secrets_manager = boto_client ( \" secretsmanager \" ) try : response = secrets_manager . get_secret_value ( SecretId = secret_arn ) [ \" SecretString \" ] except Exception as e : # pylint : disable = raise - missing - from raise TaskCatException ( \" ARN: {} encountered an error: {} \" . format ( secret_arn , str ( e )) ) return response fetch_ssm_parameter_value def fetch_ssm_parameter_value ( boto_client , parameter_path ) View Source def fetch_ssm_parameter_value ( boto_client , parameter_path ) : ssm = boto_client ( \" ssm \" ) response = ssm . get_parameter ( Name = parameter_path ) return response [ \" Parameter \" ][ \" Value \" ] get_s3_domain def get_s3_domain ( region ) View Source def get_s3_domain ( region ) : try : return S3_PARTITION_MAP [ REGIONS[region ] ] except KeyError : # pylint : disable = raise - missing - from raise TaskCatException ( f \"cannot find the S3 hostname for region {region}\" ) make_dir def make_dir ( path , ignore_exists = True ) View Source def make_dir ( path , ignore_exists = True ) : path = os . path . abspath ( path ) if ignore_exists and os . path . isdir ( path ) : return os . makedirs ( path ) merge_dicts def merge_dicts ( list_of_dicts ) View Source def merge_dicts ( list_of_dicts ) : merged_dict = {} for single_dict in list_of_dicts : merged_dict = { ** merged_dict , ** single_dict } return merged_dict merge_nested_dict def merge_nested_dict ( old , new ) View Source def merge_nested_dict ( old , new ) : for k , v in new . items () : if isinstance ( old . get ( k ), dict ) and isinstance ( v , collections . Mapping ) : merge_nested_dict ( old [ k ] , v ) else : old [ k ] = v name_from_stack_id def name_from_stack_id ( stack_id ) View Source def name_from_stack_id ( stack_id ) : return stack_id . split ( \" : \" ) [ 5 ]. split ( \" / \" ) [ 1 ] neglect_submodule_templates def neglect_submodule_templates ( project_root , template_list ) View Source def neglect_submodule_templates ( project_root , template_list ) : template_dict = {} # one template object per path . for template in template_list : template_dict [ template . template_path ] = template for template_descendent in template . descendents : template_dict [ template_descendent . template_path ] = template_descendent # Removing those within a submodule . submodule_path_prefixes = [] try : gitmodule_config = ConfigFile . from_path ( Path ( project_root / \" .gitmodules \" )) except FileNotFoundError : return template_list for submodule_path , _ , _ in parse_submodules ( gitmodule_config ) : submodule_path_prefixes . append ( Path ( project_root / submodule_path . decode ( \" utf-8 \" )) ) finalized_templates = [] for template_obj in list ( template_dict . values ()) : gitmodule_template = False for gm_path in submodule_path_prefixes : if gm_path in template_obj . template_path . parents : gitmodule_template = True if not gitmodule_template : finalized_templates . append ( template_obj ) return finalized_templates ordered_dump def ordered_dump ( data , stream = None , dumper =< class ' yaml . dumper . Dumper '>, ** kwds ) View Source def ordered_dump ( data , stream = None , dumper = yaml . Dumper , ** kwds ) : class OrderedDumper ( dumper ) : # pylint : disable = too - many - ancestors pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items () ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , stream , OrderedDumper , ** kwds ) param_list_to_dict def param_list_to_dict ( original_keys ) View Source def param_list_to_dict ( original_keys ) : # Setup a list index dictionary . # - Used to give an Parameter => Index mapping for replacement . param_index = {} if not isinstance ( original_keys , list ) : # pylint : disable = raise - missing - from raise TaskCatException ( 'Invalid parameter file, outermost json element must be a list (\"[]\")' ) for ( idx , param_dict ) in enumerate ( original_keys ) : if not isinstance ( param_dict , dict ) : # pylint : disable = raise - missing - from raise TaskCatException ( 'Invalid parameter %s parameters must be of type dict (\"{}\")' % param_dict ) if \"ParameterKey\" not in param_dict or \"ParameterValue\" not in param_dict : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Invalid parameter {param_dict} all items must \" f \"have both ParameterKey and ParameterValue keys\" ) key = param_dict [ \"ParameterKey\" ] param_index [ key ] = idx return param_index pascal_to_snake def pascal_to_snake ( pascal ) View Source def pascal_to_snake ( pascal ) : sub = ALL_CAP_RE . sub ( r \" \\1_\\2 \" , pascal ) return ALL_CAP_RE . sub ( r \" \\1_\\2 \" , sub ) . lower () region_from_stack_id def region_from_stack_id ( stack_id ) View Source def region_from_stack_id ( stack_id ) : return stack_id . split ( \":\" )[ 3 ] s3_bucket_name_from_url def s3_bucket_name_from_url ( url ) View Source def s3_bucket_name_from_url ( url ) : return url . split ( \" // \" ) [ 1 ]. split ( \" . \" ) [ 0 ] s3_key_from_url def s3_key_from_url ( url ) View Source def s3_key_from_url ( url ) : return \" / \" . join ( url . split ( \" // \" ) [ 1 ]. split ( \" / \" ) [ 1 :] ) s3_url_maker def s3_url_maker ( bucket , key , s3_client , autobucket = False ) View Source def s3_url_maker ( bucket , key , s3_client , autobucket = False ) : retries = 10 while True: try: try: response = s3_client . get_bucket_location ( Bucket = bucket ) location = response [ \"LocationConstraint\" ] except ClientError as e: if e . response [ \"Error\" ][ \"Code\" ] != \"AccessDenied\" : raise resp = requests . get ( f \"https://{bucket}.s3.amazonaws.com/{key}\" ) location = resp . headers . get ( \"x-amz-bucket-region\" ) if not location: # pylint: disable = raise - missing - from raise TaskCatException ( f \"failed to discover region for bucket {bucket}\" ) break except s3_client . exceptions . NoSuchBucket: if not autobucket or retries < 1 : raise retries -= 1 sleep ( 5 ) # default case for us - east - 1 which returns no location url = f \"https://{bucket}.s3.us-east-1.amazonaws.com/{key}\" if location: domain = get_s3_domain ( location ) url = f \"https://{bucket}.s3.{location}.{domain}/{key}\" return url Classes CommonTools class CommonTools ( stack_name ) Static methods regxfind def regxfind ( re_object , data_line ) Returns the matching string. Parameters: Name Type Description Default re_object None Regex object None data_line None String to be searched None Returns: Type Description None Matching String if found, otherwise return 'Not-found' View Source @staticmethod def regxfind ( re_object , data_line ) : \"\"\" Returns the matching string. :param re_object: Regex object :param data_line: String to be searched :return: Matching String if found, otherwise return 'Not-found' \"\"\" security_group = re_object . search ( data_line ) if security_group : return str ( security_group . group ()) return str ( \"Not-found\" )","title":" Common Utils"},{"location":"reference/_common_utils.html#module-_common_utils","text":"None None View Source import collections import logging import os import re import sys from collections import OrderedDict from functools import reduce from pathlib import Path from time import sleep import requests import yaml from botocore.exceptions import ClientError from dulwich.config import ConfigFile , parse_submodules from taskcat.exceptions import TaskCatException from taskcat.regions_to_partitions import REGIONS LOG = logging . getLogger ( __name__ ) S3_PARTITION_MAP = { \"aws\" : \"amazonaws.com\" , \"aws-cn\" : \"amazonaws.com.cn\" , \"aws-us-gov\" : \"amazonaws.com\" , } FIRST_CAP_RE = re . compile ( \"(.)([A-Z][a-z]+)\" ) ALL_CAP_RE = re . compile ( \"([a-z0-9])([A-Z])\" ) def region_from_stack_id ( stack_id ): return stack_id . split ( \":\" )[ 3 ] def name_from_stack_id ( stack_id ): return stack_id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def s3_url_maker ( bucket , key , s3_client , autobucket = False ): retries = 10 while True : try : try : response = s3_client . get_bucket_location ( Bucket = bucket ) location = response [ \"LocationConstraint\" ] except ClientError as e : if e . response [ \"Error\" ][ \"Code\" ] != \"AccessDenied\" : raise resp = requests . get ( f \"https:// { bucket } .s3.amazonaws.com/ { key } \" ) location = resp . headers . get ( \"x-amz-bucket-region\" ) if not location : # pylint: disable=raise-missing-from raise TaskCatException ( f \"failed to discover region for bucket { bucket } \" ) break except s3_client . exceptions . NoSuchBucket : if not autobucket or retries < 1 : raise retries -= 1 sleep ( 5 ) # default case for us-east-1 which returns no location url = f \"https:// { bucket } .s3.us-east-1.amazonaws.com/ { key } \" if location : domain = get_s3_domain ( location ) url = f \"https:// { bucket } .s3. { location } . { domain } / { key } \" return url def get_s3_domain ( region ): try : return S3_PARTITION_MAP [ REGIONS [ region ]] except KeyError : # pylint: disable=raise-missing-from raise TaskCatException ( f \"cannot find the S3 hostname for region { region } \" ) def s3_bucket_name_from_url ( url ): return url . split ( \"//\" )[ 1 ] . split ( \".\" )[ 0 ] def s3_key_from_url ( url ): return \"/\" . join ( url . split ( \"//\" )[ 1 ] . split ( \"/\" )[ 1 :]) class CommonTools : def __init__ ( self , stack_name ): self . stack_name = stack_name @staticmethod def regxfind ( re_object , data_line ): \"\"\" Returns the matching string. :param re_object: Regex object :param data_line: String to be searched :return: Matching String if found, otherwise return 'Not-found' \"\"\" security_group = re_object . search ( data_line ) if security_group : return str ( security_group . group ()) return str ( \"Not-found\" ) def exit_with_code ( code , msg = \"\" ): if msg : LOG . error ( msg ) sys . exit ( code ) def make_dir ( path , ignore_exists = True ): path = os . path . abspath ( path ) if ignore_exists and os . path . isdir ( path ): return os . makedirs ( path ) def param_list_to_dict ( original_keys ): # Setup a list index dictionary. # - Used to give an Parameter => Index mapping for replacement. param_index = {} if not isinstance ( original_keys , list ): # pylint: disable=raise-missing-from raise TaskCatException ( 'Invalid parameter file, outermost json element must be a list (\"[]\")' ) for ( idx , param_dict ) in enumerate ( original_keys ): if not isinstance ( param_dict , dict ): # pylint: disable=raise-missing-from raise TaskCatException ( 'Invalid parameter %s parameters must be of type dict (\" {} \")' % param_dict ) if \"ParameterKey\" not in param_dict or \"ParameterValue\" not in param_dict : # pylint: disable=raise-missing-from raise TaskCatException ( f \"Invalid parameter { param_dict } all items must \" f \"have both ParameterKey and ParameterValue keys\" ) key = param_dict [ \"ParameterKey\" ] param_index [ key ] = idx return param_index def merge_dicts ( list_of_dicts ): merged_dict = {} for single_dict in list_of_dicts : merged_dict = { ** merged_dict , ** single_dict } return merged_dict def pascal_to_snake ( pascal ): sub = ALL_CAP_RE . sub ( r \"\\1_\\2\" , pascal ) return ALL_CAP_RE . sub ( r \"\\1_\\2\" , sub ) . lower () def merge_nested_dict ( old , new ): for k , v in new . items (): if isinstance ( old . get ( k ), dict ) and isinstance ( v , collections . Mapping ): merge_nested_dict ( old [ k ], v ) else : old [ k ] = v def ordered_dump ( data , stream = None , dumper = yaml . Dumper , ** kwds ): class OrderedDumper ( dumper ): # pylint: disable=too-many-ancestors pass def _dict_representer ( dumper , data ): return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items () ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , stream , OrderedDumper , ** kwds ) def deep_get ( dictionary , keys , default = None ): zulu = reduce ( lambda d , key : d . get ( key , default ) if isinstance ( d , dict ) else default , keys . split ( \"/\" ), dictionary , ) return zulu def neglect_submodule_templates ( project_root , template_list ): template_dict = {} # one template object per path. for template in template_list : template_dict [ template . template_path ] = template for template_descendent in template . descendents : template_dict [ template_descendent . template_path ] = template_descendent # Removing those within a submodule. submodule_path_prefixes = [] try : gitmodule_config = ConfigFile . from_path ( Path ( project_root / \".gitmodules\" )) except FileNotFoundError : return template_list for submodule_path , _ , _ in parse_submodules ( gitmodule_config ): submodule_path_prefixes . append ( Path ( project_root / submodule_path . decode ( \"utf-8\" )) ) finalized_templates = [] for template_obj in list ( template_dict . values ()): gitmodule_template = False for gm_path in submodule_path_prefixes : if gm_path in template_obj . template_path . parents : gitmodule_template = True if not gitmodule_template : finalized_templates . append ( template_obj ) return finalized_templates def determine_profile_for_region ( auth_dict , region ): profile = auth_dict . get ( region , auth_dict . get ( \"default\" , \"default\" )) return profile def fetch_ssm_parameter_value ( boto_client , parameter_path ): ssm = boto_client ( \"ssm\" ) response = ssm . get_parameter ( Name = parameter_path ) return response [ \"Parameter\" ][ \"Value\" ] def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ): secrets_manager = boto_client ( \"secretsmanager\" ) try : response = secrets_manager . get_secret_value ( SecretId = secret_arn )[ \"SecretString\" ] except Exception as e : # pylint: disable=raise-missing-from raise TaskCatException ( \"ARN: {} encountered an error: {} \" . format ( secret_arn , str ( e )) ) return response","title":"Module _common_utils"},{"location":"reference/_common_utils.html#variables","text":"ALL_CAP_RE FIRST_CAP_RE LOG REGIONS S3_PARTITION_MAP","title":"Variables"},{"location":"reference/_common_utils.html#functions","text":"","title":"Functions"},{"location":"reference/_common_utils.html#deep_get","text":"def deep_get ( dictionary , keys , default = None ) View Source def deep_get ( dictionary , keys , default = None ) : zulu = reduce ( lambda d , key : d . get ( key , default ) if isinstance ( d , dict ) else default , keys . split ( \" / \" ) , dictionary , ) return zulu","title":"deep_get"},{"location":"reference/_common_utils.html#determine_profile_for_region","text":"def determine_profile_for_region ( auth_dict , region ) View Source def determine_profile_for_region ( auth_dict , region ) : profile = auth_dict . get ( region , auth_dict . get ( \"default\" , \"default\" )) return profile","title":"determine_profile_for_region"},{"location":"reference/_common_utils.html#exit_with_code","text":"def exit_with_code ( code , msg = '' ) View Source def exit_with_code ( code , msg = \"\" ) : if msg : LOG . error ( msg ) sys . exit ( code )","title":"exit_with_code"},{"location":"reference/_common_utils.html#fetch_secretsmanager_parameter_value","text":"def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ) View Source def fetch_secretsmanager_parameter_value ( boto_client , secret_arn ) : secrets_manager = boto_client ( \" secretsmanager \" ) try : response = secrets_manager . get_secret_value ( SecretId = secret_arn ) [ \" SecretString \" ] except Exception as e : # pylint : disable = raise - missing - from raise TaskCatException ( \" ARN: {} encountered an error: {} \" . format ( secret_arn , str ( e )) ) return response","title":"fetch_secretsmanager_parameter_value"},{"location":"reference/_common_utils.html#fetch_ssm_parameter_value","text":"def fetch_ssm_parameter_value ( boto_client , parameter_path ) View Source def fetch_ssm_parameter_value ( boto_client , parameter_path ) : ssm = boto_client ( \" ssm \" ) response = ssm . get_parameter ( Name = parameter_path ) return response [ \" Parameter \" ][ \" Value \" ]","title":"fetch_ssm_parameter_value"},{"location":"reference/_common_utils.html#get_s3_domain","text":"def get_s3_domain ( region ) View Source def get_s3_domain ( region ) : try : return S3_PARTITION_MAP [ REGIONS[region ] ] except KeyError : # pylint : disable = raise - missing - from raise TaskCatException ( f \"cannot find the S3 hostname for region {region}\" )","title":"get_s3_domain"},{"location":"reference/_common_utils.html#make_dir","text":"def make_dir ( path , ignore_exists = True ) View Source def make_dir ( path , ignore_exists = True ) : path = os . path . abspath ( path ) if ignore_exists and os . path . isdir ( path ) : return os . makedirs ( path )","title":"make_dir"},{"location":"reference/_common_utils.html#merge_dicts","text":"def merge_dicts ( list_of_dicts ) View Source def merge_dicts ( list_of_dicts ) : merged_dict = {} for single_dict in list_of_dicts : merged_dict = { ** merged_dict , ** single_dict } return merged_dict","title":"merge_dicts"},{"location":"reference/_common_utils.html#merge_nested_dict","text":"def merge_nested_dict ( old , new ) View Source def merge_nested_dict ( old , new ) : for k , v in new . items () : if isinstance ( old . get ( k ), dict ) and isinstance ( v , collections . Mapping ) : merge_nested_dict ( old [ k ] , v ) else : old [ k ] = v","title":"merge_nested_dict"},{"location":"reference/_common_utils.html#name_from_stack_id","text":"def name_from_stack_id ( stack_id ) View Source def name_from_stack_id ( stack_id ) : return stack_id . split ( \" : \" ) [ 5 ]. split ( \" / \" ) [ 1 ]","title":"name_from_stack_id"},{"location":"reference/_common_utils.html#neglect_submodule_templates","text":"def neglect_submodule_templates ( project_root , template_list ) View Source def neglect_submodule_templates ( project_root , template_list ) : template_dict = {} # one template object per path . for template in template_list : template_dict [ template . template_path ] = template for template_descendent in template . descendents : template_dict [ template_descendent . template_path ] = template_descendent # Removing those within a submodule . submodule_path_prefixes = [] try : gitmodule_config = ConfigFile . from_path ( Path ( project_root / \" .gitmodules \" )) except FileNotFoundError : return template_list for submodule_path , _ , _ in parse_submodules ( gitmodule_config ) : submodule_path_prefixes . append ( Path ( project_root / submodule_path . decode ( \" utf-8 \" )) ) finalized_templates = [] for template_obj in list ( template_dict . values ()) : gitmodule_template = False for gm_path in submodule_path_prefixes : if gm_path in template_obj . template_path . parents : gitmodule_template = True if not gitmodule_template : finalized_templates . append ( template_obj ) return finalized_templates","title":"neglect_submodule_templates"},{"location":"reference/_common_utils.html#ordered_dump","text":"def ordered_dump ( data , stream = None , dumper =< class ' yaml . dumper . Dumper '>, ** kwds ) View Source def ordered_dump ( data , stream = None , dumper = yaml . Dumper , ** kwds ) : class OrderedDumper ( dumper ) : # pylint : disable = too - many - ancestors pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items () ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , stream , OrderedDumper , ** kwds )","title":"ordered_dump"},{"location":"reference/_common_utils.html#param_list_to_dict","text":"def param_list_to_dict ( original_keys ) View Source def param_list_to_dict ( original_keys ) : # Setup a list index dictionary . # - Used to give an Parameter => Index mapping for replacement . param_index = {} if not isinstance ( original_keys , list ) : # pylint : disable = raise - missing - from raise TaskCatException ( 'Invalid parameter file, outermost json element must be a list (\"[]\")' ) for ( idx , param_dict ) in enumerate ( original_keys ) : if not isinstance ( param_dict , dict ) : # pylint : disable = raise - missing - from raise TaskCatException ( 'Invalid parameter %s parameters must be of type dict (\"{}\")' % param_dict ) if \"ParameterKey\" not in param_dict or \"ParameterValue\" not in param_dict : # pylint : disable = raise - missing - from raise TaskCatException ( f \"Invalid parameter {param_dict} all items must \" f \"have both ParameterKey and ParameterValue keys\" ) key = param_dict [ \"ParameterKey\" ] param_index [ key ] = idx return param_index","title":"param_list_to_dict"},{"location":"reference/_common_utils.html#pascal_to_snake","text":"def pascal_to_snake ( pascal ) View Source def pascal_to_snake ( pascal ) : sub = ALL_CAP_RE . sub ( r \" \\1_\\2 \" , pascal ) return ALL_CAP_RE . sub ( r \" \\1_\\2 \" , sub ) . lower ()","title":"pascal_to_snake"},{"location":"reference/_common_utils.html#region_from_stack_id","text":"def region_from_stack_id ( stack_id ) View Source def region_from_stack_id ( stack_id ) : return stack_id . split ( \":\" )[ 3 ]","title":"region_from_stack_id"},{"location":"reference/_common_utils.html#s3_bucket_name_from_url","text":"def s3_bucket_name_from_url ( url ) View Source def s3_bucket_name_from_url ( url ) : return url . split ( \" // \" ) [ 1 ]. split ( \" . \" ) [ 0 ]","title":"s3_bucket_name_from_url"},{"location":"reference/_common_utils.html#s3_key_from_url","text":"def s3_key_from_url ( url ) View Source def s3_key_from_url ( url ) : return \" / \" . join ( url . split ( \" // \" ) [ 1 ]. split ( \" / \" ) [ 1 :] )","title":"s3_key_from_url"},{"location":"reference/_common_utils.html#s3_url_maker","text":"def s3_url_maker ( bucket , key , s3_client , autobucket = False ) View Source def s3_url_maker ( bucket , key , s3_client , autobucket = False ) : retries = 10 while True: try: try: response = s3_client . get_bucket_location ( Bucket = bucket ) location = response [ \"LocationConstraint\" ] except ClientError as e: if e . response [ \"Error\" ][ \"Code\" ] != \"AccessDenied\" : raise resp = requests . get ( f \"https://{bucket}.s3.amazonaws.com/{key}\" ) location = resp . headers . get ( \"x-amz-bucket-region\" ) if not location: # pylint: disable = raise - missing - from raise TaskCatException ( f \"failed to discover region for bucket {bucket}\" ) break except s3_client . exceptions . NoSuchBucket: if not autobucket or retries < 1 : raise retries -= 1 sleep ( 5 ) # default case for us - east - 1 which returns no location url = f \"https://{bucket}.s3.us-east-1.amazonaws.com/{key}\" if location: domain = get_s3_domain ( location ) url = f \"https://{bucket}.s3.{location}.{domain}/{key}\" return url","title":"s3_url_maker"},{"location":"reference/_common_utils.html#classes","text":"","title":"Classes"},{"location":"reference/_common_utils.html#commontools","text":"class CommonTools ( stack_name )","title":"CommonTools"},{"location":"reference/_common_utils.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_common_utils.html#regxfind","text":"def regxfind ( re_object , data_line ) Returns the matching string. Parameters: Name Type Description Default re_object None Regex object None data_line None String to be searched None Returns: Type Description None Matching String if found, otherwise return 'Not-found' View Source @staticmethod def regxfind ( re_object , data_line ) : \"\"\" Returns the matching string. :param re_object: Regex object :param data_line: String to be searched :return: Matching String if found, otherwise return 'Not-found' \"\"\" security_group = re_object . search ( data_line ) if security_group : return str ( security_group . group ()) return str ( \"Not-found\" )","title":"regxfind"},{"location":"reference/_config.html","text":"Module _config None None View Source import logging import os import uuid from pathlib import Path from typing import Dict , Optional , Union import yaml from botocore.exceptions import ClientError from taskcat._cfn.template import Template , tcat_template_cache from taskcat._client_factory import Boto3Cache from taskcat._dataclasses import ( BaseConfig , RegionObj , S3BucketObj , Tag , TestObj , TestRegion , generate_bucket_name , generate_regional_bucket_name , ) from taskcat._legacy_config import legacy_overrides , parse_legacy_config from taskcat._template_params import ParamGen from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) GENERAL = Path ( \"~/.taskcat.yml\" ) . expanduser () . resolve () PROJECT = Path ( \"./.taskcat.yml\" ) . resolve () PROJECT_ROOT = Path ( \"./\" ) . resolve () OVERRIDES = Path ( \"./.taskcat_overrides.yml\" ) . resolve () DEFAULTS = { \"project\" : { \"s3_enable_sig_v2\" : False , \"build_submodules\" : True , \"package_lambda\" : True , \"lambda_zip_path\" : \"lambda_functions/packages\" , \"lambda_source_path\" : \"lambda_functions/source\" , \"shorten_stack_name\" : False , } } class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from { file_path } \" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path { file_path } \" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from { file_path } \" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def get_regions ( self , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = {} for region in test . regions : # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \" { region . account_id }{ region . name } \" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \" { region . account_id }{ region . name } \" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \" { test . s3_bucket } - { region . name } \" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj @staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \" { self . config . project . name } /\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests Variables DEFAULTS GENERAL LOG OVERRIDES PROJECT PROJECT_ROOT Classes Config class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path ) Static methods create def create ( template_file : Optional [ pathlib . Path ] = None , args : Optional [ dict ] = None , global_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat_overrides.yml' ), env_vars : Optional [ dict ] = None , project_root : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) get_params_from_templates def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters Methods get_buckets def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings get_regions def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = {} for region in test . regions : # TODO : comon_utils / determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects get_rendered_parameters def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters get_templates def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates get_tests def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":" Config"},{"location":"reference/_config.html#module-_config","text":"None None View Source import logging import os import uuid from pathlib import Path from typing import Dict , Optional , Union import yaml from botocore.exceptions import ClientError from taskcat._cfn.template import Template , tcat_template_cache from taskcat._client_factory import Boto3Cache from taskcat._dataclasses import ( BaseConfig , RegionObj , S3BucketObj , Tag , TestObj , TestRegion , generate_bucket_name , generate_regional_bucket_name , ) from taskcat._legacy_config import legacy_overrides , parse_legacy_config from taskcat._template_params import ParamGen from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) GENERAL = Path ( \"~/.taskcat.yml\" ) . expanduser () . resolve () PROJECT = Path ( \"./.taskcat.yml\" ) . resolve () PROJECT_ROOT = Path ( \"./\" ) . resolve () OVERRIDES = Path ( \"./.taskcat_overrides.yml\" ) . resolve () DEFAULTS = { \"project\" : { \"s3_enable_sig_v2\" : False , \"build_submodules\" : True , \"package_lambda\" : True , \"lambda_zip_path\" : \"lambda_functions/packages\" , \"lambda_source_path\" : \"lambda_functions/source\" , \"shorten_stack_name\" : False , } } class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from { file_path } \" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path { file_path } \" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from { file_path } \" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def get_regions ( self , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = {} for region in test . regions : # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \" { region . account_id }{ region . name } \" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \" { region . account_id }{ region . name } \" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \" { test . s3_bucket } - { region . name } \" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj @staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \" { self . config . project . name } /\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"Module _config"},{"location":"reference/_config.html#variables","text":"DEFAULTS GENERAL LOG OVERRIDES PROJECT PROJECT_ROOT","title":"Variables"},{"location":"reference/_config.html#classes","text":"","title":"Classes"},{"location":"reference/_config.html#config","text":"class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path )","title":"Config"},{"location":"reference/_config.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_config.html#create","text":"def create ( template_file : Optional [ pathlib . Path ] = None , args : Optional [ dict ] = None , global_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat_overrides.yml' ), env_vars : Optional [ dict ] = None , project_root : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root )","title":"create"},{"location":"reference/_config.html#get_params_from_templates","text":"def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters","title":"get_params_from_templates"},{"location":"reference/_config.html#methods","text":"","title":"Methods"},{"location":"reference/_config.html#get_buckets","text":"def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings","title":"get_buckets"},{"location":"reference/_config.html#get_regions","text":"def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = {} for region in test . regions : # TODO : comon_utils / determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects","title":"get_regions"},{"location":"reference/_config.html#get_rendered_parameters","text":"def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters","title":"get_rendered_parameters"},{"location":"reference/_config.html#get_templates","text":"def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates","title":"get_templates"},{"location":"reference/_config.html#get_tests","text":"def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"get_tests"},{"location":"reference/_dataclasses.html","text":"Module _dataclasses None None View Source import json import logging import random import string from dataclasses import dataclass , field from pathlib import Path from typing import Any , Dict , List , NewType , Optional , Union from uuid import UUID , uuid5 import boto3 from dataclasses_jsonschema import FieldEncoder , JsonSchemaMixin from taskcat._cfn.template import Template from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_nested_dict from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) # property descriptions METADATA = { \"project__name\" : { \"description\" : \"Project name, used as s3 key prefix when \" \"uploading objects\" }, \"auth\" : { \"description\" : \"AWS authentication section\" }, \"project__owner\" : { \"description\" : \"email address for project owner (not used at present)\" }, \"regions\" : { \"description\" : \"List of AWS regions\" }, \"az_ids\" : { \"description\" : \"List of Availablilty Zones ID's to exclude when generating \" \"availability zones\" }, \"package_lambda\" : { \"description\" : \"Package Lambda functions into zips before uploading to s3, \" \"set to false to disable\" }, \"s3_regional_buckets\" : { \"description\" : \"Enable regional auto-buckets.\" }, \"lambda_zip_path\" : { \"description\" : \"Path relative to the project root to place Lambda zip \" \"files, default is 'lambda_functions/zips'\" }, \"lambda_source_path\" : { \"description\" : \"Path relative to the project root containing Lambda zip \" \"files, default is 'lambda_functions/source'\" }, \"s3_bucket\" : { \"description\" : \"Name of S3 bucket to upload project to, if left out \" \"a bucket will be auto-generated\" }, \"parameters\" : { \"description\" : \"Parameter key-values to pass to CloudFormation, \" \"parameters provided in global config take precedence\" }, \"build_submodules\" : { \"description\" : \"Build Lambda zips recursively for submodules, \" \"set to false to disable\" }, \"template\" : { \"description\" : \"path to template file relative to the project \" \"config file path\" }, \"tags\" : { \"description\" : \"Tags to apply to CloudFormation template\" }, \"enable_sig_v2\" : { \"description\" : \"Enable (deprecated) sigv2 access to auto-generated buckets\" }, \"s3_object_acl\" : { \"description\" : \"ACL for uploaded s3 objects, defaults to 'private'\" }, \"shorten_stack_name\" : { \"description\" : \"Shorten stack names generated for tests, set to true to enable\" }, \"role_name\" : { \"description\" : \"Role name to use when launching CFN Stacks.\" }, \"stack_name\" : { \"description\" : \"Cloudformation Stack Name\" }, \"stack_name_prefix\" : { \"description\" : \"Prefix to apply to generated CFN Stack Name\" }, \"stack_name_suffix\" : { \"description\" : \"Suffix to apply to generated CFN Stack Name\" }, \"prehooks\" : { \"description\" : \"hooks to execute prior to executing tests\" }, \"posthooks\" : { \"description\" : \"hooks to execute after executing tests\" }, \"type\" : { \"description\" : \"hook type\" }, \"config\" : { \"description\" : \"hook configuration\" }, } # types ParameterKey = NewType ( \"ParameterKey\" , str ) ParameterValue = Union [ str , int , bool , List [ Union [ int , str ]]] TagKey = NewType ( \"TagKey\" , str ) TagValue = NewType ( \"TagValue\" , str ) S3Acl = NewType ( \"S3Acl\" , str ) Region = NewType ( \"Region\" , str ) AlNumDash = NewType ( \"AlNumDash\" , str ) ProjectName = NewType ( \"ProjectName\" , AlNumDash ) S3BucketName = NewType ( \"S3BucketName\" , AlNumDash ) TestName = NewType ( \"TestName\" , AlNumDash ) AzId = NewType ( \"AzId\" , str ) Templates = NewType ( \"Templates\" , Dict [ TestName , Template ]) # regex validation class ParameterKeyField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"[a-zA-Z0-9]*^$\" , \"Description\" : \"CloudFormation parameter name, can contain letters and \" \"numbers only\" , } JsonSchemaMixin . register_field_encoders ({ ParameterKey : ParameterKeyField ()}) class RegionField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|\" r \"west|southeast|southwest|northeast|northwest)-[0-9]$\" , \"description\" : \"AWS Region name eg.: 'us-east-1'\" , } JsonSchemaMixin . register_field_encoders ({ Region : RegionField ()}) class S3AclField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^(\" r \"bucket-owner-full-control|\" r \"bucket-owner-read|\" r \"authenticated-read|\" r \"aws-exec-read|\" r \"public-read-write|\" r \"public-read|\" r \"private)$\" , \"description\" : \"Must be a valid S3 ACL (private, public-read, \" \"aws-exec-read, public-read-write, authenticated-read, \" \"bucket-owner-read, bucket-owner-full-control)\" , } JsonSchemaMixin . register_field_encoders ({ S3Acl : S3AclField ()}) class AlNumDashField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^[a-z0-9-]*$\" , \"description\" : \"accepts lower case letters, numbers and -\" , } JsonSchemaMixin . register_field_encoders ({ AlNumDash : AlNumDashField ()}) class AzIdField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^((ap|eu|us|sa|ca|cn|af|me)(n|s|e|w|c|ne|se|nw|sw)\" r \"[0-9]-az[0-9]|usw2-lax1-az(1|2))$\" , \"description\" : \"Availability Zone ID, eg.: 'use1-az1'\" , } JsonSchemaMixin . register_field_encoders ({ AzId : AzIdField ()}) # dataclasses @dataclass class RegionObj : name : str account_id : str partition : str profile : str taskcat_id : UUID _boto3_cache : Boto3Cache _role_name : Optional [ str ] def client ( self , service : str ): return self . _boto3_cache . client ( service , region = self . name , profile = self . profile ) @property def session ( self ): return self . _boto3_cache . session ( region = self . name , profile = self . profile ) @property def role_arn ( self ): if self . _role_name : return f \"arn: { self . partition } :iam:: { self . account_id } :role/ { self . _role_name } \" return None @dataclass class S3BucketObj : name : str region : str account_id : str partition : str s3_client : boto3 . client sigv4 : bool auto_generated : bool regional_buckets : bool object_acl : str taskcat_id : UUID @property def sigv4_policy ( self ): policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Test\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : f \"arn: { self . partition } :s3::: { self . name } /*\" , \"Condition\" : { \"StringEquals\" : { \"s3:signatureversion\" : \"AWS\" }}, } ], } return json . dumps ( policy ) def create ( self ): if self . _bucket_matches_existing (): return kwargs = { \"Bucket\" : self . name } if self . region != \"us-east-1\" : kwargs [ \"CreateBucketConfiguration\" ] = { \"LocationConstraint\" : self . region } self . s3_client . create_bucket ( ** kwargs ) error = None try : self . s3_client . get_waiter ( \"bucket_exists\" ) . wait ( Bucket = self . name ) if not self . regional_buckets : self . s3_client . put_bucket_tagging ( Bucket = self . name , Tagging = { \"TagSet\" : [{ \"Key\" : \"taskcat-id\" , \"Value\" : self . taskcat_id . hex }] }, ) if self . sigv4 : self . s3_client . put_bucket_policy ( Bucket = self . name , Policy = self . sigv4_policy ) except Exception as e : # pylint: disable=broad-except error = e try : self . s3_client . delete_bucket ( Bucket = self . name ) except Exception as inner_e : # pylint: disable=broad-except LOG . warning ( f \"failed to remove bucket { self . name } : { inner_e } \" ) if error : raise error def empty ( self ): if not self . auto_generated : LOG . error ( f \"Will not empty bucket created outside of taskcat { self . name } \" ) return objects_to_delete = [] pages = self . s3_client . get_paginator ( \"list_objects_v2\" ) . paginate ( Bucket = self . name ) for page in pages : objects = [] for obj in page . get ( \"Contents\" , []): del_obj = { \"Key\" : obj [ \"Key\" ]} if obj . get ( \"VersionId\" ): del_obj [ \"VersionId\" ] = obj [ \"VersionId\" ] objects . append ( del_obj ) objects_to_delete += objects batched_objects = [ objects_to_delete [ i : i + 1000 ] for i in range ( 0 , len ( objects_to_delete ), 1000 ) ] for objects in batched_objects : if objects : self . s3_client . delete_objects ( Bucket = self . name , Delete = { \"Objects\" : objects } ) def delete ( self , delete_objects = False ): if not self . auto_generated : LOG . info ( f \"Will not delete bucket created outside of taskcat { self . name } \" ) return if delete_objects : try : self . empty () except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \"Cannot delete bucket { self . name } as it does not exist\" ) return try : self . s3_client . delete_bucket ( Bucket = self . name ) except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \"Cannot delete bucket { self . name } as it does not exist\" ) def _bucket_matches_existing ( self ): try : location = self . s3_client . get_bucket_location ( Bucket = self . name )[ \"LocationConstraint\" ] location = location if location else \"us-east-1\" except self . s3_client . exceptions . NoSuchBucket : location = None if location != self . region and location is not None : raise TaskCatException ( f \"bucket { self . name } already exists, but is not in \" f \"the expected region { self . region } , expected { location } \" ) if location : if self . regional_buckets : return True tags = self . s3_client . get_bucket_tagging ( Bucket = self . name )[ \"TagSet\" ] tags = { t [ \"Key\" ]: t [ \"Value\" ] for t in tags } uid = tags . get ( \"taskcat-id\" ) uid = UUID ( uid ) if uid else uid if uid != self . taskcat_id : raise TaskCatException ( f \"bucket { self . name } already exists, but does not have a matching\" f \" uuid\" ) return True return False class Tag : def __init__ ( self , tag_dict : dict ): if isinstance ( tag_dict , Tag ): tag_dict = { \"Key\" : tag_dict . key , \"Value\" : tag_dict . value } self . key : str = tag_dict [ \"Key\" ] self . value : str = tag_dict [ \"Value\" ] def dump ( self ): tag_dict = { \"Key\" : self . key , \"Value\" : self . value } return tag_dict @dataclass class TestRegion ( RegionObj ): s3_bucket : S3BucketObj parameters : Dict [ ParameterKey , ParameterValue ] @classmethod def from_region_obj ( cls , region : RegionObj , s3_bucket , parameters ): return cls ( s3_bucket = s3_bucket , parameters = parameters , ** region . __dict__ ) @dataclass # pylint: disable=too-many-instance-attributes class TestObj : def __init__ ( self , template_path : Path , template : Template , project_root : Path , name : TestName , regions : List [ TestRegion ], tags : List [ Tag ], uid : UUID , _project_name : str , _stack_name : str = \"\" , _stack_name_prefix : str = \"\" , _stack_name_suffix : str = \"\" , _shorten_stack_name : bool = False , ): self . template_path = template_path self . template = template self . project_root = project_root self . name = name self . regions = regions self . tags = tags self . uid = uid self . _project_name = _project_name self . _stack_name = _stack_name self . _stack_name_prefix = _stack_name_prefix self . _stack_name_suffix = _stack_name_suffix self . _shorten_stack_name = _shorten_stack_name self . _assert_param_combo () def _assert_param_combo ( self ): throw = False if self . _stack_name_prefix and self . _stack_name_suffix : throw = True if self . _stack_name and ( self . _stack_name_prefix or self . _stack_name_suffix ): throw = True if throw : raise TaskCatException ( \"Please provide only *ONE* of stack_name, stack_name_prefix, \\ or stack_name_suffix \" ) @property def stack_name ( self ): prefix = \"tCaT-\" if self . _stack_name : return self . _stack_name # TODO: prefix *OR* suffix if self . _stack_name_prefix : if self . _shorten_stack_name : return \" {}{} - {} \" . format ( self . _stack_name_prefix , self . name , self . uid . hex [: 6 ] ) return \" {}{} - {} - {} \" . format ( self . _stack_name_prefix , self . _project_name , self . name , self . uid . hex ) if self . _stack_name_suffix : return \" {}{} - {} - {} \" . format ( prefix , self . _project_name , self . name , self . _stack_name_suffix ) if self . _shorten_stack_name : return \" {}{} - {} \" . format ( prefix , self . name , self . uid . hex [: 6 ]) return \" {}{} - {} - {} \" . format ( prefix , self . _project_name , self . name , self . uid . hex ) @dataclass class HookData ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Hook definition\"\"\" type : Optional [ str ] = field ( default = None , metadata = METADATA [ \"type\" ]) config : Optional [ Dict [ str , Any ]] = field ( default = None , metadata = METADATA [ \"config\" ]) @dataclass class GeneralConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"General configuration settings.\"\"\" parameters : Optional [ Dict [ ParameterKey , ParameterValue ]] = field ( default = None , metadata = METADATA [ \"parameters\" ] ) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) s3_bucket : Optional [ str ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ]) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) @dataclass class TestConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Test specific configuration section.\"\"\" template : Optional [ str ] = field ( default = None , metadata = METADATA [ \"template\" ]) parameters : Dict [ ParameterKey , ParameterValue ] = field ( default_factory = dict , metadata = METADATA [ \"parameters\" ] ) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) s3_bucket : Optional [ S3BucketName ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ] ) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) az_blacklist : Optional [ List [ AzId ]] = field ( default = None , metadata = METADATA [ \"az_ids\" ] ) role_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"role_name\" ]) stack_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name\" ]) stack_name_prefix : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name_prefix\" ] ) stack_name_suffix : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name_suffix\" ] ) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) # pylint: disable=too-many-instance-attributes @dataclass class ProjectConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Project specific configuration section\"\"\" name : Optional [ ProjectName ] = field ( default = None , metadata = METADATA [ \"project__name\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) owner : Optional [ str ] = field ( default = None , metadata = METADATA [ \"project__owner\" ]) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) az_blacklist : Optional [ List [ AzId ]] = field ( default = None , metadata = METADATA [ \"az_ids\" ] ) package_lambda : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"package_lambda\" ] ) lambda_zip_path : Optional [ str ] = field ( default = None , metadata = METADATA [ \"lambda_zip_path\" ] ) lambda_source_path : Optional [ str ] = field ( default = None , metadata = METADATA [ \"lambda_source_path\" ] ) s3_bucket : Optional [ S3BucketName ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ] ) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) parameters : Optional [ Dict [ ParameterKey , ParameterValue ]] = field ( default = None , metadata = METADATA [ \"parameters\" ] ) build_submodules : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"build_submodules\" ] ) template : Optional [ str ] = field ( default = None , metadata = METADATA [ \"template\" ]) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) s3_enable_sig_v2 : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"enable_sig_v2\" ] ) s3_object_acl : Optional [ S3Acl ] = field ( default = None , metadata = METADATA [ \"s3_object_acl\" ] ) shorten_stack_name : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"shorten_stack_name\" ] ) role_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"role_name\" ]) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) PROPAGATE_KEYS = [ \"tags\" , \"parameters\" , \"auth\" ] PROPOGATE_ITEMS = [ \"regions\" , \"s3_bucket\" , \"template\" , \"az_blacklist\" , \"s3_regional_buckets\" , \"prehooks\" , \"posthooks\" , ] def generate_regional_bucket_name ( region_obj : RegionObj , prefix : str = \"tcat\" ): if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable=len-as-condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) hashed_account_id = uuid5 ( name = str ( region_obj . account_id ), namespace = UUID ( int = 0 ) ) . hex return f \" { prefix } - { hashed_account_id } - { region_obj . name } \" def generate_bucket_name ( project : str , prefix : str = \"tcat\" ): if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable=len-as-condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) alnum = string . ascii_lowercase + string . digits suffix = \"\" . join ( random . choice ( alnum ) for i in range ( 8 )) # nosec: B311 mid = f \"- { project } -\" avail_len = 63 - len ( mid ) mid = mid [: avail_len ] return f \" { prefix }{ mid }{ suffix } \" # pylint raises false positive due to json-dataclass # pylint: disable=no-member @dataclass class BaseConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Taskcat configuration file\"\"\" general : GeneralConfig = field ( default_factory = GeneralConfig ) project : ProjectConfig = field ( default_factory = ProjectConfig ) tests : Dict [ TestName , TestConfig ] = field ( default_factory = dict ) # pylint doesn't like instance variables being added in post_init # pylint: disable=attribute-defined-outside-init def __post_init__ ( self ): self . _source : Dict [ str , Any ] = {} self . _propogate () self . set_source ( \"UNKNOWN\" ) self . _propogate_source () @staticmethod def _merge ( source , dest ): for section_key , section_value in source . items (): if section_key in PROPAGATE_KEYS + PROPOGATE_ITEMS : if section_key not in dest : dest [ section_key ] = section_value continue if section_key in PROPAGATE_KEYS : for key , value in section_value . items (): dest [ section_key ][ key ] = value return dest def _propogate ( self ): project_dict = self . _merge ( self . general . to_dict (), self . project . to_dict ()) self . project = ProjectConfig . from_dict ( project_dict ) for test_key , test in self . tests . items (): test_dict = self . _merge ( self . project . to_dict (), test . to_dict ()) self . tests [ test_key ] = TestConfig . from_dict ( test_dict ) def _propogate_source ( self ): self . _source [ \"project\" ] = self . _merge ( self . _source [ \"general\" ], self . _source [ \"project\" ] ) for test_key in self . _source [ \"tests\" ]: test = self . _merge ( self . _source [ \"project\" ], self . _source [ \"tests\" ][ test_key ]) self . _source [ \"tests\" ][ test_key ] = test def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Optional [ Union [ str , dict ]]: base_case = False if dest is None : base_case = True self . _source = self . to_dict () dest = self . _source if not isinstance ( dest , dict ): return source_name if isinstance ( dest , dict ): for item in dest : dest [ item ] = self . set_source ( source_name , dest [ item ]) if not base_case : return dest return None @classmethod def merge ( cls , base_config : \"BaseConfig\" , merge_config : \"BaseConfig\" ) -> \"BaseConfig\" : merged = base_config . to_dict () merge_nested_dict ( merged , merge_config . to_dict ()) merged_source = base_config . _source . copy () merge_nested_dict ( merged_source , merge_config . _source ) config = cls . from_dict ( merged ) config . _source = merged_source config . _propogate_source () # pylint: disable=protected-access return config Variables LOG METADATA PROPAGATE_KEYS PROPOGATE_ITEMS ParameterValue Functions AlNumDash def AlNumDash ( x ) View Source def new_type ( x ) : return x AzId def AzId ( x ) View Source def new_type ( x ) : return x ParameterKey def ParameterKey ( x ) View Source def new_type ( x ) : return x ProjectName def ProjectName ( x ) View Source def new_type ( x ) : return x Region def Region ( x ) View Source def new_type ( x ) : return x S3Acl def S3Acl ( x ) View Source def new_type ( x ) : return x S3BucketName def S3BucketName ( x ) View Source def new_type ( x ) : return x TagKey def TagKey ( x ) View Source def new_type ( x ) : return x TagValue def TagValue ( x ) View Source def new_type ( x ) : return x Templates def Templates ( x ) View Source def new_type ( x ) : return x TestName def TestName ( x ) View Source def new_type ( x ) : return x generate_bucket_name def generate_bucket_name ( project : str , prefix : str = 'tcat' ) View Source def generate_bucket_name ( project : str , prefix : str = \" tcat \" ) : if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint : disable = len - as - condition raise TaskCatException ( \" prefix must be between 1 and 8 characters long \" ) alnum = string . ascii_lowercase + string . digits suffix = \"\" . join ( random . choice ( alnum ) for i in range ( 8 )) # nosec : B311 mid = f \" -{project}- \" avail_len = 63 - len ( mid ) mid = mid [: avail_len ] return f \" {prefix}{mid}{suffix} \" generate_regional_bucket_name def generate_regional_bucket_name ( region_obj : _dataclasses . RegionObj , prefix : str = 'tcat' ) View Source def generate_regional_bucket_name ( region_obj: RegionObj , prefix: str = \"tcat\" ) : if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable = len - as - condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) hashed_account_id = uuid5 ( name = str ( region_obj . account_id ), namespace = UUID ( int = 0 ) ). hex return f \"{prefix}-{hashed_account_id}-{region_obj.name}\" Classes AlNumDashField class AlNumDashField ( / , * args , ** kwargs ) Ancestors (in MRO) dataclasses_jsonschema.field_types.FieldEncoder typing.Generic Instance variables json_schema Methods to_python def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value ) to_wire def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value ) AzIdField class AzIdField ( / , * args , ** kwargs ) Ancestors (in MRO) dataclasses_jsonschema.field_types.FieldEncoder typing.Generic Instance variables json_schema Methods to_python def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value ) to_wire def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value ) BaseConfig class BaseConfig ( general : _dataclasses . GeneralConfig = < factory > , project : _dataclasses . ProjectConfig = < factory > , tests : Dict [ TestName , _dataclasses . TestConfig ] = < factory > ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema merge def merge ( base_config : 'BaseConfig' , merge_config : 'BaseConfig' ) -> 'BaseConfig' View Source @classmethod def merge ( cls , base_config : \"BaseConfig\" , merge_config : \"BaseConfig\" ) -> \"BaseConfig\" : merged = base_config . to_dict () merge_nested_dict ( merged , merge_config . to_dict ()) merged_source = base_config . _source . copy () merge_nested_dict ( merged_source , merge_config . _source ) config = cls . from_dict ( merged ) config . _source = merged_source config . _propogate_source () # pylint : disable = protected - access return config register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods set_source def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Union [ str , dict , NoneType ] View Source def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Optional [ Union[str, dict ] ]: base_case = False if dest is None : base_case = True self . _source = self . to_dict () dest = self . _source if not isinstance ( dest , dict ) : return source_name if isinstance ( dest , dict ) : for item in dest : dest [ item ] = self . set_source ( source_name , dest [ item ] ) if not base_case : return dest return None to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) GeneralConfig class GeneralConfig ( parameters : Optional [ Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]]] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , auth : Optional [ Dict [ Region , str ]] = None , s3_bucket : Optional [ str ] = None , s3_regional_buckets : Optional [ bool ] = None , regions : Optional [ List [ Region ]] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Class variables auth parameters posthooks prehooks regions s3_bucket s3_regional_buckets tags Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) HookData class HookData ( type : Optional [ str ] = None , config : Optional [ Dict [ str , Any ]] = None ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Class variables config type Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) ParameterKeyField class ParameterKeyField ( / , * args , ** kwargs ) Ancestors (in MRO) dataclasses_jsonschema.field_types.FieldEncoder typing.Generic Instance variables json_schema Methods to_python def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value ) to_wire def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value ) ProjectConfig class ProjectConfig ( name : Optional [ ProjectName ] = None , auth : Optional [ Dict [ Region , str ]] = None , owner : Optional [ str ] = None , regions : Optional [ List [ Region ]] = None , az_blacklist : Optional [ List [ AzId ]] = None , package_lambda : Optional [ bool ] = None , lambda_zip_path : Optional [ str ] = None , lambda_source_path : Optional [ str ] = None , s3_bucket : Optional [ S3BucketName ] = None , s3_regional_buckets : Optional [ bool ] = None , parameters : Optional [ Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]]] = None , build_submodules : Optional [ bool ] = None , template : Optional [ str ] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , s3_enable_sig_v2 : Optional [ bool ] = None , s3_object_acl : Optional [ S3Acl ] = None , shorten_stack_name : Optional [ bool ] = None , role_name : Optional [ str ] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Class variables auth az_blacklist build_submodules lambda_source_path lambda_zip_path name owner package_lambda parameters posthooks prehooks regions role_name s3_bucket s3_enable_sig_v2 s3_object_acl s3_regional_buckets shorten_stack_name tags template Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) RegionField class RegionField ( / , * args , ** kwargs ) Ancestors (in MRO) dataclasses_jsonschema.field_types.FieldEncoder typing.Generic Instance variables json_schema Methods to_python def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value ) to_wire def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value ) RegionObj class RegionObj ( name : str , account_id : str , partition : str , profile : str , taskcat_id : uuid . UUID , _boto3_cache : taskcat . _client_factory . Boto3Cache , _role_name : Optional [ str ] ) Descendants _dataclasses.TestRegion Instance variables role_arn session Methods client def client ( self , service : str ) View Source def client ( self , service: str ) : return self . _boto3_cache . client ( service , region = self . name , profile = self . profile ) S3AclField class S3AclField ( / , * args , ** kwargs ) Ancestors (in MRO) dataclasses_jsonschema.field_types.FieldEncoder typing.Generic Instance variables json_schema Methods to_python def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value ) to_wire def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value ) S3BucketObj class S3BucketObj ( name : str , region : str , account_id : str , partition : str , s3_client : < function client at 0x103f3b4c0 > , sigv4 : bool , auto_generated : bool , regional_buckets : bool , object_acl : str , taskcat_id : uuid . UUID ) Instance variables sigv4_policy Methods create def create ( self ) View Source def create ( self ) : if self . _bucket_matches_existing () : return kwargs = { \"Bucket\" : self . name } if self . region != \"us-east-1\" : kwargs [ \"CreateBucketConfiguration\" ] = { \"LocationConstraint\" : self . region } self . s3_client . create_bucket ( ** kwargs ) error = None try: self . s3_client . get_waiter ( \"bucket_exists\" ). wait ( Bucket = self . name ) if not self . regional_buckets: self . s3_client . put_bucket_tagging ( Bucket = self . name , Tagging = { \"TagSet\" : [{ \"Key\" : \"taskcat-id\" , \"Value\" : self . taskcat_id . hex }] }, ) if self . sigv4: self . s3_client . put_bucket_policy ( Bucket = self . name , Policy = self . sigv4_policy ) except Exception as e: # pylint: disable = broad - except error = e try: self . s3_client . delete_bucket ( Bucket = self . name ) except Exception as inner_e: # pylint: disable = broad - except LOG . warning ( f \"failed to remove bucket {self.name}: {inner_e}\" ) if error: raise error delete def delete ( self , delete_objects = False ) View Source def delete ( self , delete_objects = False ) : if not self . auto_generated : LOG . info ( f \" Will not delete bucket created outside of taskcat {self.name} \" ) return if delete_objects : try : self . empty () except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \" Cannot delete bucket {self.name} as it does not exist \" ) return try : self . s3_client . delete_bucket ( Bucket = self . name ) except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \" Cannot delete bucket {self.name} as it does not exist \" ) empty def empty ( self ) View Source def empty ( self ) : if not self . auto_generated : LOG . error ( f \"Will not empty bucket created outside of taskcat { self . name } \") return objects_to_delete = [] pages = self . s3_client . get_paginator ( \"list_objects_v2\" ). paginate ( Bucket = self . name ) for page in pages : objects = [] for obj in page . get ( \"Contents\" , []) : del_obj = { \"Key\" : obj [ \"Key\" ]} if obj . get ( \"VersionId\" ) : del_obj [ \"VersionId\" ] = obj [ \"VersionId\" ] objects . append ( del_obj ) objects_to_delete += objects batched_objects = [ objects_to_delete [ i : i + 1000 ] for i in range ( 0 , len ( objects_to_delete ), 1000 ) ] for objects in batched_objects : if objects : self . s3_client . delete_objects ( Bucket = self . name , Delete = { \"Objects\" : objects } ) Tag class Tag ( tag_dict : dict ) Methods dump def dump ( self ) View Source def dump ( self ) : tag_dict = { \" Key \" : self . key , \" Value \" : self . value } return tag_dict TestConfig class TestConfig ( template : Optional [ str ] = None , parameters : Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]] = < factory > , regions : Optional [ List [ Region ]] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , auth : Optional [ Dict [ Region , str ]] = None , s3_bucket : Optional [ S3BucketName ] = None , s3_regional_buckets : Optional [ bool ] = None , az_blacklist : Optional [ List [ AzId ]] = None , role_name : Optional [ str ] = None , stack_name : Optional [ str ] = None , stack_name_prefix : Optional [ str ] = None , stack_name_suffix : Optional [ str ] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Class variables auth az_blacklist posthooks prehooks regions role_name s3_bucket s3_regional_buckets stack_name stack_name_prefix stack_name_suffix tags template Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) TestObj class TestObj ( template_path : pathlib . Path , template : taskcat . _cfn . template . Template , project_root : pathlib . Path , name : < function NewType .< locals >. new_type at 0x1130354c0 > , regions : List [ _dataclasses . TestRegion ], tags : List [ _dataclasses . Tag ], uid : uuid . UUID , _project_name : str , _stack_name : str = '' , _stack_name_prefix : str = '' , _stack_name_suffix : str = '' , _shorten_stack_name : bool = False ) Instance variables stack_name TestRegion class TestRegion ( name : str , account_id : str , partition : str , profile : str , taskcat_id : uuid . UUID , _boto3_cache : taskcat . _client_factory . Boto3Cache , _role_name : Optional [ str ], s3_bucket : _dataclasses . S3BucketObj , parameters : Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]] ) Ancestors (in MRO) _dataclasses.RegionObj Static methods from_region_obj def from_region_obj ( region : _dataclasses . RegionObj , s3_bucket , parameters ) View Source @classmethod def from_region_obj ( cls , region : RegionObj , s3_bucket , parameters ) : return cls ( s3_bucket = s3_bucket , parameters = parameters , ** region . __dict__ ) Instance variables role_arn session Methods client def client ( self , service : str ) View Source def client ( self , service: str ) : return self . _boto3_cache . client ( service , region = self . name , profile = self . profile )","title":" Dataclasses"},{"location":"reference/_dataclasses.html#module-_dataclasses","text":"None None View Source import json import logging import random import string from dataclasses import dataclass , field from pathlib import Path from typing import Any , Dict , List , NewType , Optional , Union from uuid import UUID , uuid5 import boto3 from dataclasses_jsonschema import FieldEncoder , JsonSchemaMixin from taskcat._cfn.template import Template from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_nested_dict from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) # property descriptions METADATA = { \"project__name\" : { \"description\" : \"Project name, used as s3 key prefix when \" \"uploading objects\" }, \"auth\" : { \"description\" : \"AWS authentication section\" }, \"project__owner\" : { \"description\" : \"email address for project owner (not used at present)\" }, \"regions\" : { \"description\" : \"List of AWS regions\" }, \"az_ids\" : { \"description\" : \"List of Availablilty Zones ID's to exclude when generating \" \"availability zones\" }, \"package_lambda\" : { \"description\" : \"Package Lambda functions into zips before uploading to s3, \" \"set to false to disable\" }, \"s3_regional_buckets\" : { \"description\" : \"Enable regional auto-buckets.\" }, \"lambda_zip_path\" : { \"description\" : \"Path relative to the project root to place Lambda zip \" \"files, default is 'lambda_functions/zips'\" }, \"lambda_source_path\" : { \"description\" : \"Path relative to the project root containing Lambda zip \" \"files, default is 'lambda_functions/source'\" }, \"s3_bucket\" : { \"description\" : \"Name of S3 bucket to upload project to, if left out \" \"a bucket will be auto-generated\" }, \"parameters\" : { \"description\" : \"Parameter key-values to pass to CloudFormation, \" \"parameters provided in global config take precedence\" }, \"build_submodules\" : { \"description\" : \"Build Lambda zips recursively for submodules, \" \"set to false to disable\" }, \"template\" : { \"description\" : \"path to template file relative to the project \" \"config file path\" }, \"tags\" : { \"description\" : \"Tags to apply to CloudFormation template\" }, \"enable_sig_v2\" : { \"description\" : \"Enable (deprecated) sigv2 access to auto-generated buckets\" }, \"s3_object_acl\" : { \"description\" : \"ACL for uploaded s3 objects, defaults to 'private'\" }, \"shorten_stack_name\" : { \"description\" : \"Shorten stack names generated for tests, set to true to enable\" }, \"role_name\" : { \"description\" : \"Role name to use when launching CFN Stacks.\" }, \"stack_name\" : { \"description\" : \"Cloudformation Stack Name\" }, \"stack_name_prefix\" : { \"description\" : \"Prefix to apply to generated CFN Stack Name\" }, \"stack_name_suffix\" : { \"description\" : \"Suffix to apply to generated CFN Stack Name\" }, \"prehooks\" : { \"description\" : \"hooks to execute prior to executing tests\" }, \"posthooks\" : { \"description\" : \"hooks to execute after executing tests\" }, \"type\" : { \"description\" : \"hook type\" }, \"config\" : { \"description\" : \"hook configuration\" }, } # types ParameterKey = NewType ( \"ParameterKey\" , str ) ParameterValue = Union [ str , int , bool , List [ Union [ int , str ]]] TagKey = NewType ( \"TagKey\" , str ) TagValue = NewType ( \"TagValue\" , str ) S3Acl = NewType ( \"S3Acl\" , str ) Region = NewType ( \"Region\" , str ) AlNumDash = NewType ( \"AlNumDash\" , str ) ProjectName = NewType ( \"ProjectName\" , AlNumDash ) S3BucketName = NewType ( \"S3BucketName\" , AlNumDash ) TestName = NewType ( \"TestName\" , AlNumDash ) AzId = NewType ( \"AzId\" , str ) Templates = NewType ( \"Templates\" , Dict [ TestName , Template ]) # regex validation class ParameterKeyField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"[a-zA-Z0-9]*^$\" , \"Description\" : \"CloudFormation parameter name, can contain letters and \" \"numbers only\" , } JsonSchemaMixin . register_field_encoders ({ ParameterKey : ParameterKeyField ()}) class RegionField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^(ap|eu|us|sa|ca|cn|af|me|us-gov)-(central|south|north|east|\" r \"west|southeast|southwest|northeast|northwest)-[0-9]$\" , \"description\" : \"AWS Region name eg.: 'us-east-1'\" , } JsonSchemaMixin . register_field_encoders ({ Region : RegionField ()}) class S3AclField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^(\" r \"bucket-owner-full-control|\" r \"bucket-owner-read|\" r \"authenticated-read|\" r \"aws-exec-read|\" r \"public-read-write|\" r \"public-read|\" r \"private)$\" , \"description\" : \"Must be a valid S3 ACL (private, public-read, \" \"aws-exec-read, public-read-write, authenticated-read, \" \"bucket-owner-read, bucket-owner-full-control)\" , } JsonSchemaMixin . register_field_encoders ({ S3Acl : S3AclField ()}) class AlNumDashField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^[a-z0-9-]*$\" , \"description\" : \"accepts lower case letters, numbers and -\" , } JsonSchemaMixin . register_field_encoders ({ AlNumDash : AlNumDashField ()}) class AzIdField ( FieldEncoder ): @property def json_schema ( self ): return { \"type\" : \"string\" , \"pattern\" : r \"^((ap|eu|us|sa|ca|cn|af|me)(n|s|e|w|c|ne|se|nw|sw)\" r \"[0-9]-az[0-9]|usw2-lax1-az(1|2))$\" , \"description\" : \"Availability Zone ID, eg.: 'use1-az1'\" , } JsonSchemaMixin . register_field_encoders ({ AzId : AzIdField ()}) # dataclasses @dataclass class RegionObj : name : str account_id : str partition : str profile : str taskcat_id : UUID _boto3_cache : Boto3Cache _role_name : Optional [ str ] def client ( self , service : str ): return self . _boto3_cache . client ( service , region = self . name , profile = self . profile ) @property def session ( self ): return self . _boto3_cache . session ( region = self . name , profile = self . profile ) @property def role_arn ( self ): if self . _role_name : return f \"arn: { self . partition } :iam:: { self . account_id } :role/ { self . _role_name } \" return None @dataclass class S3BucketObj : name : str region : str account_id : str partition : str s3_client : boto3 . client sigv4 : bool auto_generated : bool regional_buckets : bool object_acl : str taskcat_id : UUID @property def sigv4_policy ( self ): policy = { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Test\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : f \"arn: { self . partition } :s3::: { self . name } /*\" , \"Condition\" : { \"StringEquals\" : { \"s3:signatureversion\" : \"AWS\" }}, } ], } return json . dumps ( policy ) def create ( self ): if self . _bucket_matches_existing (): return kwargs = { \"Bucket\" : self . name } if self . region != \"us-east-1\" : kwargs [ \"CreateBucketConfiguration\" ] = { \"LocationConstraint\" : self . region } self . s3_client . create_bucket ( ** kwargs ) error = None try : self . s3_client . get_waiter ( \"bucket_exists\" ) . wait ( Bucket = self . name ) if not self . regional_buckets : self . s3_client . put_bucket_tagging ( Bucket = self . name , Tagging = { \"TagSet\" : [{ \"Key\" : \"taskcat-id\" , \"Value\" : self . taskcat_id . hex }] }, ) if self . sigv4 : self . s3_client . put_bucket_policy ( Bucket = self . name , Policy = self . sigv4_policy ) except Exception as e : # pylint: disable=broad-except error = e try : self . s3_client . delete_bucket ( Bucket = self . name ) except Exception as inner_e : # pylint: disable=broad-except LOG . warning ( f \"failed to remove bucket { self . name } : { inner_e } \" ) if error : raise error def empty ( self ): if not self . auto_generated : LOG . error ( f \"Will not empty bucket created outside of taskcat { self . name } \" ) return objects_to_delete = [] pages = self . s3_client . get_paginator ( \"list_objects_v2\" ) . paginate ( Bucket = self . name ) for page in pages : objects = [] for obj in page . get ( \"Contents\" , []): del_obj = { \"Key\" : obj [ \"Key\" ]} if obj . get ( \"VersionId\" ): del_obj [ \"VersionId\" ] = obj [ \"VersionId\" ] objects . append ( del_obj ) objects_to_delete += objects batched_objects = [ objects_to_delete [ i : i + 1000 ] for i in range ( 0 , len ( objects_to_delete ), 1000 ) ] for objects in batched_objects : if objects : self . s3_client . delete_objects ( Bucket = self . name , Delete = { \"Objects\" : objects } ) def delete ( self , delete_objects = False ): if not self . auto_generated : LOG . info ( f \"Will not delete bucket created outside of taskcat { self . name } \" ) return if delete_objects : try : self . empty () except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \"Cannot delete bucket { self . name } as it does not exist\" ) return try : self . s3_client . delete_bucket ( Bucket = self . name ) except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \"Cannot delete bucket { self . name } as it does not exist\" ) def _bucket_matches_existing ( self ): try : location = self . s3_client . get_bucket_location ( Bucket = self . name )[ \"LocationConstraint\" ] location = location if location else \"us-east-1\" except self . s3_client . exceptions . NoSuchBucket : location = None if location != self . region and location is not None : raise TaskCatException ( f \"bucket { self . name } already exists, but is not in \" f \"the expected region { self . region } , expected { location } \" ) if location : if self . regional_buckets : return True tags = self . s3_client . get_bucket_tagging ( Bucket = self . name )[ \"TagSet\" ] tags = { t [ \"Key\" ]: t [ \"Value\" ] for t in tags } uid = tags . get ( \"taskcat-id\" ) uid = UUID ( uid ) if uid else uid if uid != self . taskcat_id : raise TaskCatException ( f \"bucket { self . name } already exists, but does not have a matching\" f \" uuid\" ) return True return False class Tag : def __init__ ( self , tag_dict : dict ): if isinstance ( tag_dict , Tag ): tag_dict = { \"Key\" : tag_dict . key , \"Value\" : tag_dict . value } self . key : str = tag_dict [ \"Key\" ] self . value : str = tag_dict [ \"Value\" ] def dump ( self ): tag_dict = { \"Key\" : self . key , \"Value\" : self . value } return tag_dict @dataclass class TestRegion ( RegionObj ): s3_bucket : S3BucketObj parameters : Dict [ ParameterKey , ParameterValue ] @classmethod def from_region_obj ( cls , region : RegionObj , s3_bucket , parameters ): return cls ( s3_bucket = s3_bucket , parameters = parameters , ** region . __dict__ ) @dataclass # pylint: disable=too-many-instance-attributes class TestObj : def __init__ ( self , template_path : Path , template : Template , project_root : Path , name : TestName , regions : List [ TestRegion ], tags : List [ Tag ], uid : UUID , _project_name : str , _stack_name : str = \"\" , _stack_name_prefix : str = \"\" , _stack_name_suffix : str = \"\" , _shorten_stack_name : bool = False , ): self . template_path = template_path self . template = template self . project_root = project_root self . name = name self . regions = regions self . tags = tags self . uid = uid self . _project_name = _project_name self . _stack_name = _stack_name self . _stack_name_prefix = _stack_name_prefix self . _stack_name_suffix = _stack_name_suffix self . _shorten_stack_name = _shorten_stack_name self . _assert_param_combo () def _assert_param_combo ( self ): throw = False if self . _stack_name_prefix and self . _stack_name_suffix : throw = True if self . _stack_name and ( self . _stack_name_prefix or self . _stack_name_suffix ): throw = True if throw : raise TaskCatException ( \"Please provide only *ONE* of stack_name, stack_name_prefix, \\ or stack_name_suffix \" ) @property def stack_name ( self ): prefix = \"tCaT-\" if self . _stack_name : return self . _stack_name # TODO: prefix *OR* suffix if self . _stack_name_prefix : if self . _shorten_stack_name : return \" {}{} - {} \" . format ( self . _stack_name_prefix , self . name , self . uid . hex [: 6 ] ) return \" {}{} - {} - {} \" . format ( self . _stack_name_prefix , self . _project_name , self . name , self . uid . hex ) if self . _stack_name_suffix : return \" {}{} - {} - {} \" . format ( prefix , self . _project_name , self . name , self . _stack_name_suffix ) if self . _shorten_stack_name : return \" {}{} - {} \" . format ( prefix , self . name , self . uid . hex [: 6 ]) return \" {}{} - {} - {} \" . format ( prefix , self . _project_name , self . name , self . uid . hex ) @dataclass class HookData ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Hook definition\"\"\" type : Optional [ str ] = field ( default = None , metadata = METADATA [ \"type\" ]) config : Optional [ Dict [ str , Any ]] = field ( default = None , metadata = METADATA [ \"config\" ]) @dataclass class GeneralConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"General configuration settings.\"\"\" parameters : Optional [ Dict [ ParameterKey , ParameterValue ]] = field ( default = None , metadata = METADATA [ \"parameters\" ] ) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) s3_bucket : Optional [ str ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ]) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) @dataclass class TestConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Test specific configuration section.\"\"\" template : Optional [ str ] = field ( default = None , metadata = METADATA [ \"template\" ]) parameters : Dict [ ParameterKey , ParameterValue ] = field ( default_factory = dict , metadata = METADATA [ \"parameters\" ] ) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) s3_bucket : Optional [ S3BucketName ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ] ) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) az_blacklist : Optional [ List [ AzId ]] = field ( default = None , metadata = METADATA [ \"az_ids\" ] ) role_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"role_name\" ]) stack_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name\" ]) stack_name_prefix : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name_prefix\" ] ) stack_name_suffix : Optional [ str ] = field ( default = None , metadata = METADATA [ \"stack_name_suffix\" ] ) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) # pylint: disable=too-many-instance-attributes @dataclass class ProjectConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Project specific configuration section\"\"\" name : Optional [ ProjectName ] = field ( default = None , metadata = METADATA [ \"project__name\" ] ) auth : Optional [ Dict [ Region , str ]] = field ( default = None , metadata = METADATA [ \"auth\" ]) owner : Optional [ str ] = field ( default = None , metadata = METADATA [ \"project__owner\" ]) regions : Optional [ List [ Region ]] = field ( default = None , metadata = METADATA [ \"regions\" ]) az_blacklist : Optional [ List [ AzId ]] = field ( default = None , metadata = METADATA [ \"az_ids\" ] ) package_lambda : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"package_lambda\" ] ) lambda_zip_path : Optional [ str ] = field ( default = None , metadata = METADATA [ \"lambda_zip_path\" ] ) lambda_source_path : Optional [ str ] = field ( default = None , metadata = METADATA [ \"lambda_source_path\" ] ) s3_bucket : Optional [ S3BucketName ] = field ( default = None , metadata = METADATA [ \"s3_bucket\" ] ) s3_regional_buckets : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"s3_regional_buckets\" ] ) parameters : Optional [ Dict [ ParameterKey , ParameterValue ]] = field ( default = None , metadata = METADATA [ \"parameters\" ] ) build_submodules : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"build_submodules\" ] ) template : Optional [ str ] = field ( default = None , metadata = METADATA [ \"template\" ]) tags : Optional [ Dict [ TagKey , TagValue ]] = field ( default = None , metadata = METADATA [ \"tags\" ] ) s3_enable_sig_v2 : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"enable_sig_v2\" ] ) s3_object_acl : Optional [ S3Acl ] = field ( default = None , metadata = METADATA [ \"s3_object_acl\" ] ) shorten_stack_name : Optional [ bool ] = field ( default = None , metadata = METADATA [ \"shorten_stack_name\" ] ) role_name : Optional [ str ] = field ( default = None , metadata = METADATA [ \"role_name\" ]) prehooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"prehooks\" ] ) posthooks : Optional [ List [ HookData ]] = field ( default = None , metadata = METADATA [ \"posthooks\" ] ) PROPAGATE_KEYS = [ \"tags\" , \"parameters\" , \"auth\" ] PROPOGATE_ITEMS = [ \"regions\" , \"s3_bucket\" , \"template\" , \"az_blacklist\" , \"s3_regional_buckets\" , \"prehooks\" , \"posthooks\" , ] def generate_regional_bucket_name ( region_obj : RegionObj , prefix : str = \"tcat\" ): if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable=len-as-condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) hashed_account_id = uuid5 ( name = str ( region_obj . account_id ), namespace = UUID ( int = 0 ) ) . hex return f \" { prefix } - { hashed_account_id } - { region_obj . name } \" def generate_bucket_name ( project : str , prefix : str = \"tcat\" ): if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable=len-as-condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) alnum = string . ascii_lowercase + string . digits suffix = \"\" . join ( random . choice ( alnum ) for i in range ( 8 )) # nosec: B311 mid = f \"- { project } -\" avail_len = 63 - len ( mid ) mid = mid [: avail_len ] return f \" { prefix }{ mid }{ suffix } \" # pylint raises false positive due to json-dataclass # pylint: disable=no-member @dataclass class BaseConfig ( JsonSchemaMixin , allow_additional_props = False ): # type: ignore \"\"\"Taskcat configuration file\"\"\" general : GeneralConfig = field ( default_factory = GeneralConfig ) project : ProjectConfig = field ( default_factory = ProjectConfig ) tests : Dict [ TestName , TestConfig ] = field ( default_factory = dict ) # pylint doesn't like instance variables being added in post_init # pylint: disable=attribute-defined-outside-init def __post_init__ ( self ): self . _source : Dict [ str , Any ] = {} self . _propogate () self . set_source ( \"UNKNOWN\" ) self . _propogate_source () @staticmethod def _merge ( source , dest ): for section_key , section_value in source . items (): if section_key in PROPAGATE_KEYS + PROPOGATE_ITEMS : if section_key not in dest : dest [ section_key ] = section_value continue if section_key in PROPAGATE_KEYS : for key , value in section_value . items (): dest [ section_key ][ key ] = value return dest def _propogate ( self ): project_dict = self . _merge ( self . general . to_dict (), self . project . to_dict ()) self . project = ProjectConfig . from_dict ( project_dict ) for test_key , test in self . tests . items (): test_dict = self . _merge ( self . project . to_dict (), test . to_dict ()) self . tests [ test_key ] = TestConfig . from_dict ( test_dict ) def _propogate_source ( self ): self . _source [ \"project\" ] = self . _merge ( self . _source [ \"general\" ], self . _source [ \"project\" ] ) for test_key in self . _source [ \"tests\" ]: test = self . _merge ( self . _source [ \"project\" ], self . _source [ \"tests\" ][ test_key ]) self . _source [ \"tests\" ][ test_key ] = test def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Optional [ Union [ str , dict ]]: base_case = False if dest is None : base_case = True self . _source = self . to_dict () dest = self . _source if not isinstance ( dest , dict ): return source_name if isinstance ( dest , dict ): for item in dest : dest [ item ] = self . set_source ( source_name , dest [ item ]) if not base_case : return dest return None @classmethod def merge ( cls , base_config : \"BaseConfig\" , merge_config : \"BaseConfig\" ) -> \"BaseConfig\" : merged = base_config . to_dict () merge_nested_dict ( merged , merge_config . to_dict ()) merged_source = base_config . _source . copy () merge_nested_dict ( merged_source , merge_config . _source ) config = cls . from_dict ( merged ) config . _source = merged_source config . _propogate_source () # pylint: disable=protected-access return config","title":"Module _dataclasses"},{"location":"reference/_dataclasses.html#variables","text":"LOG METADATA PROPAGATE_KEYS PROPOGATE_ITEMS ParameterValue","title":"Variables"},{"location":"reference/_dataclasses.html#functions","text":"","title":"Functions"},{"location":"reference/_dataclasses.html#alnumdash","text":"def AlNumDash ( x ) View Source def new_type ( x ) : return x","title":"AlNumDash"},{"location":"reference/_dataclasses.html#azid","text":"def AzId ( x ) View Source def new_type ( x ) : return x","title":"AzId"},{"location":"reference/_dataclasses.html#parameterkey","text":"def ParameterKey ( x ) View Source def new_type ( x ) : return x","title":"ParameterKey"},{"location":"reference/_dataclasses.html#projectname","text":"def ProjectName ( x ) View Source def new_type ( x ) : return x","title":"ProjectName"},{"location":"reference/_dataclasses.html#region","text":"def Region ( x ) View Source def new_type ( x ) : return x","title":"Region"},{"location":"reference/_dataclasses.html#s3acl","text":"def S3Acl ( x ) View Source def new_type ( x ) : return x","title":"S3Acl"},{"location":"reference/_dataclasses.html#s3bucketname","text":"def S3BucketName ( x ) View Source def new_type ( x ) : return x","title":"S3BucketName"},{"location":"reference/_dataclasses.html#tagkey","text":"def TagKey ( x ) View Source def new_type ( x ) : return x","title":"TagKey"},{"location":"reference/_dataclasses.html#tagvalue","text":"def TagValue ( x ) View Source def new_type ( x ) : return x","title":"TagValue"},{"location":"reference/_dataclasses.html#templates","text":"def Templates ( x ) View Source def new_type ( x ) : return x","title":"Templates"},{"location":"reference/_dataclasses.html#testname","text":"def TestName ( x ) View Source def new_type ( x ) : return x","title":"TestName"},{"location":"reference/_dataclasses.html#generate_bucket_name","text":"def generate_bucket_name ( project : str , prefix : str = 'tcat' ) View Source def generate_bucket_name ( project : str , prefix : str = \" tcat \" ) : if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint : disable = len - as - condition raise TaskCatException ( \" prefix must be between 1 and 8 characters long \" ) alnum = string . ascii_lowercase + string . digits suffix = \"\" . join ( random . choice ( alnum ) for i in range ( 8 )) # nosec : B311 mid = f \" -{project}- \" avail_len = 63 - len ( mid ) mid = mid [: avail_len ] return f \" {prefix}{mid}{suffix} \"","title":"generate_bucket_name"},{"location":"reference/_dataclasses.html#generate_regional_bucket_name","text":"def generate_regional_bucket_name ( region_obj : _dataclasses . RegionObj , prefix : str = 'tcat' ) View Source def generate_regional_bucket_name ( region_obj: RegionObj , prefix: str = \"tcat\" ) : if len ( prefix ) > 8 or len ( prefix ) < 1 : # pylint: disable = len - as - condition raise TaskCatException ( \"prefix must be between 1 and 8 characters long\" ) hashed_account_id = uuid5 ( name = str ( region_obj . account_id ), namespace = UUID ( int = 0 ) ). hex return f \"{prefix}-{hashed_account_id}-{region_obj.name}\"","title":"generate_regional_bucket_name"},{"location":"reference/_dataclasses.html#classes","text":"","title":"Classes"},{"location":"reference/_dataclasses.html#alnumdashfield","text":"class AlNumDashField ( / , * args , ** kwargs )","title":"AlNumDashField"},{"location":"reference/_dataclasses.html#ancestors-in-mro","text":"dataclasses_jsonschema.field_types.FieldEncoder typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#instance-variables","text":"json_schema","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_python","text":"def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value )","title":"to_python"},{"location":"reference/_dataclasses.html#to_wire","text":"def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value )","title":"to_wire"},{"location":"reference/_dataclasses.html#azidfield","text":"class AzIdField ( / , * args , ** kwargs )","title":"AzIdField"},{"location":"reference/_dataclasses.html#ancestors-in-mro_1","text":"dataclasses_jsonschema.field_types.FieldEncoder typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#instance-variables_1","text":"json_schema","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_1","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_python_1","text":"def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value )","title":"to_python"},{"location":"reference/_dataclasses.html#to_wire_1","text":"def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value )","title":"to_wire"},{"location":"reference/_dataclasses.html#baseconfig","text":"class BaseConfig ( general : _dataclasses . GeneralConfig = < factory > , project : _dataclasses . ProjectConfig = < factory > , tests : Dict [ TestName , _dataclasses . TestConfig ] = < factory > )","title":"BaseConfig"},{"location":"reference/_dataclasses.html#ancestors-in-mro_2","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#all_json_schemas","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_dataclasses.html#field_mapping","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_dataclasses.html#from_dict","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_dataclasses.html#from_json","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_dataclasses.html#from_object","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_dataclasses.html#json_schema","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_dataclasses.html#merge","text":"def merge ( base_config : 'BaseConfig' , merge_config : 'BaseConfig' ) -> 'BaseConfig' View Source @classmethod def merge ( cls , base_config : \"BaseConfig\" , merge_config : \"BaseConfig\" ) -> \"BaseConfig\" : merged = base_config . to_dict () merge_nested_dict ( merged , merge_config . to_dict ()) merged_source = base_config . _source . copy () merge_nested_dict ( merged_source , merge_config . _source ) config = cls . from_dict ( merged ) config . _source = merged_source config . _propogate_source () # pylint : disable = protected - access return config","title":"merge"},{"location":"reference/_dataclasses.html#register_field_encoders","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_dataclasses.html#methods_2","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#set_source","text":"def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Union [ str , dict , NoneType ] View Source def set_source ( self , source_name : str , dest : Optional [ Any ] = None ) -> Optional [ Union[str, dict ] ]: base_case = False if dest is None : base_case = True self . _source = self . to_dict () dest = self . _source if not isinstance ( dest , dict ) : return source_name if isinstance ( dest , dict ) : for item in dest : dest [ item ] = self . set_source ( source_name , dest [ item ] ) if not base_case : return dest return None","title":"set_source"},{"location":"reference/_dataclasses.html#to_dict","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_dataclasses.html#to_json","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_dataclasses.html#generalconfig","text":"class GeneralConfig ( parameters : Optional [ Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]]] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , auth : Optional [ Dict [ Region , str ]] = None , s3_bucket : Optional [ str ] = None , s3_regional_buckets : Optional [ bool ] = None , regions : Optional [ List [ Region ]] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None )","title":"GeneralConfig"},{"location":"reference/_dataclasses.html#ancestors-in-mro_3","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#class-variables","text":"auth parameters posthooks prehooks regions s3_bucket s3_regional_buckets tags","title":"Class variables"},{"location":"reference/_dataclasses.html#static-methods_1","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#all_json_schemas_1","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_dataclasses.html#field_mapping_1","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_dataclasses.html#from_dict_1","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_dataclasses.html#from_json_1","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_dataclasses.html#from_object_1","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_dataclasses.html#json_schema_1","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_dataclasses.html#register_field_encoders_1","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_dataclasses.html#methods_3","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_dict_1","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_dataclasses.html#to_json_1","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_dataclasses.html#hookdata","text":"class HookData ( type : Optional [ str ] = None , config : Optional [ Dict [ str , Any ]] = None )","title":"HookData"},{"location":"reference/_dataclasses.html#ancestors-in-mro_4","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#class-variables_1","text":"config type","title":"Class variables"},{"location":"reference/_dataclasses.html#static-methods_2","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#all_json_schemas_2","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_dataclasses.html#field_mapping_2","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_dataclasses.html#from_dict_2","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_dataclasses.html#from_json_2","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_dataclasses.html#from_object_2","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_dataclasses.html#json_schema_2","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_dataclasses.html#register_field_encoders_2","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_dataclasses.html#methods_4","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_dict_2","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_dataclasses.html#to_json_2","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_dataclasses.html#parameterkeyfield","text":"class ParameterKeyField ( / , * args , ** kwargs )","title":"ParameterKeyField"},{"location":"reference/_dataclasses.html#ancestors-in-mro_5","text":"dataclasses_jsonschema.field_types.FieldEncoder typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#instance-variables_2","text":"json_schema","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_5","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_python_2","text":"def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value )","title":"to_python"},{"location":"reference/_dataclasses.html#to_wire_2","text":"def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value )","title":"to_wire"},{"location":"reference/_dataclasses.html#projectconfig","text":"class ProjectConfig ( name : Optional [ ProjectName ] = None , auth : Optional [ Dict [ Region , str ]] = None , owner : Optional [ str ] = None , regions : Optional [ List [ Region ]] = None , az_blacklist : Optional [ List [ AzId ]] = None , package_lambda : Optional [ bool ] = None , lambda_zip_path : Optional [ str ] = None , lambda_source_path : Optional [ str ] = None , s3_bucket : Optional [ S3BucketName ] = None , s3_regional_buckets : Optional [ bool ] = None , parameters : Optional [ Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]]] = None , build_submodules : Optional [ bool ] = None , template : Optional [ str ] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , s3_enable_sig_v2 : Optional [ bool ] = None , s3_object_acl : Optional [ S3Acl ] = None , shorten_stack_name : Optional [ bool ] = None , role_name : Optional [ str ] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None )","title":"ProjectConfig"},{"location":"reference/_dataclasses.html#ancestors-in-mro_6","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#class-variables_2","text":"auth az_blacklist build_submodules lambda_source_path lambda_zip_path name owner package_lambda parameters posthooks prehooks regions role_name s3_bucket s3_enable_sig_v2 s3_object_acl s3_regional_buckets shorten_stack_name tags template","title":"Class variables"},{"location":"reference/_dataclasses.html#static-methods_3","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#all_json_schemas_3","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_dataclasses.html#field_mapping_3","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_dataclasses.html#from_dict_3","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_dataclasses.html#from_json_3","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_dataclasses.html#from_object_3","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_dataclasses.html#json_schema_3","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_dataclasses.html#register_field_encoders_3","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_dataclasses.html#methods_6","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_dict_3","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_dataclasses.html#to_json_3","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_dataclasses.html#regionfield","text":"class RegionField ( / , * args , ** kwargs )","title":"RegionField"},{"location":"reference/_dataclasses.html#ancestors-in-mro_7","text":"dataclasses_jsonschema.field_types.FieldEncoder typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#instance-variables_3","text":"json_schema","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_7","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_python_3","text":"def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value )","title":"to_python"},{"location":"reference/_dataclasses.html#to_wire_3","text":"def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value )","title":"to_wire"},{"location":"reference/_dataclasses.html#regionobj","text":"class RegionObj ( name : str , account_id : str , partition : str , profile : str , taskcat_id : uuid . UUID , _boto3_cache : taskcat . _client_factory . Boto3Cache , _role_name : Optional [ str ] )","title":"RegionObj"},{"location":"reference/_dataclasses.html#descendants","text":"_dataclasses.TestRegion","title":"Descendants"},{"location":"reference/_dataclasses.html#instance-variables_4","text":"role_arn session","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_8","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#client","text":"def client ( self , service : str ) View Source def client ( self , service: str ) : return self . _boto3_cache . client ( service , region = self . name , profile = self . profile )","title":"client"},{"location":"reference/_dataclasses.html#s3aclfield","text":"class S3AclField ( / , * args , ** kwargs )","title":"S3AclField"},{"location":"reference/_dataclasses.html#ancestors-in-mro_8","text":"dataclasses_jsonschema.field_types.FieldEncoder typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#instance-variables_5","text":"json_schema","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_9","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_python_4","text":"def to_python ( self , value : ~ OutType ) -> ~ T View Source def to_python ( self , value : OutType ) -> T : return cast ( T , value )","title":"to_python"},{"location":"reference/_dataclasses.html#to_wire_4","text":"def to_wire ( self , value : ~ T ) -> ~ OutType View Source def to_wire ( self , value: T ) -> OutType: return cast ( OutType , value )","title":"to_wire"},{"location":"reference/_dataclasses.html#s3bucketobj","text":"class S3BucketObj ( name : str , region : str , account_id : str , partition : str , s3_client : < function client at 0x103f3b4c0 > , sigv4 : bool , auto_generated : bool , regional_buckets : bool , object_acl : str , taskcat_id : uuid . UUID )","title":"S3BucketObj"},{"location":"reference/_dataclasses.html#instance-variables_6","text":"sigv4_policy","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_10","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#create","text":"def create ( self ) View Source def create ( self ) : if self . _bucket_matches_existing () : return kwargs = { \"Bucket\" : self . name } if self . region != \"us-east-1\" : kwargs [ \"CreateBucketConfiguration\" ] = { \"LocationConstraint\" : self . region } self . s3_client . create_bucket ( ** kwargs ) error = None try: self . s3_client . get_waiter ( \"bucket_exists\" ). wait ( Bucket = self . name ) if not self . regional_buckets: self . s3_client . put_bucket_tagging ( Bucket = self . name , Tagging = { \"TagSet\" : [{ \"Key\" : \"taskcat-id\" , \"Value\" : self . taskcat_id . hex }] }, ) if self . sigv4: self . s3_client . put_bucket_policy ( Bucket = self . name , Policy = self . sigv4_policy ) except Exception as e: # pylint: disable = broad - except error = e try: self . s3_client . delete_bucket ( Bucket = self . name ) except Exception as inner_e: # pylint: disable = broad - except LOG . warning ( f \"failed to remove bucket {self.name}: {inner_e}\" ) if error: raise error","title":"create"},{"location":"reference/_dataclasses.html#delete","text":"def delete ( self , delete_objects = False ) View Source def delete ( self , delete_objects = False ) : if not self . auto_generated : LOG . info ( f \" Will not delete bucket created outside of taskcat {self.name} \" ) return if delete_objects : try : self . empty () except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \" Cannot delete bucket {self.name} as it does not exist \" ) return try : self . s3_client . delete_bucket ( Bucket = self . name ) except self . s3_client . exceptions . NoSuchBucket : LOG . info ( f \" Cannot delete bucket {self.name} as it does not exist \" )","title":"delete"},{"location":"reference/_dataclasses.html#empty","text":"def empty ( self ) View Source def empty ( self ) : if not self . auto_generated : LOG . error ( f \"Will not empty bucket created outside of taskcat { self . name } \") return objects_to_delete = [] pages = self . s3_client . get_paginator ( \"list_objects_v2\" ). paginate ( Bucket = self . name ) for page in pages : objects = [] for obj in page . get ( \"Contents\" , []) : del_obj = { \"Key\" : obj [ \"Key\" ]} if obj . get ( \"VersionId\" ) : del_obj [ \"VersionId\" ] = obj [ \"VersionId\" ] objects . append ( del_obj ) objects_to_delete += objects batched_objects = [ objects_to_delete [ i : i + 1000 ] for i in range ( 0 , len ( objects_to_delete ), 1000 ) ] for objects in batched_objects : if objects : self . s3_client . delete_objects ( Bucket = self . name , Delete = { \"Objects\" : objects } )","title":"empty"},{"location":"reference/_dataclasses.html#tag","text":"class Tag ( tag_dict : dict )","title":"Tag"},{"location":"reference/_dataclasses.html#methods_11","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#dump","text":"def dump ( self ) View Source def dump ( self ) : tag_dict = { \" Key \" : self . key , \" Value \" : self . value } return tag_dict","title":"dump"},{"location":"reference/_dataclasses.html#testconfig","text":"class TestConfig ( template : Optional [ str ] = None , parameters : Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]] = < factory > , regions : Optional [ List [ Region ]] = None , tags : Optional [ Dict [ TagKey , TagValue ]] = None , auth : Optional [ Dict [ Region , str ]] = None , s3_bucket : Optional [ S3BucketName ] = None , s3_regional_buckets : Optional [ bool ] = None , az_blacklist : Optional [ List [ AzId ]] = None , role_name : Optional [ str ] = None , stack_name : Optional [ str ] = None , stack_name_prefix : Optional [ str ] = None , stack_name_suffix : Optional [ str ] = None , prehooks : Optional [ List [ _dataclasses . HookData ]] = None , posthooks : Optional [ List [ _dataclasses . HookData ]] = None )","title":"TestConfig"},{"location":"reference/_dataclasses.html#ancestors-in-mro_9","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#class-variables_3","text":"auth az_blacklist posthooks prehooks regions role_name s3_bucket s3_regional_buckets stack_name stack_name_prefix stack_name_suffix tags template","title":"Class variables"},{"location":"reference/_dataclasses.html#static-methods_4","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#all_json_schemas_4","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_dataclasses.html#field_mapping_4","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_dataclasses.html#from_dict_4","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_dataclasses.html#from_json_4","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_dataclasses.html#from_object_4","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_dataclasses.html#json_schema_4","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_dataclasses.html#register_field_encoders_4","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_dataclasses.html#methods_12","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#to_dict_4","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_dataclasses.html#to_json_4","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_dataclasses.html#testobj","text":"class TestObj ( template_path : pathlib . Path , template : taskcat . _cfn . template . Template , project_root : pathlib . Path , name : < function NewType .< locals >. new_type at 0x1130354c0 > , regions : List [ _dataclasses . TestRegion ], tags : List [ _dataclasses . Tag ], uid : uuid . UUID , _project_name : str , _stack_name : str = '' , _stack_name_prefix : str = '' , _stack_name_suffix : str = '' , _shorten_stack_name : bool = False )","title":"TestObj"},{"location":"reference/_dataclasses.html#instance-variables_7","text":"stack_name","title":"Instance variables"},{"location":"reference/_dataclasses.html#testregion","text":"class TestRegion ( name : str , account_id : str , partition : str , profile : str , taskcat_id : uuid . UUID , _boto3_cache : taskcat . _client_factory . Boto3Cache , _role_name : Optional [ str ], s3_bucket : _dataclasses . S3BucketObj , parameters : Dict [ ParameterKey , Union [ str , int , bool , List [ Union [ int , str ]]]] )","title":"TestRegion"},{"location":"reference/_dataclasses.html#ancestors-in-mro_10","text":"_dataclasses.RegionObj","title":"Ancestors (in MRO)"},{"location":"reference/_dataclasses.html#static-methods_5","text":"","title":"Static methods"},{"location":"reference/_dataclasses.html#from_region_obj","text":"def from_region_obj ( region : _dataclasses . RegionObj , s3_bucket , parameters ) View Source @classmethod def from_region_obj ( cls , region : RegionObj , s3_bucket , parameters ) : return cls ( s3_bucket = s3_bucket , parameters = parameters , ** region . __dict__ )","title":"from_region_obj"},{"location":"reference/_dataclasses.html#instance-variables_8","text":"role_arn session","title":"Instance variables"},{"location":"reference/_dataclasses.html#methods_13","text":"","title":"Methods"},{"location":"reference/_dataclasses.html#client_1","text":"def client ( self , service : str ) View Source def client ( self , service: str ) : return self . _boto3_cache . client ( service , region = self . name , profile = self . profile )","title":"client"},{"location":"reference/_legacy_config.html","text":"Module _legacy_config None None View Source import logging from dataclasses import dataclass , field from pathlib import Path from typing import Dict , List import yaml from dataclasses_jsonschema import JsonSchemaMixin from taskcat._dataclasses import BaseConfig from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) @dataclass class LegacyGlobalConfig ( JsonSchemaMixin ): qsname : str govcloud : bool = field ( default = False ) marketplace_ami : bool = field ( default = False ) owner : str = field ( default = \"\" ) regions : List [ str ] = field ( default_factory = list ) reporting : bool = field ( default = True ) lambda_build : bool = field ( default = False ) s3bucket : str = field ( default = \"\" ) @dataclass class LegacyTestConfig ( JsonSchemaMixin ): template_file : str parameter_input : str regions : List [ str ] = field ( default_factory = list ) @dataclass class LegacyConfig ( JsonSchemaMixin ): global_ : LegacyGlobalConfig tests : Dict [ str , LegacyTestConfig ] def parse_legacy_config ( project_root : Path ): config_file = ( project_root / \"ci/taskcat.yml\" ) . expanduser () . resolve () if not config_file . is_file (): raise TaskCatException ( f \"No config_file at { config_file } \" ) with open ( str ( config_file ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) # need to rename global key, as it's a python keyword config_dict [ \"global_\" ] = config_dict . pop ( \"global\" ) legacy_config = LegacyConfig . from_dict ( config_dict ) tests = {} for test_name , test_data in legacy_config . tests . items (): parameters = {} parameter_file = project_root / \"ci/\" / test_data . parameter_input parameter_file = parameter_file . expanduser () . resolve () with open ( str ( parameter_file ), \"r\" ) as file_handle : for param in yaml . safe_load ( file_handle ): parameters [ param [ \"ParameterKey\" ]] = param [ \"ParameterValue\" ] tests [ test_name ] = { \"template\" : \"templates/\" + test_data . template_file , \"parameters\" : parameters , \"regions\" : test_data . regions , } if not tests [ test_name ][ \"regions\" ]: del tests [ test_name ][ \"regions\" ] new_config_dict = { \"project\" : { \"name\" : legacy_config . global_ . qsname , \"owner\" : legacy_config . global_ . owner , \"s3_bucket\" : legacy_config . global_ . s3bucket , \"package_lambda\" : legacy_config . global_ . lambda_build , \"regions\" : legacy_config . global_ . regions , }, \"tests\" : tests , } new_config = BaseConfig . from_dict ( new_config_dict ) LOG . warning ( \"config is in a legacy format, support for which will be dropped in a \" \"future version. a new format config (.taskcat.yml) will been placed \" \"in your project_root\" ) new_config_path = project_root / \".taskcat.yml\" if new_config_path . exists (): LOG . warning ( f \"skipping new config file creation, file already exits at \" f \" { new_config_path } \" ) else : with open ( str ( new_config_path ), \"w\" ) as file_handle : config_dict = new_config . to_dict () config_dict . pop ( \"general\" ) yaml . dump ( config_dict , file_handle , default_flow_style = False ) return new_config def legacy_overrides ( legacy_override , overrides_path , override_type ): if legacy_override . is_file (): with open ( str ( legacy_override ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) LOG . warning ( f \"overrides file { str ( legacy_override ) } is in legacy \" f \"format, support for this format will be deprecated \" f \"in a future version.\" ) override_params = { i [ \"ParameterKey\" ]: i [ \"ParameterValue\" ] for i in override_params } if override_type == \"global\" : override_params = { \"general\" : { \"parameters\" : override_params }} if not overrides_path . exists (): LOG . warning ( f \"Converting overrides to new format and saving in \" f \" { overrides_path } \" ) with open ( str ( overrides_path ), \"w\" ) as file_handle : file_handle . write ( yaml . dump ( override_params , default_flow_style = False )) else : LOG . warning ( f \"Ignoring legacy overrides as a current format \" f \"file has been found in { str ( overrides_path ) } \" ) Variables LOG Functions legacy_overrides def legacy_overrides ( legacy_override , overrides_path , override_type ) View Source def legacy_overrides ( legacy_override , overrides_path , override_type ): if legacy_override . is_file (): with open ( str ( legacy_override ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) LOG . warning ( f \"overrides file {str(legacy_override)} is in legacy \" f \"format, support for this format will be deprecated \" f \"in a future version.\" ) override_params = { i [ \"ParameterKey\" ]: i [ \"ParameterValue\" ] for i in override_params } if override_type == \"global\" : override_params = { \"general\" : { \"parameters\" : override_params }} if not overrides_path . exists (): LOG . warning ( f \"Converting overrides to new format and saving in \" f \"{overrides_path}\" ) with open ( str ( overrides_path ), \"w\" ) as file_handle : file_handle . write ( yaml . dump ( override_params , default_flow_style = False )) else : LOG . warning ( f \"Ignoring legacy overrides as a current format \" f \"file has been found in {str(overrides_path)}\" ) parse_legacy_config def parse_legacy_config ( project_root : pathlib . Path ) View Source def parse_legacy_config ( project_root : Path ) : config_file = ( project_root / \"ci/taskcat.yml\" ). expanduser (). resolve () if not config_file . is_file () : raise TaskCatException ( f \"No config_file at {config_file}\" ) with open ( str ( config_file ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) # need to rename global key , as it ' s a python keyword config_dict [ \"global_\" ] = config_dict . pop ( \"global\" ) legacy_config = LegacyConfig . from_dict ( config_dict ) tests = {} for test_name , test_data in legacy_config . tests . items () : parameters = {} parameter_file = project_root / \"ci/\" / test_data . parameter_input parameter_file = parameter_file . expanduser (). resolve () with open ( str ( parameter_file ), \"r\" ) as file_handle : for param in yaml . safe_load ( file_handle ) : parameters [ param[\"ParameterKey\" ] ] = param [ \"ParameterValue\" ] tests [ test_name ] = { \"template\" : \"templates/\" + test_data . template_file , \"parameters\" : parameters , \"regions\" : test_data . regions , } if not tests [ test_name ][ \"regions\" ] : del tests [ test_name ][ \"regions\" ] new_config_dict = { \"project\" : { \"name\" : legacy_config . global_ . qsname , \"owner\" : legacy_config . global_ . owner , \"s3_bucket\" : legacy_config . global_ . s3bucket , \"package_lambda\" : legacy_config . global_ . lambda_build , \"regions\" : legacy_config . global_ . regions , } , \"tests\" : tests , } new_config = BaseConfig . from_dict ( new_config_dict ) LOG . warning ( \"config is in a legacy format, support for which will be dropped in a \" \"future version. a new format config (.taskcat.yml) will been placed \" \"in your project_root\" ) new_config_path = project_root / \".taskcat.yml\" if new_config_path . exists () : LOG . warning ( f \"skipping new config file creation, file already exits at \" f \"{new_config_path}\" ) else : with open ( str ( new_config_path ), \"w\" ) as file_handle : config_dict = new_config . to_dict () config_dict . pop ( \"general\" ) yaml . dump ( config_dict , file_handle , default_flow_style = False ) return new_config Classes LegacyConfig class LegacyConfig ( global_ : _legacy_config . LegacyGlobalConfig , tests : Dict [ str , _legacy_config . LegacyTestConfig ] ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) LegacyGlobalConfig class LegacyGlobalConfig ( qsname : str , govcloud : bool = False , marketplace_ami : bool = False , owner : str = '' , regions : List [ str ] = < factory > , reporting : bool = True , lambda_build : bool = False , s3bucket : str = '' ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Class variables govcloud lambda_build marketplace_ami owner reporting s3bucket Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs ) LegacyTestConfig class LegacyTestConfig ( template_file : str , parameter_input : str , regions : List [ str ] = < factory > ) Ancestors (in MRO) dataclasses_jsonschema.JsonSchemaMixin Static methods all_json_schemas def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions field_mapping def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {} from_dict def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance from_json def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate ) from_object def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance json_schema def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema register_field_encoders def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders ) Methods to_dict def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data to_json def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":" Legacy Config"},{"location":"reference/_legacy_config.html#module-_legacy_config","text":"None None View Source import logging from dataclasses import dataclass , field from pathlib import Path from typing import Dict , List import yaml from dataclasses_jsonschema import JsonSchemaMixin from taskcat._dataclasses import BaseConfig from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) @dataclass class LegacyGlobalConfig ( JsonSchemaMixin ): qsname : str govcloud : bool = field ( default = False ) marketplace_ami : bool = field ( default = False ) owner : str = field ( default = \"\" ) regions : List [ str ] = field ( default_factory = list ) reporting : bool = field ( default = True ) lambda_build : bool = field ( default = False ) s3bucket : str = field ( default = \"\" ) @dataclass class LegacyTestConfig ( JsonSchemaMixin ): template_file : str parameter_input : str regions : List [ str ] = field ( default_factory = list ) @dataclass class LegacyConfig ( JsonSchemaMixin ): global_ : LegacyGlobalConfig tests : Dict [ str , LegacyTestConfig ] def parse_legacy_config ( project_root : Path ): config_file = ( project_root / \"ci/taskcat.yml\" ) . expanduser () . resolve () if not config_file . is_file (): raise TaskCatException ( f \"No config_file at { config_file } \" ) with open ( str ( config_file ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) # need to rename global key, as it's a python keyword config_dict [ \"global_\" ] = config_dict . pop ( \"global\" ) legacy_config = LegacyConfig . from_dict ( config_dict ) tests = {} for test_name , test_data in legacy_config . tests . items (): parameters = {} parameter_file = project_root / \"ci/\" / test_data . parameter_input parameter_file = parameter_file . expanduser () . resolve () with open ( str ( parameter_file ), \"r\" ) as file_handle : for param in yaml . safe_load ( file_handle ): parameters [ param [ \"ParameterKey\" ]] = param [ \"ParameterValue\" ] tests [ test_name ] = { \"template\" : \"templates/\" + test_data . template_file , \"parameters\" : parameters , \"regions\" : test_data . regions , } if not tests [ test_name ][ \"regions\" ]: del tests [ test_name ][ \"regions\" ] new_config_dict = { \"project\" : { \"name\" : legacy_config . global_ . qsname , \"owner\" : legacy_config . global_ . owner , \"s3_bucket\" : legacy_config . global_ . s3bucket , \"package_lambda\" : legacy_config . global_ . lambda_build , \"regions\" : legacy_config . global_ . regions , }, \"tests\" : tests , } new_config = BaseConfig . from_dict ( new_config_dict ) LOG . warning ( \"config is in a legacy format, support for which will be dropped in a \" \"future version. a new format config (.taskcat.yml) will been placed \" \"in your project_root\" ) new_config_path = project_root / \".taskcat.yml\" if new_config_path . exists (): LOG . warning ( f \"skipping new config file creation, file already exits at \" f \" { new_config_path } \" ) else : with open ( str ( new_config_path ), \"w\" ) as file_handle : config_dict = new_config . to_dict () config_dict . pop ( \"general\" ) yaml . dump ( config_dict , file_handle , default_flow_style = False ) return new_config def legacy_overrides ( legacy_override , overrides_path , override_type ): if legacy_override . is_file (): with open ( str ( legacy_override ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) LOG . warning ( f \"overrides file { str ( legacy_override ) } is in legacy \" f \"format, support for this format will be deprecated \" f \"in a future version.\" ) override_params = { i [ \"ParameterKey\" ]: i [ \"ParameterValue\" ] for i in override_params } if override_type == \"global\" : override_params = { \"general\" : { \"parameters\" : override_params }} if not overrides_path . exists (): LOG . warning ( f \"Converting overrides to new format and saving in \" f \" { overrides_path } \" ) with open ( str ( overrides_path ), \"w\" ) as file_handle : file_handle . write ( yaml . dump ( override_params , default_flow_style = False )) else : LOG . warning ( f \"Ignoring legacy overrides as a current format \" f \"file has been found in { str ( overrides_path ) } \" )","title":"Module _legacy_config"},{"location":"reference/_legacy_config.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_legacy_config.html#functions","text":"","title":"Functions"},{"location":"reference/_legacy_config.html#legacy_overrides","text":"def legacy_overrides ( legacy_override , overrides_path , override_type ) View Source def legacy_overrides ( legacy_override , overrides_path , override_type ): if legacy_override . is_file (): with open ( str ( legacy_override ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) LOG . warning ( f \"overrides file {str(legacy_override)} is in legacy \" f \"format, support for this format will be deprecated \" f \"in a future version.\" ) override_params = { i [ \"ParameterKey\" ]: i [ \"ParameterValue\" ] for i in override_params } if override_type == \"global\" : override_params = { \"general\" : { \"parameters\" : override_params }} if not overrides_path . exists (): LOG . warning ( f \"Converting overrides to new format and saving in \" f \"{overrides_path}\" ) with open ( str ( overrides_path ), \"w\" ) as file_handle : file_handle . write ( yaml . dump ( override_params , default_flow_style = False )) else : LOG . warning ( f \"Ignoring legacy overrides as a current format \" f \"file has been found in {str(overrides_path)}\" )","title":"legacy_overrides"},{"location":"reference/_legacy_config.html#parse_legacy_config","text":"def parse_legacy_config ( project_root : pathlib . Path ) View Source def parse_legacy_config ( project_root : Path ) : config_file = ( project_root / \"ci/taskcat.yml\" ). expanduser (). resolve () if not config_file . is_file () : raise TaskCatException ( f \"No config_file at {config_file}\" ) with open ( str ( config_file ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) # need to rename global key , as it ' s a python keyword config_dict [ \"global_\" ] = config_dict . pop ( \"global\" ) legacy_config = LegacyConfig . from_dict ( config_dict ) tests = {} for test_name , test_data in legacy_config . tests . items () : parameters = {} parameter_file = project_root / \"ci/\" / test_data . parameter_input parameter_file = parameter_file . expanduser (). resolve () with open ( str ( parameter_file ), \"r\" ) as file_handle : for param in yaml . safe_load ( file_handle ) : parameters [ param[\"ParameterKey\" ] ] = param [ \"ParameterValue\" ] tests [ test_name ] = { \"template\" : \"templates/\" + test_data . template_file , \"parameters\" : parameters , \"regions\" : test_data . regions , } if not tests [ test_name ][ \"regions\" ] : del tests [ test_name ][ \"regions\" ] new_config_dict = { \"project\" : { \"name\" : legacy_config . global_ . qsname , \"owner\" : legacy_config . global_ . owner , \"s3_bucket\" : legacy_config . global_ . s3bucket , \"package_lambda\" : legacy_config . global_ . lambda_build , \"regions\" : legacy_config . global_ . regions , } , \"tests\" : tests , } new_config = BaseConfig . from_dict ( new_config_dict ) LOG . warning ( \"config is in a legacy format, support for which will be dropped in a \" \"future version. a new format config (.taskcat.yml) will been placed \" \"in your project_root\" ) new_config_path = project_root / \".taskcat.yml\" if new_config_path . exists () : LOG . warning ( f \"skipping new config file creation, file already exits at \" f \"{new_config_path}\" ) else : with open ( str ( new_config_path ), \"w\" ) as file_handle : config_dict = new_config . to_dict () config_dict . pop ( \"general\" ) yaml . dump ( config_dict , file_handle , default_flow_style = False ) return new_config","title":"parse_legacy_config"},{"location":"reference/_legacy_config.html#classes","text":"","title":"Classes"},{"location":"reference/_legacy_config.html#legacyconfig","text":"class LegacyConfig ( global_ : _legacy_config . LegacyGlobalConfig , tests : Dict [ str , _legacy_config . LegacyTestConfig ] )","title":"LegacyConfig"},{"location":"reference/_legacy_config.html#ancestors-in-mro","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_legacy_config.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_legacy_config.html#all_json_schemas","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_legacy_config.html#field_mapping","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_legacy_config.html#from_dict","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_legacy_config.html#from_json","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_legacy_config.html#from_object","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_legacy_config.html#json_schema","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_legacy_config.html#register_field_encoders","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_legacy_config.html#methods","text":"","title":"Methods"},{"location":"reference/_legacy_config.html#to_dict","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_legacy_config.html#to_json","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_legacy_config.html#legacyglobalconfig","text":"class LegacyGlobalConfig ( qsname : str , govcloud : bool = False , marketplace_ami : bool = False , owner : str = '' , regions : List [ str ] = < factory > , reporting : bool = True , lambda_build : bool = False , s3bucket : str = '' )","title":"LegacyGlobalConfig"},{"location":"reference/_legacy_config.html#ancestors-in-mro_1","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_legacy_config.html#class-variables","text":"govcloud lambda_build marketplace_ami owner reporting s3bucket","title":"Class variables"},{"location":"reference/_legacy_config.html#static-methods_1","text":"","title":"Static methods"},{"location":"reference/_legacy_config.html#all_json_schemas_1","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_legacy_config.html#field_mapping_1","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_legacy_config.html#from_dict_1","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_legacy_config.html#from_json_1","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_legacy_config.html#from_object_1","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_legacy_config.html#json_schema_1","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_legacy_config.html#register_field_encoders_1","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_legacy_config.html#methods_1","text":"","title":"Methods"},{"location":"reference/_legacy_config.html#to_dict_1","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_legacy_config.html#to_json_1","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_legacy_config.html#legacytestconfig","text":"class LegacyTestConfig ( template_file : str , parameter_input : str , regions : List [ str ] = < factory > )","title":"LegacyTestConfig"},{"location":"reference/_legacy_config.html#ancestors-in-mro_2","text":"dataclasses_jsonschema.JsonSchemaMixin","title":"Ancestors (in MRO)"},{"location":"reference/_legacy_config.html#static-methods_2","text":"","title":"Static methods"},{"location":"reference/_legacy_config.html#all_json_schemas_2","text":"def all_json_schemas ( schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True ) -> Dict [ str , Any ] Returns JSON schemas for all subclasses View Source @classmethod def all_json_schemas ( cls : Type [ T ] , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True ) -> JsonDict : \"\"\"Returns JSON schemas for all subclasses\"\"\" definitions = {} for subclass in cls . __subclasses__ () : if is_dataclass ( subclass ) : definitions . update ( subclass . json_schema ( embeddable = True , schema_type = schema_type , validate_enums = validate_enums ) ) else : definitions . update ( subclass . all_json_schemas ( schema_type = schema_type , validate_enums = validate_enums )) return definitions","title":"all_json_schemas"},{"location":"reference/_legacy_config.html#field_mapping_2","text":"def field_mapping ( ) -> Dict [ str , str ] Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords View Source @classmethod def field_mapping ( cls ) -> Dict [ str, str ] : \"\"\"Defines the mapping of python field names to JSON field names. The main use-case is to allow JSON field names which are Python keywords \"\"\" return {}","title":"field_mapping"},{"location":"reference/_legacy_config.html#from_dict_2","text":"def from_dict ( data : Dict [ str , Any ], validate = True , validate_enums : bool = True ) -> ~ T Returns a dataclass instance with all nested classes converted from the dict given View Source @classmethod def from_dict ( cls : Type [ T ] , data : JsonDict , validate = True , validate_enums : bool = True ) -> T : \"\"\"Returns a dataclass instance with all nested classes converted from the dict given\"\"\" if cls is JsonSchemaMixin : raise NotImplementedError if cls . __discriminator_name is not None and cls . __discriminator_name in data : if data [ cls.__discriminator_name ] != cls . __name__ : for subclass in cls . __subclasses__ () : if subclass . __name__ == data [ cls.__discriminator_name ] : return subclass . from_dict ( data , validate ) init_values : Dict [ str, Any ] = {} non_init_values : Dict [ str, Any ] = {} if validate : cls . _validate ( data , validate_enums ) for f in cls . _get_fields () : values = init_values if f . field . init else non_init_values if f . mapped_name in data or ( f . field . default == MISSING and f . field . default_factory == MISSING ) : # type : ignore try : values [ f.field.name ] = cls . _decode_field ( f . field . name , f . field . type , data . get ( f . mapped_name )) except ValueError : ftype = unwrap_optional ( f . field . type ) if is_optional ( f . field . type ) else f . field . type if is_enum ( ftype ) : values [ f.field.name ] = data . get ( f . mapped_name ) else : raise # Need to ignore the type error here , since mypy doesn ' t know that subclasses are dataclasses instance = cls ( ** init_values ) # type : ignore for field_name , value in non_init_values . items () : setattr ( instance , field_name , value ) return instance","title":"from_dict"},{"location":"reference/_legacy_config.html#from_json_2","text":"def from_json ( data : str , validate : bool = True , ** json_kwargs ) -> ~ T View Source @classmethod def from_json ( cls : Type [ T ] , data : str , validate : bool = True , ** json_kwargs ) -> T : return cls . from_dict ( json . loads ( data , ** json_kwargs ), validate )","title":"from_json"},{"location":"reference/_legacy_config.html#from_object_2","text":"def from_object ( obj : Any , exclude : Tuple [ Union [ str , Tuple [ str , ForwardRef ( 'FieldExcludeList' )]], ... ] = () ) -> ~ T Returns a dataclass instance from another object (typically an ORM model). The exclude parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example exclude=('artist_name', ('albums', ('tracks',)) will exclude the artist_name and tracks from related albums View Source @classmethod def from_object ( cls : Type [ T ] , obj : Any , exclude : FieldExcludeList = tuple ()) -> T : \" \"\" Returns a dataclass instance from another object (typically an ORM model). The `exclude` parameter is a tuple of field names or (field.name, nested_exclude) to exclude from the conversion. For example `exclude=('artist_name', ('albums', ('tracks',))` will exclude the `artist_name` and `tracks` from related albums \"\" \" exclude_dict = dict ( [ ( f [ 0 ] , f [ 1 ] ) if isinstance ( f , tuple ) else ( f , None ) for f in exclude ] ) init_values : Dict [ str , Any ] = {} non_init_values : Dict [ str , Any ] = {} for f in cls . _get_fields () : sub_exclude : FieldExcludeList = tuple () if f . field . name in exclude_dict : if exclude_dict [ f . field . name ] is None : if f . field . default == MISSING and f . field . default == MISSING : raise ValueError ( \"Excluded fields must have a default value\" ) continue else : sub_exclude = exclude_dict [ f . field . name ] # type: ignore values = init_values if f . field . init else non_init_values ft = f . field . type if is_optional ( ft ) : ft = unwrap_optional ( ft ) field_type_name = cls . _get_field_type_name ( ft ) from_value = getattr ( obj , f . field . name ) if from_value is None : values [ f . field . name ] = from_value elif cls . _is_json_schema_subclass ( ft ) : values [ f . field . name ] = ft . from_object ( from_value , exclude = sub_exclude ) elif is_enum ( ft ) : values [ f . field . name ] = ft ( from_value ) elif field_type_name in ( \"List\" , \"list\" ) and cls . _is_json_schema_subclass ( ft . __args__ [ 0 ] ) : values [ f . field . name ] = [ ft . __args__ [ 0 ] . from_object ( v , exclude = sub_exclude ) for v in from_value ] else : values [ f . field . name ] = from_value instance = cls ( ** init_values ) # type: ignore for field_name , value in non_init_values . items () : set attr ( instance , field_name , value ) return instance","title":"from_object"},{"location":"reference/_legacy_config.html#json_schema_2","text":"def json_schema ( embeddable : bool = False , schema_type : dataclasses_jsonschema . type_defs . SchemaType = < SchemaType . DRAFT_06 : 'Draft6' > , validate_enums : bool = True , ** kwargs ) -> Dict [ str , Any ] Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. View Source @classmethod def json_schema ( cls , embeddable : bool = False , schema_type : SchemaType = DEFAULT_SCHEMA_TYPE , validate_enums : bool = True , ** kwargs ) -> JsonDict : \"\"\"Returns the JSON schema for the dataclass, along with the schema of any nested dataclasses within the 'definitions' field. Enable the embeddable flag to generate the schema in a format for embedding into other schemas or documents supporting JSON schema such as Swagger specs. If embedding the schema into a swagger api, specify 'swagger_version' to generate a spec compatible with that version. \"\"\" if 'swagger_version' in kwargs and kwargs [ 'swagger_version' ] is not None : schema_type = kwargs [ 'swagger_version' ] schema_options = SchemaOptions ( schema_type , validate_enums ) if schema_options . schema_type in ( SchemaType . SWAGGER_V3 , SchemaType . SWAGGER_V2 ) and not embeddable : schema_options = SchemaOptions ( SchemaType . DRAFT_06 , validate_enums ) warnings . warn ( \"'Swagger schema types unsupported when 'embeddable=False', using 'SchemaType.DRAFT_06'\" ) if cls is JsonSchemaMixin : warnings . warn ( \"Calling 'JsonSchemaMixin.json_schema' is deprecated. Use 'JsonSchemaMixin.all_json_schemas' instead\" , DeprecationWarning ) return cls . all_json_schemas ( schema_options . schema_type , validate_enums ) definitions : JsonDict = {} if schema_options not in cls . __definitions : cls . __definitions [ schema_options ] = definitions else : definitions = cls . __definitions [ schema_options ] if schema_options in cls . __schema : schema = cls . __schema [ schema_options ] else : properties = {} required = [] for f in cls . _get_fields ( base_fields = False ) : properties [ f.mapped_name ] , is_required = cls . _get_field_schema ( f . field , schema_options ) if f . is_property : properties [ f.mapped_name ][ \"readOnly\" ] = True cls . _get_field_definitions ( f . field . type , definitions , schema_options ) # Only add 'readOnly' properties to required for OpenAPI 3 if is_required and ( not f . is_property or schema_options . schema_type == SchemaType . OPENAPI_3 ) : required . append ( f . mapped_name ) schema = { 'type' : 'object' , 'required' : required , 'properties' : properties } if schema_options . schema_type == SchemaType . OPENAPI_3 : schema [ 'x-module-name' ] = cls . __module__ if not cls . __allow_additional_props : schema [ \"additionalProperties\" ] = False if cls . __discriminator_name is not None and \\ schema_options . schema_type == SchemaType . OPENAPI_3 and \\ not cls . __discriminator_inherited : schema [ 'discriminator' ] = { 'propertyName' : cls . __discriminator_name } properties [ cls.__discriminator_name ] = { \"type\" : \"string\" } required . append ( cls . __discriminator_name ) # Needed for Draft 04 backwards compatibility if len ( required ) == 0 : del schema [ \"required\" ] dataclass_bases = [ klass for klass in cls.__bases__ if is_dataclass(klass) and issubclass(klass, JsonSchemaMixin) ] if len ( dataclass_bases ) > 0 : schema = { \"allOf\" : [ schema_reference(schema_options.schema_type, base.__name__) for base in dataclass_bases ] + [ schema ] } for base in dataclass_bases : definitions . update ( base . json_schema ( embeddable = True , schema_type = schema_options . schema_type , validate_enums = schema_options . validate_enums ) ) if cls . __doc__ : schema [ 'description' ] = cls . __doc__ cls . __schema [ schema_options ] = schema if embeddable : return { ** definitions , cls . __name__ : schema } else : schema_uri = 'http://json-schema.org/draft-06/schema#' if schema_options . schema_type == SchemaType . DRAFT_04 : schema_uri = 'http://json-shema.org/draft-04/schema#' full_schema = { ** schema , ** { '$schema' : schema_uri }} if len ( definitions ) > 0 : full_schema [ 'definitions' ] = definitions return full_schema","title":"json_schema"},{"location":"reference/_legacy_config.html#register_field_encoders_2","text":"def register_field_encoders ( field_encoders : Dict [ Type , dataclasses_jsonschema . field_types . FieldEncoder ] ) Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. View Source @classmethod def register_field_encoders ( cls , field_encoders : Dict [ Type, FieldEncoder ] ) : \"\"\"Registers additional custom field encoders. If called on the base, these are added globally. The DateTimeFieldEncoder is included by default. \"\"\" if cls is not JsonSchemaMixin : cls . _field_encoders = { ** cls . _field_encoders , ** field_encoders } else : cls . _field_encoders . update ( field_encoders )","title":"register_field_encoders"},{"location":"reference/_legacy_config.html#methods_2","text":"","title":"Methods"},{"location":"reference/_legacy_config.html#to_dict_2","text":"def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> Dict [ str , Any ] Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed View Source def to_dict ( self , omit_none : bool = True , validate : bool = False , validate_enums : bool = True ) -> JsonDict : \"\"\"Converts the dataclass instance to a JSON encodable dict, with optional JSON schema validation. If omit_none (default True) is specified, any items with value None are removed \"\"\" data = {} for f in self . _get_fields (): value = getattr ( self , f . field . name ) try : value = self . _encode_field ( f . field . type , value , omit_none ) except UnknownEnumValueError as e : warnings . warn ( str ( e )) if omit_none and value is None : continue if value is NULL : value = None data [ f . mapped_name ] = value if self . __discriminator_name is not None : data [ self . __discriminator_name ] = self . __class__ . __name__ if validate : self . _validate ( data , validate_enums ) return data","title":"to_dict"},{"location":"reference/_legacy_config.html#to_json_2","text":"def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str View Source def to_json ( self , omit_none : bool = True , validate : bool = False , ** json_kwargs ) -> str : return json . dumps ( self . to_dict ( omit_none , validate ), ** json_kwargs )","title":"to_json"},{"location":"reference/_logger.html","text":"Module _logger None None View Source import logging LOG = logging . getLogger ( __name__ ) class PrintMsg : header = \" \\x1b [1;41;0m\" highlight = \" \\x1b [0;30;47m\" name_color = \" \\x1b [0;37;44m\" aqua = \" \\x1b [0;30;46m\" green = \" \\x1b [0;30;42m\" white = \" \\x1b [0;30;47m\" orange = \" \\x1b [0;30;43m\" red = \" \\x1b [0;30;41m\" rst_color = \" \\x1b [0m\" CRITICAL = \" {} [FATAL ] {} : \" . format ( red , rst_color ) ERROR = \" {} [ERROR ] {} : \" . format ( red , rst_color ) DEBUG = \" {} [DEBUG ] {} : \" . format ( aqua , rst_color ) PASS = \" {} [PASS ] {} : \" . format ( green , rst_color ) INFO = \" {} [INFO ] {} : \" . format ( white , rst_color ) WARNING = \" {} [WARN ] {} : \" . format ( orange , rst_color ) NAMETAG = \" {1}{0}{2} \" . format ( \"taskcat\" , name_color , rst_color ) S3 = \" {} [S3: -> ] {} \" . format ( white , rst_color ) S3DELETE = \" {} [S3: DELETE ] {} \" . format ( white , rst_color ) class AppFilter ( logging . Filter ): def filter ( self , record ): if \"nametag\" in dir ( record ): record . color_loglevel = record . nametag else : record . color_loglevel = getattr ( PrintMsg , record . levelname ) return True def init_taskcat_cli_logger ( loglevel = None ): log = logging . getLogger ( __package__ ) cli_handler = logging . StreamHandler () formatter = logging . Formatter ( \" %(color_loglevel)s%(message)s \" ) cli_handler . setFormatter ( formatter ) cli_handler . addFilter ( AppFilter ()) log . addHandler ( cli_handler ) if loglevel : loglevel = getattr ( logging , loglevel . upper (), 20 ) log . setLevel ( loglevel ) return log Variables LOG Functions init_taskcat_cli_logger def init_taskcat_cli_logger ( loglevel = None ) View Source def init_taskcat_cli_logger ( loglevel = None ) : log = logging . getLogger ( __package__ ) cli_handler = logging . StreamHandler () formatter = logging . Formatter ( \" %(color_loglevel)s%(message)s \" ) cli_handler . setFormatter ( formatter ) cli_handler . addFilter ( AppFilter ()) log . addHandler ( cli_handler ) if loglevel : loglevel = getattr ( logging , loglevel . upper () , 20 ) log . setLevel ( loglevel ) return log Classes AppFilter class AppFilter ( name = '' ) Ancestors (in MRO) logging.Filter Methods filter def filter ( self , record ) View Source def filter ( self , record ) : if \" nametag \" in dir ( record ) : record . color_loglevel = record . nametag else : record . color_loglevel = getattr ( PrintMsg , record . levelname ) return True PrintMsg class PrintMsg ( / , * args , ** kwargs ) Class variables CRITICAL DEBUG ERROR INFO NAMETAG PASS S3 S3DELETE WARNING aqua green header highlight name_color orange red rst_color white","title":" Logger"},{"location":"reference/_logger.html#module-_logger","text":"None None View Source import logging LOG = logging . getLogger ( __name__ ) class PrintMsg : header = \" \\x1b [1;41;0m\" highlight = \" \\x1b [0;30;47m\" name_color = \" \\x1b [0;37;44m\" aqua = \" \\x1b [0;30;46m\" green = \" \\x1b [0;30;42m\" white = \" \\x1b [0;30;47m\" orange = \" \\x1b [0;30;43m\" red = \" \\x1b [0;30;41m\" rst_color = \" \\x1b [0m\" CRITICAL = \" {} [FATAL ] {} : \" . format ( red , rst_color ) ERROR = \" {} [ERROR ] {} : \" . format ( red , rst_color ) DEBUG = \" {} [DEBUG ] {} : \" . format ( aqua , rst_color ) PASS = \" {} [PASS ] {} : \" . format ( green , rst_color ) INFO = \" {} [INFO ] {} : \" . format ( white , rst_color ) WARNING = \" {} [WARN ] {} : \" . format ( orange , rst_color ) NAMETAG = \" {1}{0}{2} \" . format ( \"taskcat\" , name_color , rst_color ) S3 = \" {} [S3: -> ] {} \" . format ( white , rst_color ) S3DELETE = \" {} [S3: DELETE ] {} \" . format ( white , rst_color ) class AppFilter ( logging . Filter ): def filter ( self , record ): if \"nametag\" in dir ( record ): record . color_loglevel = record . nametag else : record . color_loglevel = getattr ( PrintMsg , record . levelname ) return True def init_taskcat_cli_logger ( loglevel = None ): log = logging . getLogger ( __package__ ) cli_handler = logging . StreamHandler () formatter = logging . Formatter ( \" %(color_loglevel)s%(message)s \" ) cli_handler . setFormatter ( formatter ) cli_handler . addFilter ( AppFilter ()) log . addHandler ( cli_handler ) if loglevel : loglevel = getattr ( logging , loglevel . upper (), 20 ) log . setLevel ( loglevel ) return log","title":"Module _logger"},{"location":"reference/_logger.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_logger.html#functions","text":"","title":"Functions"},{"location":"reference/_logger.html#init_taskcat_cli_logger","text":"def init_taskcat_cli_logger ( loglevel = None ) View Source def init_taskcat_cli_logger ( loglevel = None ) : log = logging . getLogger ( __package__ ) cli_handler = logging . StreamHandler () formatter = logging . Formatter ( \" %(color_loglevel)s%(message)s \" ) cli_handler . setFormatter ( formatter ) cli_handler . addFilter ( AppFilter ()) log . addHandler ( cli_handler ) if loglevel : loglevel = getattr ( logging , loglevel . upper () , 20 ) log . setLevel ( loglevel ) return log","title":"init_taskcat_cli_logger"},{"location":"reference/_logger.html#classes","text":"","title":"Classes"},{"location":"reference/_logger.html#appfilter","text":"class AppFilter ( name = '' )","title":"AppFilter"},{"location":"reference/_logger.html#ancestors-in-mro","text":"logging.Filter","title":"Ancestors (in MRO)"},{"location":"reference/_logger.html#methods","text":"","title":"Methods"},{"location":"reference/_logger.html#filter","text":"def filter ( self , record ) View Source def filter ( self , record ) : if \" nametag \" in dir ( record ) : record . color_loglevel = record . nametag else : record . color_loglevel = getattr ( PrintMsg , record . levelname ) return True","title":"filter"},{"location":"reference/_logger.html#printmsg","text":"class PrintMsg ( / , * args , ** kwargs )","title":"PrintMsg"},{"location":"reference/_logger.html#class-variables","text":"CRITICAL DEBUG ERROR INFO NAMETAG PASS S3 S3DELETE WARNING aqua green header highlight name_color orange red rst_color white","title":"Class variables"},{"location":"reference/_name_generator.html","text":"Module _name_generator None None View Source from pathlib import Path from random import choice from taskcat.exceptions import TaskCatException def generate_name (): path : Path = ( Path ( __file__ ) . parent / \"./cfg/\" ) . resolve () if not ( path / \"animals.txt\" ) . is_file () or not ( path / \"descriptors.txt\" ) . is_file (): raise TaskCatException ( \"cannot find dictionary files\" ) animals = open ( str ( path / \"animals.txt\" ), \"r\" ) . read () . split ( \" \\n \" ) descriptors = open ( str ( path / \"descriptors.txt\" ), \"r\" ) . read () . split ( \" \\n \" ) return choice ( descriptors ) + \"-\" + choice ( animals ) # nosec: B311 Functions generate_name def generate_name ( ) View Source def generate_name () : path : Path = ( Path ( __file__ ) . parent / \" ./cfg/ \" ) . resolve () if not ( path / \" animals.txt \" ) . is_file () or not ( path / \" descriptors.txt \" ) . is_file () : raise TaskCatException ( \" cannot find dictionary files \" ) animals = open ( str ( path / \" animals.txt \" ) , \" r \" ) . read () . split ( \" \\n \" ) descriptors = open ( str ( path / \" descriptors.txt \" ) , \" r \" ) . read () . split ( \" \\n \" ) return choice ( descriptors ) + \" - \" + choice ( animals ) # nosec : B311","title":" Name Generator"},{"location":"reference/_name_generator.html#module-_name_generator","text":"None None View Source from pathlib import Path from random import choice from taskcat.exceptions import TaskCatException def generate_name (): path : Path = ( Path ( __file__ ) . parent / \"./cfg/\" ) . resolve () if not ( path / \"animals.txt\" ) . is_file () or not ( path / \"descriptors.txt\" ) . is_file (): raise TaskCatException ( \"cannot find dictionary files\" ) animals = open ( str ( path / \"animals.txt\" ), \"r\" ) . read () . split ( \" \\n \" ) descriptors = open ( str ( path / \"descriptors.txt\" ), \"r\" ) . read () . split ( \" \\n \" ) return choice ( descriptors ) + \"-\" + choice ( animals ) # nosec: B311","title":"Module _name_generator"},{"location":"reference/_name_generator.html#functions","text":"","title":"Functions"},{"location":"reference/_name_generator.html#generate_name","text":"def generate_name ( ) View Source def generate_name () : path : Path = ( Path ( __file__ ) . parent / \" ./cfg/ \" ) . resolve () if not ( path / \" animals.txt \" ) . is_file () or not ( path / \" descriptors.txt \" ) . is_file () : raise TaskCatException ( \" cannot find dictionary files \" ) animals = open ( str ( path / \" animals.txt \" ) , \" r \" ) . read () . split ( \" \\n \" ) descriptors = open ( str ( path / \" descriptors.txt \" ) , \" r \" ) . read () . split ( \" \\n \" ) return choice ( descriptors ) + \" - \" + choice ( animals ) # nosec : B311","title":"generate_name"},{"location":"reference/_project_generator.html","text":"Module _project_generator None None View Source import logging import os from collections import namedtuple from jinja2 import Template TEMPLATES_ROOT_DIR = \"project_templates\" TEMPLATE_FILE_EXTENSION = \".jinja\" def full_path ( root , resource ): return root + os . sep + resource def template_paths ( template_dir , templates ): return [ template_dir + os . sep + t for t in templates if t . endswith ( TEMPLATE_FILE_EXTENSION ) ] ProjectConfiguration = namedtuple ( \"ProjectConfiguration\" , \"owner_email, project_name, project_type, supported_regions\" ) LOG = logging . getLogger () class ProjectGenerator : def __init__ ( self , config , destination_directory , filesystem_service ): LOG . info ( \"Initializing with...\" ) LOG . info ( f \"Project configuration: { config } \" ) LOG . info ( f \"Project destination: { destination_directory } \" ) self . config = config self . destination = destination_directory self . filesystem = filesystem_service def generate ( self ): for directory , _ , files in self . _traverse_templates (): project_path = self . _full_destination_path ( directory ) self . _make_project_directory ( project_path ) template_filepaths = template_paths ( directory , files ) self . _generate_project_files ( template_filepaths ) def _traverse_templates ( self ): return self . filesystem . traverse_templates ( self . config . project_type ) def _full_destination_path ( self , destination_path ): templates_root = self . filesystem . project_templates_root ( self . config . project_type ) destination_path = destination_path . replace ( templates_root , \"\" ) return full_path ( self . destination , destination_path ) def _make_project_directory ( self , project_directory ): try : LOG . info ( f \"creating { project_directory } \" ) self . filesystem . create_project_directory ( project_directory ) except FileExistsError as e : logging . warning ( f \" { e } - skipping...\" ) def _generate_project_files ( self , template_filepaths ): LOG . info ( \"generating files...\" ) for filepath in template_filepaths : template = self . filesystem . load_template ( filepath ) destination_filepath = self . _destination_filepath ( filepath ) self . filesystem . generate_file ( self . _render_template_content ( template ), destination_filepath ) LOG . info ( f \"generated { destination_filepath } \" ) def _destination_filepath ( self , filepath ): destination = self . _full_destination_path ( filepath ) return self . _remove_template_extension ( destination ) @staticmethod def _remove_template_extension ( filename ): return os . path . splitext ( filename )[ 0 ] def _render_template_content ( self , template ): return template . render ( config = self . config ) class FilesystemService : def project_templates_root ( self , project_type ): root = self . _templates_root_path () + os . sep + project_type + os . sep return root def traverse_templates ( self , project_type ): \"\"\" A wrapper around os.walk that returns the generator to traverse the templates directory \"\"\" return os . walk ( self . project_templates_root ( project_type )) @staticmethod def create_project_directory ( project_path ): os . mkdir ( project_path ) @staticmethod def generate_file ( content , destination_path ): \"\"\" Given the generated content and a destination path, it will write that content to a file in that path. \"\"\" with open ( destination_path , \"w\" ) as file_handle : file_handle . write ( content ) @staticmethod def load_template ( template_path ): \"\"\" Give a full path to a template file it will return a jinja2 Template object that responds to `render` method taking the template parameters \"\"\" with open ( template_path ) as file_handle : return Template ( file_handle . read ()) @staticmethod def _templates_root_path (): return os . path . dirname ( os . path . realpath ( __file__ )) + os . sep + TEMPLATES_ROOT_DIR Variables LOG TEMPLATES_ROOT_DIR TEMPLATE_FILE_EXTENSION Functions full_path def full_path ( root , resource ) View Source def full_path ( root , resource ) : return root + os . sep + resource template_paths def template_paths ( template_dir , templates ) View Source def template_paths ( template_dir , templates ) : return [ template_dir + os . sep + t for t in templates if t . endswith ( TEMPLATE_FILE_EXTENSION ) ] Classes FilesystemService class FilesystemService ( / , * args , ** kwargs ) Static methods create_project_directory def create_project_directory ( project_path ) View Source @staticmethod def create_project_directory ( project_path ) : os . mkdir ( project_path ) generate_file def generate_file ( content , destination_path ) Given the generated content and a destination path, it will write that content to a file in that path. View Source @staticmethod def generate_file ( content , destination_path ) : \"\"\" Given the generated content and a destination path, it will write that content to a file in that path. \"\"\" with open ( destination_path , \"w\" ) as file_handle : file_handle . write ( content ) load_template def load_template ( template_path ) Give a full path to a template file it will return a jinja2 Template object that responds to render method taking the template parameters View Source @staticmethod def load_template ( template_path ) : \" \"\" Give a full path to a template file it will return a jinja2 Template object that responds to `render` method taking the template parameters \"\" \" with open ( template_path ) as file_handle : return Template ( file_handle . read ()) Methods project_templates_root def project_templates_root ( self , project_type ) View Source def project_templates_root ( self , project_type ) : root = self . _templates_root_path () + os . sep + project_type + os . sep return root traverse_templates def traverse_templates ( self , project_type ) A wrapper around os.walk that returns the generator to traverse the templates directory View Source def traverse_templates ( self , project_type ) : \"\"\" A wrapper around os . walk that returns the generator to traverse the templates directory \"\"\" return os . walk ( self . project_templates_root ( project_type )) ProjectConfiguration class ProjectConfiguration ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.tuple Class variables owner_email project_name project_type supported_regions Methods count def count ( self , value , / ) Return number of occurrences of value. index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. ProjectGenerator class ProjectGenerator ( config , destination_directory , filesystem_service ) Methods generate def generate ( self ) View Source def generate ( self ) : for directory , _ , files in self . _traverse_templates () : project_path = self . _full_destination_path ( directory ) self . _make_project_directory ( project_path ) template_filepaths = template_paths ( directory , files ) self . _generate_project_files ( template_filepaths )","title":" Project Generator"},{"location":"reference/_project_generator.html#module-_project_generator","text":"None None View Source import logging import os from collections import namedtuple from jinja2 import Template TEMPLATES_ROOT_DIR = \"project_templates\" TEMPLATE_FILE_EXTENSION = \".jinja\" def full_path ( root , resource ): return root + os . sep + resource def template_paths ( template_dir , templates ): return [ template_dir + os . sep + t for t in templates if t . endswith ( TEMPLATE_FILE_EXTENSION ) ] ProjectConfiguration = namedtuple ( \"ProjectConfiguration\" , \"owner_email, project_name, project_type, supported_regions\" ) LOG = logging . getLogger () class ProjectGenerator : def __init__ ( self , config , destination_directory , filesystem_service ): LOG . info ( \"Initializing with...\" ) LOG . info ( f \"Project configuration: { config } \" ) LOG . info ( f \"Project destination: { destination_directory } \" ) self . config = config self . destination = destination_directory self . filesystem = filesystem_service def generate ( self ): for directory , _ , files in self . _traverse_templates (): project_path = self . _full_destination_path ( directory ) self . _make_project_directory ( project_path ) template_filepaths = template_paths ( directory , files ) self . _generate_project_files ( template_filepaths ) def _traverse_templates ( self ): return self . filesystem . traverse_templates ( self . config . project_type ) def _full_destination_path ( self , destination_path ): templates_root = self . filesystem . project_templates_root ( self . config . project_type ) destination_path = destination_path . replace ( templates_root , \"\" ) return full_path ( self . destination , destination_path ) def _make_project_directory ( self , project_directory ): try : LOG . info ( f \"creating { project_directory } \" ) self . filesystem . create_project_directory ( project_directory ) except FileExistsError as e : logging . warning ( f \" { e } - skipping...\" ) def _generate_project_files ( self , template_filepaths ): LOG . info ( \"generating files...\" ) for filepath in template_filepaths : template = self . filesystem . load_template ( filepath ) destination_filepath = self . _destination_filepath ( filepath ) self . filesystem . generate_file ( self . _render_template_content ( template ), destination_filepath ) LOG . info ( f \"generated { destination_filepath } \" ) def _destination_filepath ( self , filepath ): destination = self . _full_destination_path ( filepath ) return self . _remove_template_extension ( destination ) @staticmethod def _remove_template_extension ( filename ): return os . path . splitext ( filename )[ 0 ] def _render_template_content ( self , template ): return template . render ( config = self . config ) class FilesystemService : def project_templates_root ( self , project_type ): root = self . _templates_root_path () + os . sep + project_type + os . sep return root def traverse_templates ( self , project_type ): \"\"\" A wrapper around os.walk that returns the generator to traverse the templates directory \"\"\" return os . walk ( self . project_templates_root ( project_type )) @staticmethod def create_project_directory ( project_path ): os . mkdir ( project_path ) @staticmethod def generate_file ( content , destination_path ): \"\"\" Given the generated content and a destination path, it will write that content to a file in that path. \"\"\" with open ( destination_path , \"w\" ) as file_handle : file_handle . write ( content ) @staticmethod def load_template ( template_path ): \"\"\" Give a full path to a template file it will return a jinja2 Template object that responds to `render` method taking the template parameters \"\"\" with open ( template_path ) as file_handle : return Template ( file_handle . read ()) @staticmethod def _templates_root_path (): return os . path . dirname ( os . path . realpath ( __file__ )) + os . sep + TEMPLATES_ROOT_DIR","title":"Module _project_generator"},{"location":"reference/_project_generator.html#variables","text":"LOG TEMPLATES_ROOT_DIR TEMPLATE_FILE_EXTENSION","title":"Variables"},{"location":"reference/_project_generator.html#functions","text":"","title":"Functions"},{"location":"reference/_project_generator.html#full_path","text":"def full_path ( root , resource ) View Source def full_path ( root , resource ) : return root + os . sep + resource","title":"full_path"},{"location":"reference/_project_generator.html#template_paths","text":"def template_paths ( template_dir , templates ) View Source def template_paths ( template_dir , templates ) : return [ template_dir + os . sep + t for t in templates if t . endswith ( TEMPLATE_FILE_EXTENSION ) ]","title":"template_paths"},{"location":"reference/_project_generator.html#classes","text":"","title":"Classes"},{"location":"reference/_project_generator.html#filesystemservice","text":"class FilesystemService ( / , * args , ** kwargs )","title":"FilesystemService"},{"location":"reference/_project_generator.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_project_generator.html#create_project_directory","text":"def create_project_directory ( project_path ) View Source @staticmethod def create_project_directory ( project_path ) : os . mkdir ( project_path )","title":"create_project_directory"},{"location":"reference/_project_generator.html#generate_file","text":"def generate_file ( content , destination_path ) Given the generated content and a destination path, it will write that content to a file in that path. View Source @staticmethod def generate_file ( content , destination_path ) : \"\"\" Given the generated content and a destination path, it will write that content to a file in that path. \"\"\" with open ( destination_path , \"w\" ) as file_handle : file_handle . write ( content )","title":"generate_file"},{"location":"reference/_project_generator.html#load_template","text":"def load_template ( template_path ) Give a full path to a template file it will return a jinja2 Template object that responds to render method taking the template parameters View Source @staticmethod def load_template ( template_path ) : \" \"\" Give a full path to a template file it will return a jinja2 Template object that responds to `render` method taking the template parameters \"\" \" with open ( template_path ) as file_handle : return Template ( file_handle . read ())","title":"load_template"},{"location":"reference/_project_generator.html#methods","text":"","title":"Methods"},{"location":"reference/_project_generator.html#project_templates_root","text":"def project_templates_root ( self , project_type ) View Source def project_templates_root ( self , project_type ) : root = self . _templates_root_path () + os . sep + project_type + os . sep return root","title":"project_templates_root"},{"location":"reference/_project_generator.html#traverse_templates","text":"def traverse_templates ( self , project_type ) A wrapper around os.walk that returns the generator to traverse the templates directory View Source def traverse_templates ( self , project_type ) : \"\"\" A wrapper around os . walk that returns the generator to traverse the templates directory \"\"\" return os . walk ( self . project_templates_root ( project_type ))","title":"traverse_templates"},{"location":"reference/_project_generator.html#projectconfiguration","text":"class ProjectConfiguration ( / , * args , ** kwargs )","title":"ProjectConfiguration"},{"location":"reference/_project_generator.html#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/_project_generator.html#class-variables","text":"owner_email project_name project_type supported_regions","title":"Class variables"},{"location":"reference/_project_generator.html#methods_1","text":"","title":"Methods"},{"location":"reference/_project_generator.html#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/_project_generator.html#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/_project_generator.html#projectgenerator","text":"class ProjectGenerator ( config , destination_directory , filesystem_service )","title":"ProjectGenerator"},{"location":"reference/_project_generator.html#methods_2","text":"","title":"Methods"},{"location":"reference/_project_generator.html#generate","text":"def generate ( self ) View Source def generate ( self ) : for directory , _ , files in self . _traverse_templates () : project_path = self . _full_destination_path ( directory ) self . _make_project_directory ( project_path ) template_filepaths = template_paths ( directory , files ) self . _generate_project_files ( template_filepaths )","title":"generate"},{"location":"reference/_s3_stage.html","text":"Module _s3_stage None None View Source import logging from functools import partial from multiprocessing.dummy import Pool as ThreadPool from taskcat._s3_sync import S3Sync from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class S3APIResponse : def __init__ ( self , x ): self . _http_code = x [ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] @property def ok ( self ): if self . _http_code == 200 : return True return False class S3BucketCreatorException ( TaskCatException ): pass def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ): distinct_buckets = {} for test in buckets . values (): for bucket in test . values (): distinct_buckets [ f \" { bucket . name } - { bucket . partition } \" ] = bucket pool = ThreadPool ( 32 ) func = partial ( _sync_wrap , project_name = project_name , project_root = project_root , dry_run = dry_run , ) pool . map ( func , distinct_buckets . values ()) pool . close () pool . join () def _sync_wrap ( bucket , project_name , project_root , dry_run ): S3Sync ( bucket . s3_client , bucket . name , project_name , project_root , bucket . object_acl , dry_run = dry_run , ) Variables LOG Functions stage_in_s3 def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ) View Source def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ): distinct_buckets = {} for test in buckets . values (): for bucket in test . values (): distinct_buckets [ f \"{bucket.name}-{bucket.partition}\" ] = bucket pool = ThreadPool ( 32 ) func = partial ( _sync_wrap , project_name = project_name , project_root = project_root , dry_run = dry_run , ) pool . map ( func , distinct_buckets . values ()) pool . close () pool . join () Classes S3APIResponse class S3APIResponse ( x ) Instance variables ok S3BucketCreatorException class S3BucketCreatorException ( / , * args , ** kwargs ) Ancestors (in MRO) taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":" S3 Stage"},{"location":"reference/_s3_stage.html#module-_s3_stage","text":"None None View Source import logging from functools import partial from multiprocessing.dummy import Pool as ThreadPool from taskcat._s3_sync import S3Sync from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class S3APIResponse : def __init__ ( self , x ): self . _http_code = x [ \"ResponseMetadata\" ][ \"HTTPStatusCode\" ] @property def ok ( self ): if self . _http_code == 200 : return True return False class S3BucketCreatorException ( TaskCatException ): pass def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ): distinct_buckets = {} for test in buckets . values (): for bucket in test . values (): distinct_buckets [ f \" { bucket . name } - { bucket . partition } \" ] = bucket pool = ThreadPool ( 32 ) func = partial ( _sync_wrap , project_name = project_name , project_root = project_root , dry_run = dry_run , ) pool . map ( func , distinct_buckets . values ()) pool . close () pool . join () def _sync_wrap ( bucket , project_name , project_root , dry_run ): S3Sync ( bucket . s3_client , bucket . name , project_name , project_root , bucket . object_acl , dry_run = dry_run , )","title":"Module _s3_stage"},{"location":"reference/_s3_stage.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_s3_stage.html#functions","text":"","title":"Functions"},{"location":"reference/_s3_stage.html#stage_in_s3","text":"def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ) View Source def stage_in_s3 ( buckets , project_name , project_root , dry_run = False ): distinct_buckets = {} for test in buckets . values (): for bucket in test . values (): distinct_buckets [ f \"{bucket.name}-{bucket.partition}\" ] = bucket pool = ThreadPool ( 32 ) func = partial ( _sync_wrap , project_name = project_name , project_root = project_root , dry_run = dry_run , ) pool . map ( func , distinct_buckets . values ()) pool . close () pool . join ()","title":"stage_in_s3"},{"location":"reference/_s3_stage.html#classes","text":"","title":"Classes"},{"location":"reference/_s3_stage.html#s3apiresponse","text":"class S3APIResponse ( x )","title":"S3APIResponse"},{"location":"reference/_s3_stage.html#instance-variables","text":"ok","title":"Instance variables"},{"location":"reference/_s3_stage.html#s3bucketcreatorexception","text":"class S3BucketCreatorException ( / , * args , ** kwargs )","title":"S3BucketCreatorException"},{"location":"reference/_s3_stage.html#ancestors-in-mro","text":"taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/_s3_stage.html#class-variables","text":"args","title":"Class variables"},{"location":"reference/_s3_stage.html#methods","text":"","title":"Methods"},{"location":"reference/_s3_stage.html#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/_s3_sync.html","text":"Module _s3_sync None None View Source import fnmatch import hashlib import logging import os import time from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import List from boto3.exceptions import S3UploadFailedError from boto3.s3.transfer import TransferConfig from taskcat._logger import PrintMsg from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class S3Sync : \"\"\"Syncronizes local project files with S3 based on checksums. Excludes hidden files, unpackaged lambda source and taskcat /ci/ files. Uses the Etag as an md5 which introduces the following limitations * Uses undocumented etag algorithm for multipart uploads * Does not work wil files uploaded in the console that use SSE encryption * see https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html for more info Does not support buckets with versioning enabled \"\"\" # TODO: better exclusions that support path wildcards, eg. \"*/.git/*\" exclude_files = [ \".*\" , \"*.md\" ] exclude_path_prefixes = [ \"lambda_functions/source/\" , \"functions/source/\" , \".\" , \"venv/\" , \"taskcat_outputs/\" , ] exclude_remote_path_prefixes : List [ str ] = [] def __init__ ( self , s3_client , bucket , prefix , path , acl = \"private\" , dry_run = False ): \"\"\"Syncronizes local file system with an s3 bucket/prefix \"\"\" if prefix != \"\" and not prefix . endswith ( \"/\" ): prefix = prefix + \"/\" self . s3_client = s3_client self . dry_run = dry_run file_list = self . _get_local_file_list ( path ) s3_file_list = self . _get_s3_file_list ( bucket , prefix ) self . _sync ( file_list , s3_file_list , bucket , prefix , acl = acl ) @staticmethod def _hash_file ( file_path , chunk_size = 8 * 1024 * 1024 ): # This is a bit funky because of the way multipart upload etags are done, they # are a md5 of the md5's from each part with the number of parts appended # credit to hyperknot https://github.com/aws/aws-cli/issues/2585#issue-226758933 md5s = [] with open ( file_path , \"rb\" ) as file_handle : while True : data = file_handle . read ( chunk_size ) if not data : break md5s . append ( hashlib . md5 ( data )) # nosec if len ( md5s ) == 1 : return '\" {} \"' . format ( md5s [ 0 ] . hexdigest ()) digests = b \"\" . join ( m . digest () for m in md5s ) digests_md5 = hashlib . md5 ( digests ) # nosec return '\" {} - {} \"' . format ( digests_md5 . hexdigest (), len ( md5s )) # TODO: refactor def _get_local_file_list ( self , path , include_checksums = True ): # pylint: disable=too-many-locals file_list = {} # get absolute local path path = os . path . abspath ( os . path . expanduser ( path )) # recurse through directories for root , _ , files in os . walk ( path ): relpath = os . path . relpath ( root , path ) + \"/\" exclude_path = False # relative path should be blank if there are no sub directories if relpath == \"./\" : relpath = \"\" # exclude defined paths for prefix in S3Sync . exclude_path_prefixes : if relpath . startswith ( prefix ): exclude_path = True break if not exclude_path : file_list . update ( self . _iterate_files ( files , root , include_checksums , relpath ) ) return file_list def _iterate_files ( self , files , root , include_checksums , relpath ): file_list = {} for file in files : exclude = False # exclude defined filename patterns for pattern in S3Sync . exclude_files : if fnmatch . fnmatch ( file , pattern ): exclude = True break if not exclude : full_path = root + \"/\" + file if include_checksums : # get checksum checksum = self . _hash_file ( full_path ) else : checksum = \"\" file_list [ relpath + file ] = [ full_path , checksum ] return file_list def _get_s3_file_list ( self , bucket , prefix ): objects = {} is_paginated = True continuation_token = None # While there are more results, fetch them from S3 while is_paginated : # if there's no token, this is the initial list_objects call if not continuation_token : resp = self . s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) # this is a query to get additional pages, add continuation token to get # next page else : resp = self . s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , ContinuationToken = continuation_token ) if \"Contents\" in resp : for file in resp [ \"Contents\" ]: # strip the prefix from the path relpath = file [ \"Key\" ][ len ( prefix ) :] objects [ relpath ] = file [ \"ETag\" ] if \"NextContinuationToken\" in resp . keys (): continuation_token = resp [ \"NextContinuationToken\" ] # If there's no toke in the response we've fetched all the objects else : is_paginated = False return objects @staticmethod def _exclude_remote ( path ): keep = False for exclude in S3Sync . exclude_remote_path_prefixes : if path . startswith ( exclude ): keep = True break return keep # TODO: refactor def _sync ( # noqa: C901 self , local_list , s3_list , bucket , prefix , acl , threads = 16 ): # pylint: disable=too-many-locals # determine which files to remove from S3 remove_from_s3 = [] for s3_file in s3_list . keys (): if s3_file not in local_list . keys () and not self . _exclude_remote ( s3_file ): if self . dry_run : LOG . info ( f \"[DRY RUN] s3:// { bucket } / { prefix + prefix + s3_file } \" , extra = { \"nametag\" : PrintMsg . S3DELETE }, ) else : LOG . info ( f \"s3:// { bucket } / { prefix + prefix + s3_file } \" , extra = { \"nametag\" : PrintMsg . S3DELETE }, ) remove_from_s3 . append ({ \"Key\" : prefix + s3_file }) # deleting objects, max 1k objects per s3 delete_objects call if not self . dry_run : for objects in [ remove_from_s3 [ i : i + 1000 ] for i in range ( 0 , len ( remove_from_s3 ), 1000 ) ]: response = self . s3_client . delete_objects ( Bucket = bucket , Delete = { \"Objects\" : objects } ) if \"Errors\" in response . keys (): for error in response [ \"Errors\" ]: LOG . error ( \"S3 delete error: %s \" % str ( error )) raise TaskCatException ( \"Failed to delete one or more files from S3\" ) # build list of files to upload upload_to_s3 = [] for local_file in local_list : upload = False # If file is not present in S3 if local_file not in s3_list . keys (): upload = True # If checksum is different elif local_list [ local_file ][ 1 ] != s3_list [ local_file ]: upload = True if upload : absolute_path = local_list [ local_file ][ 0 ] s3_path = local_file upload_to_s3 . append ([ absolute_path , bucket , s3_path ]) # multithread the uploading of files pool = ThreadPool ( threads ) func = partial ( self . _s3_upload_file , prefix = prefix , s3_client = self . s3_client , acl = acl ) pool . map ( func , upload_to_s3 ) pool . close () pool . join () def _s3_upload_file ( self , paths , prefix , s3_client , acl ): local_filename , bucket , s3_path = paths retry = 0 # backoff and retry while retry < 5 : if self . dry_run : LOG . info ( f \"[DRY_RUN] s3:// { bucket } / { prefix + s3_path } \" , extra = { \"nametag\" : PrintMsg . S3 }, ) break LOG . info ( f \"s3:// { bucket } / { prefix + s3_path } \" , extra = { \"nametag\" : PrintMsg . S3 } ) try : s3_client . upload_file ( local_filename , bucket , prefix + s3_path , ExtraArgs = { \"ACL\" : acl }, Config = TransferConfig ( use_threads = False ), ) break except Exception as e : # pylint: disable=broad-except retry += 1 LOG . error ( \"S3 upload error: %s \" % e ) # give up if we've exhausted retries, or if the error is not-retryable # ie. AccessDenied if retry == 5 or ( isinstance ( e , S3UploadFailedError ) and \"(AccessDenied)\" in str ( e ) ): # pylint: disable=raise-missing-from raise TaskCatException ( \"Failed to upload to S3\" ) time . sleep ( retry * 2 ) Variables LOG Classes S3Sync class S3Sync ( s3_client , bucket , prefix , path , acl = 'private' , dry_run = False ) Class variables exclude_files exclude_path_prefixes exclude_remote_path_prefixes","title":" S3 Sync"},{"location":"reference/_s3_sync.html#module-_s3_sync","text":"None None View Source import fnmatch import hashlib import logging import os import time from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import List from boto3.exceptions import S3UploadFailedError from boto3.s3.transfer import TransferConfig from taskcat._logger import PrintMsg from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class S3Sync : \"\"\"Syncronizes local project files with S3 based on checksums. Excludes hidden files, unpackaged lambda source and taskcat /ci/ files. Uses the Etag as an md5 which introduces the following limitations * Uses undocumented etag algorithm for multipart uploads * Does not work wil files uploaded in the console that use SSE encryption * see https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html for more info Does not support buckets with versioning enabled \"\"\" # TODO: better exclusions that support path wildcards, eg. \"*/.git/*\" exclude_files = [ \".*\" , \"*.md\" ] exclude_path_prefixes = [ \"lambda_functions/source/\" , \"functions/source/\" , \".\" , \"venv/\" , \"taskcat_outputs/\" , ] exclude_remote_path_prefixes : List [ str ] = [] def __init__ ( self , s3_client , bucket , prefix , path , acl = \"private\" , dry_run = False ): \"\"\"Syncronizes local file system with an s3 bucket/prefix \"\"\" if prefix != \"\" and not prefix . endswith ( \"/\" ): prefix = prefix + \"/\" self . s3_client = s3_client self . dry_run = dry_run file_list = self . _get_local_file_list ( path ) s3_file_list = self . _get_s3_file_list ( bucket , prefix ) self . _sync ( file_list , s3_file_list , bucket , prefix , acl = acl ) @staticmethod def _hash_file ( file_path , chunk_size = 8 * 1024 * 1024 ): # This is a bit funky because of the way multipart upload etags are done, they # are a md5 of the md5's from each part with the number of parts appended # credit to hyperknot https://github.com/aws/aws-cli/issues/2585#issue-226758933 md5s = [] with open ( file_path , \"rb\" ) as file_handle : while True : data = file_handle . read ( chunk_size ) if not data : break md5s . append ( hashlib . md5 ( data )) # nosec if len ( md5s ) == 1 : return '\" {} \"' . format ( md5s [ 0 ] . hexdigest ()) digests = b \"\" . join ( m . digest () for m in md5s ) digests_md5 = hashlib . md5 ( digests ) # nosec return '\" {} - {} \"' . format ( digests_md5 . hexdigest (), len ( md5s )) # TODO: refactor def _get_local_file_list ( self , path , include_checksums = True ): # pylint: disable=too-many-locals file_list = {} # get absolute local path path = os . path . abspath ( os . path . expanduser ( path )) # recurse through directories for root , _ , files in os . walk ( path ): relpath = os . path . relpath ( root , path ) + \"/\" exclude_path = False # relative path should be blank if there are no sub directories if relpath == \"./\" : relpath = \"\" # exclude defined paths for prefix in S3Sync . exclude_path_prefixes : if relpath . startswith ( prefix ): exclude_path = True break if not exclude_path : file_list . update ( self . _iterate_files ( files , root , include_checksums , relpath ) ) return file_list def _iterate_files ( self , files , root , include_checksums , relpath ): file_list = {} for file in files : exclude = False # exclude defined filename patterns for pattern in S3Sync . exclude_files : if fnmatch . fnmatch ( file , pattern ): exclude = True break if not exclude : full_path = root + \"/\" + file if include_checksums : # get checksum checksum = self . _hash_file ( full_path ) else : checksum = \"\" file_list [ relpath + file ] = [ full_path , checksum ] return file_list def _get_s3_file_list ( self , bucket , prefix ): objects = {} is_paginated = True continuation_token = None # While there are more results, fetch them from S3 while is_paginated : # if there's no token, this is the initial list_objects call if not continuation_token : resp = self . s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix ) # this is a query to get additional pages, add continuation token to get # next page else : resp = self . s3_client . list_objects_v2 ( Bucket = bucket , Prefix = prefix , ContinuationToken = continuation_token ) if \"Contents\" in resp : for file in resp [ \"Contents\" ]: # strip the prefix from the path relpath = file [ \"Key\" ][ len ( prefix ) :] objects [ relpath ] = file [ \"ETag\" ] if \"NextContinuationToken\" in resp . keys (): continuation_token = resp [ \"NextContinuationToken\" ] # If there's no toke in the response we've fetched all the objects else : is_paginated = False return objects @staticmethod def _exclude_remote ( path ): keep = False for exclude in S3Sync . exclude_remote_path_prefixes : if path . startswith ( exclude ): keep = True break return keep # TODO: refactor def _sync ( # noqa: C901 self , local_list , s3_list , bucket , prefix , acl , threads = 16 ): # pylint: disable=too-many-locals # determine which files to remove from S3 remove_from_s3 = [] for s3_file in s3_list . keys (): if s3_file not in local_list . keys () and not self . _exclude_remote ( s3_file ): if self . dry_run : LOG . info ( f \"[DRY RUN] s3:// { bucket } / { prefix + prefix + s3_file } \" , extra = { \"nametag\" : PrintMsg . S3DELETE }, ) else : LOG . info ( f \"s3:// { bucket } / { prefix + prefix + s3_file } \" , extra = { \"nametag\" : PrintMsg . S3DELETE }, ) remove_from_s3 . append ({ \"Key\" : prefix + s3_file }) # deleting objects, max 1k objects per s3 delete_objects call if not self . dry_run : for objects in [ remove_from_s3 [ i : i + 1000 ] for i in range ( 0 , len ( remove_from_s3 ), 1000 ) ]: response = self . s3_client . delete_objects ( Bucket = bucket , Delete = { \"Objects\" : objects } ) if \"Errors\" in response . keys (): for error in response [ \"Errors\" ]: LOG . error ( \"S3 delete error: %s \" % str ( error )) raise TaskCatException ( \"Failed to delete one or more files from S3\" ) # build list of files to upload upload_to_s3 = [] for local_file in local_list : upload = False # If file is not present in S3 if local_file not in s3_list . keys (): upload = True # If checksum is different elif local_list [ local_file ][ 1 ] != s3_list [ local_file ]: upload = True if upload : absolute_path = local_list [ local_file ][ 0 ] s3_path = local_file upload_to_s3 . append ([ absolute_path , bucket , s3_path ]) # multithread the uploading of files pool = ThreadPool ( threads ) func = partial ( self . _s3_upload_file , prefix = prefix , s3_client = self . s3_client , acl = acl ) pool . map ( func , upload_to_s3 ) pool . close () pool . join () def _s3_upload_file ( self , paths , prefix , s3_client , acl ): local_filename , bucket , s3_path = paths retry = 0 # backoff and retry while retry < 5 : if self . dry_run : LOG . info ( f \"[DRY_RUN] s3:// { bucket } / { prefix + s3_path } \" , extra = { \"nametag\" : PrintMsg . S3 }, ) break LOG . info ( f \"s3:// { bucket } / { prefix + s3_path } \" , extra = { \"nametag\" : PrintMsg . S3 } ) try : s3_client . upload_file ( local_filename , bucket , prefix + s3_path , ExtraArgs = { \"ACL\" : acl }, Config = TransferConfig ( use_threads = False ), ) break except Exception as e : # pylint: disable=broad-except retry += 1 LOG . error ( \"S3 upload error: %s \" % e ) # give up if we've exhausted retries, or if the error is not-retryable # ie. AccessDenied if retry == 5 or ( isinstance ( e , S3UploadFailedError ) and \"(AccessDenied)\" in str ( e ) ): # pylint: disable=raise-missing-from raise TaskCatException ( \"Failed to upload to S3\" ) time . sleep ( retry * 2 )","title":"Module _s3_sync"},{"location":"reference/_s3_sync.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_s3_sync.html#classes","text":"","title":"Classes"},{"location":"reference/_s3_sync.html#s3sync","text":"class S3Sync ( s3_client , bucket , prefix , path , acl = 'private' , dry_run = False )","title":"S3Sync"},{"location":"reference/_s3_sync.html#class-variables","text":"exclude_files exclude_path_prefixes exclude_remote_path_prefixes","title":"Class variables"},{"location":"reference/_template_params.html","text":"Module _template_params None None View Source import logging import random import re import uuid from typing import Set from taskcat . _ common_utils import ( CommonTools , fetch_secretsmanager_parameter_value , fetch_ssm_parameter_value , ) from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) # pylint : disable = too - many - instance - attributes class ParamGen : RE_GETURL = re . compile ( r \"(?<=._url_)(.+)(?=]$)\" , re . IGNORECASE ) RE_COUNT = re . compile ( r \" ( ? ! \\w+ _ ) \\d { 1 , 2 } \", re.IGNORECASE) RE_PWTYPE = re.compile(r\" ( ?<= _ genpass_ )(( \\d+ )( \\w )( \\ ])) \", re.IGNORECASE) RE_GENPW = re.compile(r\" \\ $ \\ [ \\w+ _ genpass? ( \\w ) _ \\d { 1 , 2 } \\w? ] $ \", re.IGNORECASE) RE_GENRANDSTR = re.compile(r\" \\ $ \\ [ taskcat_random - string ] \", re.IGNORECASE) RE_GENNUMB = re.compile(r\" \\ $ \\ [ taskcat_random - numbers ] \", re.IGNORECASE) RE_GENAUTOBUCKET = re.compile(r\" \\ $ \\ [ taskcat_autobucket ] \", re.IGNORECASE) RE_GENAZ = re.compile(r\" \\ $ \\ [ \\w+ _ ge [ nt ] az_\\d ] \", re.IGNORECASE) RE_GENAZ_SINGLE = re.compile( r\" \\ $ \\ [ \\w+ _ ge [ nt ] singleaz_ ( ? P < az_id>\\d+ )] \", re.IGNORECASE ) RE_GENUUID = re.compile(r\" \\ $ \\ [ \\w+ _ gen [ gu ] uid ] \", re.IGNORECASE) RE_QSKEYPAIR = re.compile(r\" \\ $ \\ [ \\w+ _ getkeypair ] \", re.IGNORECASE) RE_QSLICBUCKET = re.compile(r\" \\ $ \\ [ \\w+ _ getlicensebucket ] \", re.IGNORECASE) RE_QSMEDIABUCKET = re.compile(r\" \\ $ \\ [ \\w+ _ getmediabucket ] \", re.IGNORECASE) RE_GETLICCONTENT = re.compile(r\" \\ $ \\ [ \\w+ _ getlicensecontent ]. * $ \", re.IGNORECASE) RE_GETPRESIGNEDURL = re.compile( r\" \\ $ \\ [ \\w+ _ presignedurl ],(. *? ,){ 1 , 2 }. *? $ \", re.IGNORECASE ) RE_GETVAL = re.compile(r\" ( ?<=. _ getval_ )( \\w+ )( ?= ] $ ) \", re.IGNORECASE) RE_CURRENT_REGION = re.compile(r\" \\ $ \\ [ taskcat_current_region ] \", re.IGNORECASE) RE_PROJECT_NAME = re.compile(r\" \\ $ \\ [ taskcat_project_name ] \", re.IGNORECASE) RE_TEST_NAME = re.compile(r\" \\ $ \\ [ taskcat_test_name ] \", re.IGNORECASE) RE_SSM_PARAMETER = re.compile(r\" \\ $ \\ [ taskcat_ssm_ . * ] $ \", re.IGNORECASE) RE_SECRETSMANAGER_PARAMETER = re.compile( r\" \\ $ \\ [ taskcat_secretsmanager_ . * ] $ \", re.IGNORECASE ) def __init__( self, param_dict, bucket_name, region, boto_client, project_name, test_name, az_excludes=None, ): self.regxfind = CommonTools.regxfind self._param_dict = param_dict _missing_params = [] for param_name, param_value in param_dict.items(): if param_value is None: _missing_params.append(param_name) if _missing_params: raise TaskCatException( ( f\" The following parameters have no value whatsoever . \" f\" The CloudFormation stack will fail to launch . \" f\" Please address . str ({ _ missing_params }) \" ) ) self.results = {} self.mutated_params = {} self.param_name = None self.param_value = None self.bucket_name = bucket_name self._boto_client = boto_client self.region = region self.project_name = project_name self.test_name = test_name if not az_excludes: self.az_excludes: Set[str] = set() else: self.az_excludes: Set[str] = az_excludes self.transform_parameter() def transform_parameter(self): # Depreciated placeholders: # - $[taskcat_gets3contents] # - $[taskcat_geturl] for param_name, param_value in self._param_dict.items(): if isinstance(param_value, list): _results_list = [] _nested_param_dict = {} for idx, value in enumerate(param_value): _nested_param_dict[idx] = value nested_pg = ParamGen( _nested_param_dict, self.bucket_name, self.region, self._boto_client, self.project_name, self.test_name, self.az_excludes, ) nested_pg.transform_parameter() for result_value in nested_pg.results.values(): _results_list.append(result_value) self.param_value = _results_list self.results.update({param_name: _results_list}) continue # Setting the instance variables to reflect key/value pair we're working on. self.param_name = param_name self.param_value = param_value # Convert from bytes to string. self.convert_to_str() # $[taskcat_random-numbers] self._regex_replace_param_value(self.RE_GENNUMB, self._gen_rand_num(20)) # $[taskcat_random-string] self._regex_replace_param_value(self.RE_GENRANDSTR, self._gen_rand_str(20)) # $[taskcat_autobucket] self._regex_replace_param_value( self.RE_GENAUTOBUCKET, self._gen_autobucket() ) # $[taskcat_genpass_X] self._gen_password_wrapper(self.RE_GENPW, self.RE_PWTYPE, self.RE_COUNT) # $[taskcat_ge[nt]az_#] self._gen_az_wrapper(self.RE_GENAZ, self.RE_COUNT) # $[taskcat_ge[nt]singleaz_#] self._gen_single_az_wrapper(self.RE_GENAZ_SINGLE) # $[taskcat_getkeypair] self._regex_replace_param_value(self.RE_QSKEYPAIR, \" cikey \") # $[taskcat_getlicensebucket] self._regex_replace_param_value(self.RE_QSLICBUCKET, \" override_this \") # $[taskcat_getmediabucket] self._regex_replace_param_value(self.RE_QSMEDIABUCKET, \" override_this \") # $[taskcat_getlicensecontent] self._get_license_content_wrapper(self.RE_GETLICCONTENT) # $[taskcat_getpresignedurl] self._get_license_content_wrapper(self.RE_GETPRESIGNEDURL) # $[taskcat_getval_X] self._getval_wrapper(self.RE_GETVAL) # $[taskcat_genuuid] self._regex_replace_param_value(self.RE_GENUUID, self._gen_uuid()) # $[taskcat_ssm_X] self._get_ssm_param_value_wrapper(self.RE_SSM_PARAMETER) # $[taskcat_current_region] self._regex_replace_param_value( self.RE_CURRENT_REGION, self._gen_current_region() ) self._regex_replace_param_value( self.RE_PROJECT_NAME, self._get_project_name() ) self._regex_replace_param_value(self.RE_TEST_NAME, self._get_test_name()) self.results.update({self.param_name: self.param_value}) def get_available_azs(self, count): \"\"\" Returns a list of availability zones in a given region . :param count : Minimum number of availability zones needed : return : List of availability zones in a given region \"\"\" ec2_client = self._boto_client(\" ec2 \") available_azs = [] availability_zones = ec2_client.describe_availability_zones( Filters=[{\" Name \": \" state \", \" Values \": [\" available \"]}] ) for az in availability_zones[ # pylint: disable=invalid-name \" AvailabilityZones \" ]: if az[\" ZoneId \"] in self.az_excludes: continue available_azs.append(az[\" ZoneName \"]) if len(available_azs) < count: raise TaskCatException( \" ! Only { 0 } az's are available in {1}\".format( len(available_azs), self.region ) ) azs = \",\".join(available_azs[:count]) return azs def get_single_az(self, az_id): \"\"\" Get a single valid AZ for the region. The number passed indicates the ordinal representing the AZ returned. For instance, in the 'us - east - 1 ' region, providing ' 1 ' as the ID would return 'us - east - 1 a', providing ' 2 ' would return 'us - east - 1 b', etc. In this way it's possible to get availability zones that are guaranteed to be different without knowing their names . :param az_id: 0 - based ordinal of the AZ to get : return : The requested availability zone of the specified region . \"\"\" regional_azs = self.get_available_azs(az_id) return regional_azs.split(\" , \")[-1] def get_content(self, bucket, object_key): \"\"\" Returns the content of an object , given the bucket name and the key of the object :param bucket : Bucket name :param object_key: Key of the object :param object_key: Key of the object : return : Content of the object \"\"\" s3_client = self._boto_client(\" s3 \") try: dict_object = s3_client.get_object(Bucket=bucket, Key=object_key) except Exception: LOG.error( \" Attempted to fetch Bucket : {}, Key : {} \".format(bucket, object_key) ) raise content = dict_object[\" Body \"].read().decode(\" utf - 8 \").strip() return content @staticmethod def genpassword(pass_length, pass_type=None): \"\"\" Returns a password of given length and type . :param pass_length: Length of the desired password :param pass_type: Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric , Example 'vGceIP8EHC' : return : Password of given length and type \"\"\" password = [] numbers = \" 1234567890 \" lowercase = \" abcdefghijklmnopqrstuvwxyz \" uppercase = \" ABCDEFGHIJKLMNOPQRSTUVWXYZ \" specialchars = \" ! # $ & { *: [ = ,] - _ %@+\" # Generates password string with : # lowercase , uppercase and numeric chars if pass_type == \"A\" : # nosec while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) # Generates password string with : # lowercase , uppercase , numbers and special chars elif pass_type == \"S\" : while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) password . append ( random . choice ( specialchars )) else : # If no passtype is defined ( None ) # Defaults to alpha - numeric # Generates password string with : # lowercase , uppercase , numbers and special chars while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) if len ( password ) > pass_length: password = password [ :pass_length ] return \"\" . join ( password ) def convert_to_str ( self ) : \"\"\" Converts a parameter value to string No parameters. Operates on (ClassInstance).param_value \"\"\" if isinstance ( self . param_value , ( int , float , bytes )) : self . param_value = str ( self . param_value ) @staticmethod def _ gen_rand_str ( length ) : random_string_list = [] lowercase = \"abcdefghijklmnopqrstuvwxyz\" while len ( random_string_list ) < length : random_string_list . append ( random . choice ( lowercase )) # nosec return \"\" . join ( random_string_list ) @staticmethod def _ gen_rand_num ( length ) : random_number_list = [] numbers = \"1234567890\" while len ( random_number_list ) < length : random_number_list . append ( random . choice ( numbers )) # nosec return \"\" . join ( random_number_list ) @staticmethod def _ gen_uuid () : return str ( uuid . uuid1 ()) def _ gen_autobucket ( self ) : return self . bucket_name def _ gen_current_region ( self ) : return self . region def _ get_project_name ( self ) : return self . project_name def _ get_test_name ( self ) : return self . test_name def _ gen_password_wrapper ( self , gen_regex , type_regex , count_regex ) : if gen_regex . search ( self . param_value ) : passlen = int ( self . regxfind ( count_regex , self . param_value )) gentype = self . regxfind ( type_regex , self . param_value ) # Additional computation to identify if the gentype is one of the desired # values . Sample gentype values would be '8A]' or '24S]' or '2]' To get # the correct gentype , get 2 nd char from the last and check if its A or S gentype = gentype [ - 2 ] if gentype in ( \"a\" , \"A\" , \"s\" , \"S\" ) : gentype = gentype . upper () else : gentype = None if not gentype : # Set default password type # A value of PrintMsg . DEBUG will generate a simple alpha # aplha numeric password gentype = \"D\" if passlen : param_value = self . genpassword ( passlen , gentype ) self . _ regex_replace_param_value ( gen_regex , param_value ) def _ gen_az_wrapper ( self , genaz_regex , count_regex ) : if genaz_regex . search ( self . param_value ) : numazs = int ( self . regxfind ( count_regex , self . param_value )) if numazs : self . _ regex_replace_param_value ( genaz_regex , self . get_available_azs ( numazs ) ) else : LOG . info ( \"$[taskcat_genaz_(!)]\" ) LOG . info ( \"Number of az's not specified!\" ) LOG . info ( \" - (Defaulting to 1 az)\" ) self . _ regex_replace_param_value ( genaz_regex , self . get_available_azs ( 1 )) def _ gen_single_az_wrapper ( self , genaz_regex ) : if genaz_regex . search ( self . param_value ) : az_id = int ( genaz_regex . search ( self . param_value ). group ( \"az_id\" )) self . _ regex_replace_param_value ( genaz_regex , self . get_single_az ( az_id )) def _ get_license_content_wrapper ( self , license_content_regex ) : if license_content_regex . search ( self . param_value ) : license_str = self . regxfind ( license_content_regex , self . param_value ) license_bucket = license_str . split ( \"/\" )[ 1 ] licensekey = \"/\" . join ( license_str . split ( \"/\" )[ 2 : ]) param_value = self . get_content ( license_bucket , licensekey ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) def _ get_presigned_url_wrapper ( self , presigned_url_regex ) : if presigned_url_regex . search ( self . param_value ) : if len ( self . param_value ) < 2 : LOG . error ( \"Syntax: $[taskcat_presignedurl],bucket,key,OPTIONAL_TIMEOUT\" ) raise TaskCatException ( \"Syntax error when using $[taskcat_getpresignedurl]; Not \" \"enough parameters.\" ) paramsplit = self . regxfind ( presigned_url_regex , self . param_value ). split ( \",\" )[ 1 : ] url_bucket , url_key = paramsplit [ : 2 ] if len ( paramsplit ) == 3 : url_expire_seconds = paramsplit [ 2 ] else : url_expire_seconds = 3600 s3_client = self . _ boto_client ( \"s3\" ) param_value = s3_client . generate_presigned_url ( \"get_object\" , Params = { \"Bucket\" : url_bucket , \"Key\" : url_key }, ExpiresIn = int ( url_expire_seconds ), ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) def _ get_ssm_param_value_wrapper ( self , ssm_param_value_regex ) : if ssm_param_value_regex . search ( self . param_value ) : ssm_value_str = self . regxfind ( ssm_param_value_regex , self . param_value ) param_path = \"_\" . join ( ssm_value_str [:- 1 ]. split ( \"_\" )[ 2 : ]) param_value = fetch_ssm_parameter_value ( self . _ boto_client , param_path ) self . _ regex_replace_param_value ( re . compile ( \"^.*\" ), param_value ) def _ get_secretsmanager_param_value_wrapper ( self , secretsmanager_param_value_regex ) : if secretsmanager_param_value_regex . search ( self . param_value ) : sm_value_str = self . regxfind ( secretsmanager_param_value_regex , self . param_value ) sm_arn = \"_\" . join ( sm_value_str . split ( \"_\" )[ 2 : ]) param_value = fetch_secretsmanager_parameter_value ( self . _ boto_client , sm_arn ) self . _ regex_replace_param_value ( re . compile ( \"^.*\" ), param_value ) def _ getval_wrapper ( self , getval_regex ) : if getval_regex . search ( self . param_value ) : requested_key = self . regxfind ( getval_regex , self . param_value ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), self . mutated_params [ requested_key ] ) def _ regex_replace_param_value ( self , regex_pattern , func_output ) : if self . regxfind ( regex_pattern , self . param_value ) : self . param_value = re . sub ( regex_pattern , str ( func_output ), self . param_value ) self . mutated_params [ self . param_name ] = self . param_value Variables LOG Classes ParamGen class ParamGen ( param_dict , bucket_name , region , boto_client , project_name , test_name , az_excludes = None ) Class variables RE_COUNT RE_CURRENT_REGION RE_GENAUTOBUCKET RE_GENAZ RE_GENAZ_SINGLE RE_GENNUMB RE_GENPW RE_GENRANDSTR RE_GENUUID RE_GETLICCONTENT RE_GETPRESIGNEDURL RE_GETURL RE_GETVAL RE_PROJECT_NAME RE_PWTYPE RE_QSKEYPAIR RE_QSLICBUCKET RE_QSMEDIABUCKET RE_SECRETSMANAGER_PARAMETER RE_SSM_PARAMETER RE_TEST_NAME Static methods genpassword def genpassword ( pass_length , pass_type = None ) Returns a password of given length and type. Parameters: Name Type Description Default pass_length None Length of the desired password None pass_type None Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric, Example 'vGceIP8EHC' None Returns: Type Description None Password of given length and type View Source @staticmethod def genpassword ( pass_length , pass_type = None ) : \"\"\" Returns a password of given length and type. :param pass_length: Length of the desired password :param pass_type: Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric, Example 'vGceIP8EHC' :return: Password of given length and type \"\"\" password = [] numbers = \"1234567890\" lowercase = \"abcdefghijklmnopqrstuvwxyz\" uppercase = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" specialchars = \"!#$&{*:[=,]-_%@+\" # Generates password string with : # lowercase , uppercase and numeric chars if pass_type == \"A\" : # nosec while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) # Generates password string with : # lowercase , uppercase , numbers and special chars elif pass_type == \"S\" : while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) password . append ( random . choice ( specialchars )) else : # If no passtype is defined ( None ) # Defaults to alpha - numeric # Generates password string with : # lowercase , uppercase , numbers and special chars while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) if len ( password ) > pass_length : password = password [ :pass_length ] return \"\" . join ( password ) Methods convert_to_str def convert_to_str ( self ) Converts a parameter value to string No parameters. Operates on (ClassInstance).param_value View Source def convert_to_str ( self ) : \"\"\" Converts a parameter value to string No parameters . Operates on ( ClassInstance ) . param_value \"\"\" if isinstance ( self . param_value , ( int , float , bytes )) : self . param_value = str ( self . param_value ) get_available_azs def get_available_azs ( self , count ) Returns a list of availability zones in a given region. Parameters: Name Type Description Default count None Minimum number of availability zones needed None Returns: Type Description None List of availability zones in a given region View Source def get_available_azs ( self , count ) : \"\"\" Returns a list of availability zones in a given region . : param count: Minimum number of availability zones needed : return : List of availability zones in a given region \"\"\" ec2_client = self . _boto_client ( \"ec2\" ) available_azs = [] availability_zones = ec2_client . describe_availability_zones ( Filters = [{ \"Name\" : \"state\" , \"Values\" : [ \"available\" ]}] ) for az in availability_zones [ # pylint: disable = invalid - name \"AvailabilityZones\" ] : if az [ \"ZoneId\" ] in self . az_excludes: continue available_azs . append ( az [ \"ZoneName\" ]) if len ( available_azs ) < count: raise TaskCatException ( \"!Only {0} az's are available in {1}\" . format ( len ( available_azs ), self . region ) ) azs = \",\" . join ( available_azs [ : count ]) return azs get_content def get_content ( self , bucket , object_key ) Returns the content of an object, given the bucket name and the key of the object Parameters: Name Type Description Default bucket None Bucket name None object_key None Key of the object None object_key None Key of the object None Returns: Type Description None Content of the object View Source def get_content ( self , bucket , object_key ) : \"\"\" Returns the content of an object , given the bucket name and the key of the object : param bucket : Bucket name : param object_key : Key of the object : param object_key : Key of the object : return : Content of the object \"\"\" s3_client = self . _boto_client ( \" s3 \" ) try : dict_object = s3_client . get_object ( Bucket = bucket , Key = object_key ) except Exception : LOG . error ( \" Attempted to fetch Bucket: {}, Key: {} \" . format ( bucket , object_key ) ) raise content = dict_object [ \" Body \" ]. read () . decode ( \" utf-8 \" ) . strip () return content get_single_az def get_single_az ( self , az_id ) Get a single valid AZ for the region. The number passed indicates the ordinal representing the AZ returned. For instance, in the 'us-east-1' region, providing '1' as the ID would return 'us-east-1a', providing '2' would return 'us-east-1b', etc. In this way it's possible to get availability zones that are guaranteed to be different without knowing their names. Parameters: Name Type Description Default az_id None 0-based ordinal of the AZ to get None Returns: Type Description None The requested availability zone of the specified region. View Source def get_single_az ( self , az_id ) : \"\"\" Get a single valid AZ for the region . The number passed indicates the ordinal representing the AZ returned . For instance , in the ' us - east - 1 ' region , providing '1' as the ID would return ' us - east - 1 a ', providing '2' would return ' us - east - 1 b ', etc . In this way it ' s possible to get availability zones that are guaranteed to be different without knowing their names . : param az_id: 0 - based ordinal of the AZ to get : return : The requested availability zone of the specified region . \"\"\" regional_azs = self . get_available_azs ( az_id ) return regional_azs . split ( \",\" )[ - 1 ] transform_parameter def transform_parameter ( self ) View Source def transform_parameter ( self ) : # Depreciated placeholders : # - $ [ taskcat_gets3contents ] # - $ [ taskcat_geturl ] for param_name , param_value in self . _param_dict . items () : if isinstance ( param_value , list ) : _results_list = [] _nested_param_dict = {} for idx , value in enumerate ( param_value ) : _nested_param_dict [ idx ] = value nested_pg = ParamGen ( _nested_param_dict , self . bucket_name , self . region , self . _boto_client , self . project_name , self . test_name , self . az_excludes , ) nested_pg . transform_parameter () for result_value in nested_pg . results . values () : _results_list . append ( result_value ) self . param_value = _results_list self . results . update ( { param_name : _results_list } ) continue # Setting the instance variables to reflect key / value pair we ' re working on . self . param_name = param_name self . param_value = param_value # Convert from bytes to string . self . convert_to_str () # $ [ taskcat_random-numbers ] self . _regex_replace_param_value ( self . RE_GENNUMB , self . _gen_rand_num ( 20 )) # $ [ taskcat_random-string ] self . _regex_replace_param_value ( self . RE_GENRANDSTR , self . _gen_rand_str ( 20 )) # $ [ taskcat_autobucket ] self . _regex_replace_param_value ( self . RE_GENAUTOBUCKET , self . _gen_autobucket () ) # $ [ taskcat_genpass_X ] self . _gen_password_wrapper ( self . RE_GENPW , self . RE_PWTYPE , self . RE_COUNT ) # $ [ taskcat_ge[nt ] az_ #] self . _gen_az_wrapper ( self . RE_GENAZ , self . RE_COUNT ) # $ [ taskcat_ge[nt ] singleaz_ #] self . _gen_single_az_wrapper ( self . RE_GENAZ_SINGLE ) # $ [ taskcat_getkeypair ] self . _regex_replace_param_value ( self . RE_QSKEYPAIR , \"cikey\" ) # $ [ taskcat_getlicensebucket ] self . _regex_replace_param_value ( self . RE_QSLICBUCKET , \"override_this\" ) # $ [ taskcat_getmediabucket ] self . _regex_replace_param_value ( self . RE_QSMEDIABUCKET , \"override_this\" ) # $ [ taskcat_getlicensecontent ] self . _get_license_content_wrapper ( self . RE_GETLICCONTENT ) # $ [ taskcat_getpresignedurl ] self . _get_license_content_wrapper ( self . RE_GETPRESIGNEDURL ) # $ [ taskcat_getval_X ] self . _getval_wrapper ( self . RE_GETVAL ) # $ [ taskcat_genuuid ] self . _regex_replace_param_value ( self . RE_GENUUID , self . _gen_uuid ()) # $ [ taskcat_ssm_X ] self . _get_ssm_param_value_wrapper ( self . RE_SSM_PARAMETER ) # $ [ taskcat_current_region ] self . _regex_replace_param_value ( self . RE_CURRENT_REGION , self . _gen_current_region () ) self . _regex_replace_param_value ( self . RE_PROJECT_NAME , self . _get_project_name () ) self . _regex_replace_param_value ( self . RE_TEST_NAME , self . _get_test_name ()) self . results . update ( { self . param_name : self . param_value } )","title":" Template Params"},{"location":"reference/_template_params.html#module-_template_params","text":"None None View Source import logging import random import re import uuid from typing import Set from taskcat . _ common_utils import ( CommonTools , fetch_secretsmanager_parameter_value , fetch_ssm_parameter_value , ) from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) # pylint : disable = too - many - instance - attributes class ParamGen : RE_GETURL = re . compile ( r \"(?<=._url_)(.+)(?=]$)\" , re . IGNORECASE ) RE_COUNT = re . compile ( r \" ( ? ! \\w+ _ ) \\d { 1 , 2 } \", re.IGNORECASE) RE_PWTYPE = re.compile(r\" ( ?<= _ genpass_ )(( \\d+ )( \\w )( \\ ])) \", re.IGNORECASE) RE_GENPW = re.compile(r\" \\ $ \\ [ \\w+ _ genpass? ( \\w ) _ \\d { 1 , 2 } \\w? ] $ \", re.IGNORECASE) RE_GENRANDSTR = re.compile(r\" \\ $ \\ [ taskcat_random - string ] \", re.IGNORECASE) RE_GENNUMB = re.compile(r\" \\ $ \\ [ taskcat_random - numbers ] \", re.IGNORECASE) RE_GENAUTOBUCKET = re.compile(r\" \\ $ \\ [ taskcat_autobucket ] \", re.IGNORECASE) RE_GENAZ = re.compile(r\" \\ $ \\ [ \\w+ _ ge [ nt ] az_\\d ] \", re.IGNORECASE) RE_GENAZ_SINGLE = re.compile( r\" \\ $ \\ [ \\w+ _ ge [ nt ] singleaz_ ( ? P < az_id>\\d+ )] \", re.IGNORECASE ) RE_GENUUID = re.compile(r\" \\ $ \\ [ \\w+ _ gen [ gu ] uid ] \", re.IGNORECASE) RE_QSKEYPAIR = re.compile(r\" \\ $ \\ [ \\w+ _ getkeypair ] \", re.IGNORECASE) RE_QSLICBUCKET = re.compile(r\" \\ $ \\ [ \\w+ _ getlicensebucket ] \", re.IGNORECASE) RE_QSMEDIABUCKET = re.compile(r\" \\ $ \\ [ \\w+ _ getmediabucket ] \", re.IGNORECASE) RE_GETLICCONTENT = re.compile(r\" \\ $ \\ [ \\w+ _ getlicensecontent ]. * $ \", re.IGNORECASE) RE_GETPRESIGNEDURL = re.compile( r\" \\ $ \\ [ \\w+ _ presignedurl ],(. *? ,){ 1 , 2 }. *? $ \", re.IGNORECASE ) RE_GETVAL = re.compile(r\" ( ?<=. _ getval_ )( \\w+ )( ?= ] $ ) \", re.IGNORECASE) RE_CURRENT_REGION = re.compile(r\" \\ $ \\ [ taskcat_current_region ] \", re.IGNORECASE) RE_PROJECT_NAME = re.compile(r\" \\ $ \\ [ taskcat_project_name ] \", re.IGNORECASE) RE_TEST_NAME = re.compile(r\" \\ $ \\ [ taskcat_test_name ] \", re.IGNORECASE) RE_SSM_PARAMETER = re.compile(r\" \\ $ \\ [ taskcat_ssm_ . * ] $ \", re.IGNORECASE) RE_SECRETSMANAGER_PARAMETER = re.compile( r\" \\ $ \\ [ taskcat_secretsmanager_ . * ] $ \", re.IGNORECASE ) def __init__( self, param_dict, bucket_name, region, boto_client, project_name, test_name, az_excludes=None, ): self.regxfind = CommonTools.regxfind self._param_dict = param_dict _missing_params = [] for param_name, param_value in param_dict.items(): if param_value is None: _missing_params.append(param_name) if _missing_params: raise TaskCatException( ( f\" The following parameters have no value whatsoever . \" f\" The CloudFormation stack will fail to launch . \" f\" Please address . str ({ _ missing_params }) \" ) ) self.results = {} self.mutated_params = {} self.param_name = None self.param_value = None self.bucket_name = bucket_name self._boto_client = boto_client self.region = region self.project_name = project_name self.test_name = test_name if not az_excludes: self.az_excludes: Set[str] = set() else: self.az_excludes: Set[str] = az_excludes self.transform_parameter() def transform_parameter(self): # Depreciated placeholders: # - $[taskcat_gets3contents] # - $[taskcat_geturl] for param_name, param_value in self._param_dict.items(): if isinstance(param_value, list): _results_list = [] _nested_param_dict = {} for idx, value in enumerate(param_value): _nested_param_dict[idx] = value nested_pg = ParamGen( _nested_param_dict, self.bucket_name, self.region, self._boto_client, self.project_name, self.test_name, self.az_excludes, ) nested_pg.transform_parameter() for result_value in nested_pg.results.values(): _results_list.append(result_value) self.param_value = _results_list self.results.update({param_name: _results_list}) continue # Setting the instance variables to reflect key/value pair we're working on. self.param_name = param_name self.param_value = param_value # Convert from bytes to string. self.convert_to_str() # $[taskcat_random-numbers] self._regex_replace_param_value(self.RE_GENNUMB, self._gen_rand_num(20)) # $[taskcat_random-string] self._regex_replace_param_value(self.RE_GENRANDSTR, self._gen_rand_str(20)) # $[taskcat_autobucket] self._regex_replace_param_value( self.RE_GENAUTOBUCKET, self._gen_autobucket() ) # $[taskcat_genpass_X] self._gen_password_wrapper(self.RE_GENPW, self.RE_PWTYPE, self.RE_COUNT) # $[taskcat_ge[nt]az_#] self._gen_az_wrapper(self.RE_GENAZ, self.RE_COUNT) # $[taskcat_ge[nt]singleaz_#] self._gen_single_az_wrapper(self.RE_GENAZ_SINGLE) # $[taskcat_getkeypair] self._regex_replace_param_value(self.RE_QSKEYPAIR, \" cikey \") # $[taskcat_getlicensebucket] self._regex_replace_param_value(self.RE_QSLICBUCKET, \" override_this \") # $[taskcat_getmediabucket] self._regex_replace_param_value(self.RE_QSMEDIABUCKET, \" override_this \") # $[taskcat_getlicensecontent] self._get_license_content_wrapper(self.RE_GETLICCONTENT) # $[taskcat_getpresignedurl] self._get_license_content_wrapper(self.RE_GETPRESIGNEDURL) # $[taskcat_getval_X] self._getval_wrapper(self.RE_GETVAL) # $[taskcat_genuuid] self._regex_replace_param_value(self.RE_GENUUID, self._gen_uuid()) # $[taskcat_ssm_X] self._get_ssm_param_value_wrapper(self.RE_SSM_PARAMETER) # $[taskcat_current_region] self._regex_replace_param_value( self.RE_CURRENT_REGION, self._gen_current_region() ) self._regex_replace_param_value( self.RE_PROJECT_NAME, self._get_project_name() ) self._regex_replace_param_value(self.RE_TEST_NAME, self._get_test_name()) self.results.update({self.param_name: self.param_value}) def get_available_azs(self, count): \"\"\" Returns a list of availability zones in a given region . :param count : Minimum number of availability zones needed : return : List of availability zones in a given region \"\"\" ec2_client = self._boto_client(\" ec2 \") available_azs = [] availability_zones = ec2_client.describe_availability_zones( Filters=[{\" Name \": \" state \", \" Values \": [\" available \"]}] ) for az in availability_zones[ # pylint: disable=invalid-name \" AvailabilityZones \" ]: if az[\" ZoneId \"] in self.az_excludes: continue available_azs.append(az[\" ZoneName \"]) if len(available_azs) < count: raise TaskCatException( \" ! Only { 0 } az's are available in {1}\".format( len(available_azs), self.region ) ) azs = \",\".join(available_azs[:count]) return azs def get_single_az(self, az_id): \"\"\" Get a single valid AZ for the region. The number passed indicates the ordinal representing the AZ returned. For instance, in the 'us - east - 1 ' region, providing ' 1 ' as the ID would return 'us - east - 1 a', providing ' 2 ' would return 'us - east - 1 b', etc. In this way it's possible to get availability zones that are guaranteed to be different without knowing their names . :param az_id: 0 - based ordinal of the AZ to get : return : The requested availability zone of the specified region . \"\"\" regional_azs = self.get_available_azs(az_id) return regional_azs.split(\" , \")[-1] def get_content(self, bucket, object_key): \"\"\" Returns the content of an object , given the bucket name and the key of the object :param bucket : Bucket name :param object_key: Key of the object :param object_key: Key of the object : return : Content of the object \"\"\" s3_client = self._boto_client(\" s3 \") try: dict_object = s3_client.get_object(Bucket=bucket, Key=object_key) except Exception: LOG.error( \" Attempted to fetch Bucket : {}, Key : {} \".format(bucket, object_key) ) raise content = dict_object[\" Body \"].read().decode(\" utf - 8 \").strip() return content @staticmethod def genpassword(pass_length, pass_type=None): \"\"\" Returns a password of given length and type . :param pass_length: Length of the desired password :param pass_type: Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric , Example 'vGceIP8EHC' : return : Password of given length and type \"\"\" password = [] numbers = \" 1234567890 \" lowercase = \" abcdefghijklmnopqrstuvwxyz \" uppercase = \" ABCDEFGHIJKLMNOPQRSTUVWXYZ \" specialchars = \" ! # $ & { *: [ = ,] - _ %@+\" # Generates password string with : # lowercase , uppercase and numeric chars if pass_type == \"A\" : # nosec while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) # Generates password string with : # lowercase , uppercase , numbers and special chars elif pass_type == \"S\" : while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) password . append ( random . choice ( specialchars )) else : # If no passtype is defined ( None ) # Defaults to alpha - numeric # Generates password string with : # lowercase , uppercase , numbers and special chars while len ( password ) < pass_length: password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) if len ( password ) > pass_length: password = password [ :pass_length ] return \"\" . join ( password ) def convert_to_str ( self ) : \"\"\" Converts a parameter value to string No parameters. Operates on (ClassInstance).param_value \"\"\" if isinstance ( self . param_value , ( int , float , bytes )) : self . param_value = str ( self . param_value ) @staticmethod def _ gen_rand_str ( length ) : random_string_list = [] lowercase = \"abcdefghijklmnopqrstuvwxyz\" while len ( random_string_list ) < length : random_string_list . append ( random . choice ( lowercase )) # nosec return \"\" . join ( random_string_list ) @staticmethod def _ gen_rand_num ( length ) : random_number_list = [] numbers = \"1234567890\" while len ( random_number_list ) < length : random_number_list . append ( random . choice ( numbers )) # nosec return \"\" . join ( random_number_list ) @staticmethod def _ gen_uuid () : return str ( uuid . uuid1 ()) def _ gen_autobucket ( self ) : return self . bucket_name def _ gen_current_region ( self ) : return self . region def _ get_project_name ( self ) : return self . project_name def _ get_test_name ( self ) : return self . test_name def _ gen_password_wrapper ( self , gen_regex , type_regex , count_regex ) : if gen_regex . search ( self . param_value ) : passlen = int ( self . regxfind ( count_regex , self . param_value )) gentype = self . regxfind ( type_regex , self . param_value ) # Additional computation to identify if the gentype is one of the desired # values . Sample gentype values would be '8A]' or '24S]' or '2]' To get # the correct gentype , get 2 nd char from the last and check if its A or S gentype = gentype [ - 2 ] if gentype in ( \"a\" , \"A\" , \"s\" , \"S\" ) : gentype = gentype . upper () else : gentype = None if not gentype : # Set default password type # A value of PrintMsg . DEBUG will generate a simple alpha # aplha numeric password gentype = \"D\" if passlen : param_value = self . genpassword ( passlen , gentype ) self . _ regex_replace_param_value ( gen_regex , param_value ) def _ gen_az_wrapper ( self , genaz_regex , count_regex ) : if genaz_regex . search ( self . param_value ) : numazs = int ( self . regxfind ( count_regex , self . param_value )) if numazs : self . _ regex_replace_param_value ( genaz_regex , self . get_available_azs ( numazs ) ) else : LOG . info ( \"$[taskcat_genaz_(!)]\" ) LOG . info ( \"Number of az's not specified!\" ) LOG . info ( \" - (Defaulting to 1 az)\" ) self . _ regex_replace_param_value ( genaz_regex , self . get_available_azs ( 1 )) def _ gen_single_az_wrapper ( self , genaz_regex ) : if genaz_regex . search ( self . param_value ) : az_id = int ( genaz_regex . search ( self . param_value ). group ( \"az_id\" )) self . _ regex_replace_param_value ( genaz_regex , self . get_single_az ( az_id )) def _ get_license_content_wrapper ( self , license_content_regex ) : if license_content_regex . search ( self . param_value ) : license_str = self . regxfind ( license_content_regex , self . param_value ) license_bucket = license_str . split ( \"/\" )[ 1 ] licensekey = \"/\" . join ( license_str . split ( \"/\" )[ 2 : ]) param_value = self . get_content ( license_bucket , licensekey ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) def _ get_presigned_url_wrapper ( self , presigned_url_regex ) : if presigned_url_regex . search ( self . param_value ) : if len ( self . param_value ) < 2 : LOG . error ( \"Syntax: $[taskcat_presignedurl],bucket,key,OPTIONAL_TIMEOUT\" ) raise TaskCatException ( \"Syntax error when using $[taskcat_getpresignedurl]; Not \" \"enough parameters.\" ) paramsplit = self . regxfind ( presigned_url_regex , self . param_value ). split ( \",\" )[ 1 : ] url_bucket , url_key = paramsplit [ : 2 ] if len ( paramsplit ) == 3 : url_expire_seconds = paramsplit [ 2 ] else : url_expire_seconds = 3600 s3_client = self . _ boto_client ( \"s3\" ) param_value = s3_client . generate_presigned_url ( \"get_object\" , Params = { \"Bucket\" : url_bucket , \"Key\" : url_key }, ExpiresIn = int ( url_expire_seconds ), ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), param_value ) def _ get_ssm_param_value_wrapper ( self , ssm_param_value_regex ) : if ssm_param_value_regex . search ( self . param_value ) : ssm_value_str = self . regxfind ( ssm_param_value_regex , self . param_value ) param_path = \"_\" . join ( ssm_value_str [:- 1 ]. split ( \"_\" )[ 2 : ]) param_value = fetch_ssm_parameter_value ( self . _ boto_client , param_path ) self . _ regex_replace_param_value ( re . compile ( \"^.*\" ), param_value ) def _ get_secretsmanager_param_value_wrapper ( self , secretsmanager_param_value_regex ) : if secretsmanager_param_value_regex . search ( self . param_value ) : sm_value_str = self . regxfind ( secretsmanager_param_value_regex , self . param_value ) sm_arn = \"_\" . join ( sm_value_str . split ( \"_\" )[ 2 : ]) param_value = fetch_secretsmanager_parameter_value ( self . _ boto_client , sm_arn ) self . _ regex_replace_param_value ( re . compile ( \"^.*\" ), param_value ) def _ getval_wrapper ( self , getval_regex ) : if getval_regex . search ( self . param_value ) : requested_key = self . regxfind ( getval_regex , self . param_value ) self . _ regex_replace_param_value ( re . compile ( \"^.*$\" ), self . mutated_params [ requested_key ] ) def _ regex_replace_param_value ( self , regex_pattern , func_output ) : if self . regxfind ( regex_pattern , self . param_value ) : self . param_value = re . sub ( regex_pattern , str ( func_output ), self . param_value ) self . mutated_params [ self . param_name ] = self . param_value","title":"Module _template_params"},{"location":"reference/_template_params.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_template_params.html#classes","text":"","title":"Classes"},{"location":"reference/_template_params.html#paramgen","text":"class ParamGen ( param_dict , bucket_name , region , boto_client , project_name , test_name , az_excludes = None )","title":"ParamGen"},{"location":"reference/_template_params.html#class-variables","text":"RE_COUNT RE_CURRENT_REGION RE_GENAUTOBUCKET RE_GENAZ RE_GENAZ_SINGLE RE_GENNUMB RE_GENPW RE_GENRANDSTR RE_GENUUID RE_GETLICCONTENT RE_GETPRESIGNEDURL RE_GETURL RE_GETVAL RE_PROJECT_NAME RE_PWTYPE RE_QSKEYPAIR RE_QSLICBUCKET RE_QSMEDIABUCKET RE_SECRETSMANAGER_PARAMETER RE_SSM_PARAMETER RE_TEST_NAME","title":"Class variables"},{"location":"reference/_template_params.html#static-methods","text":"","title":"Static methods"},{"location":"reference/_template_params.html#genpassword","text":"def genpassword ( pass_length , pass_type = None ) Returns a password of given length and type. Parameters: Name Type Description Default pass_length None Length of the desired password None pass_type None Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric, Example 'vGceIP8EHC' None Returns: Type Description None Password of given length and type View Source @staticmethod def genpassword ( pass_length , pass_type = None ) : \"\"\" Returns a password of given length and type. :param pass_length: Length of the desired password :param pass_type: Type of the desired password - String only OR Alphanumeric * A = AlphaNumeric, Example 'vGceIP8EHC' :return: Password of given length and type \"\"\" password = [] numbers = \"1234567890\" lowercase = \"abcdefghijklmnopqrstuvwxyz\" uppercase = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" specialchars = \"!#$&{*:[=,]-_%@+\" # Generates password string with : # lowercase , uppercase and numeric chars if pass_type == \"A\" : # nosec while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) # Generates password string with : # lowercase , uppercase , numbers and special chars elif pass_type == \"S\" : while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) password . append ( random . choice ( specialchars )) else : # If no passtype is defined ( None ) # Defaults to alpha - numeric # Generates password string with : # lowercase , uppercase , numbers and special chars while len ( password ) < pass_length : password . append ( random . choice ( lowercase )) password . append ( random . choice ( uppercase )) password . append ( random . choice ( numbers )) if len ( password ) > pass_length : password = password [ :pass_length ] return \"\" . join ( password )","title":"genpassword"},{"location":"reference/_template_params.html#methods","text":"","title":"Methods"},{"location":"reference/_template_params.html#convert_to_str","text":"def convert_to_str ( self ) Converts a parameter value to string No parameters. Operates on (ClassInstance).param_value View Source def convert_to_str ( self ) : \"\"\" Converts a parameter value to string No parameters . Operates on ( ClassInstance ) . param_value \"\"\" if isinstance ( self . param_value , ( int , float , bytes )) : self . param_value = str ( self . param_value )","title":"convert_to_str"},{"location":"reference/_template_params.html#get_available_azs","text":"def get_available_azs ( self , count ) Returns a list of availability zones in a given region. Parameters: Name Type Description Default count None Minimum number of availability zones needed None Returns: Type Description None List of availability zones in a given region View Source def get_available_azs ( self , count ) : \"\"\" Returns a list of availability zones in a given region . : param count: Minimum number of availability zones needed : return : List of availability zones in a given region \"\"\" ec2_client = self . _boto_client ( \"ec2\" ) available_azs = [] availability_zones = ec2_client . describe_availability_zones ( Filters = [{ \"Name\" : \"state\" , \"Values\" : [ \"available\" ]}] ) for az in availability_zones [ # pylint: disable = invalid - name \"AvailabilityZones\" ] : if az [ \"ZoneId\" ] in self . az_excludes: continue available_azs . append ( az [ \"ZoneName\" ]) if len ( available_azs ) < count: raise TaskCatException ( \"!Only {0} az's are available in {1}\" . format ( len ( available_azs ), self . region ) ) azs = \",\" . join ( available_azs [ : count ]) return azs","title":"get_available_azs"},{"location":"reference/_template_params.html#get_content","text":"def get_content ( self , bucket , object_key ) Returns the content of an object, given the bucket name and the key of the object Parameters: Name Type Description Default bucket None Bucket name None object_key None Key of the object None object_key None Key of the object None Returns: Type Description None Content of the object View Source def get_content ( self , bucket , object_key ) : \"\"\" Returns the content of an object , given the bucket name and the key of the object : param bucket : Bucket name : param object_key : Key of the object : param object_key : Key of the object : return : Content of the object \"\"\" s3_client = self . _boto_client ( \" s3 \" ) try : dict_object = s3_client . get_object ( Bucket = bucket , Key = object_key ) except Exception : LOG . error ( \" Attempted to fetch Bucket: {}, Key: {} \" . format ( bucket , object_key ) ) raise content = dict_object [ \" Body \" ]. read () . decode ( \" utf-8 \" ) . strip () return content","title":"get_content"},{"location":"reference/_template_params.html#get_single_az","text":"def get_single_az ( self , az_id ) Get a single valid AZ for the region. The number passed indicates the ordinal representing the AZ returned. For instance, in the 'us-east-1' region, providing '1' as the ID would return 'us-east-1a', providing '2' would return 'us-east-1b', etc. In this way it's possible to get availability zones that are guaranteed to be different without knowing their names. Parameters: Name Type Description Default az_id None 0-based ordinal of the AZ to get None Returns: Type Description None The requested availability zone of the specified region. View Source def get_single_az ( self , az_id ) : \"\"\" Get a single valid AZ for the region . The number passed indicates the ordinal representing the AZ returned . For instance , in the ' us - east - 1 ' region , providing '1' as the ID would return ' us - east - 1 a ', providing '2' would return ' us - east - 1 b ', etc . In this way it ' s possible to get availability zones that are guaranteed to be different without knowing their names . : param az_id: 0 - based ordinal of the AZ to get : return : The requested availability zone of the specified region . \"\"\" regional_azs = self . get_available_azs ( az_id ) return regional_azs . split ( \",\" )[ - 1 ]","title":"get_single_az"},{"location":"reference/_template_params.html#transform_parameter","text":"def transform_parameter ( self ) View Source def transform_parameter ( self ) : # Depreciated placeholders : # - $ [ taskcat_gets3contents ] # - $ [ taskcat_geturl ] for param_name , param_value in self . _param_dict . items () : if isinstance ( param_value , list ) : _results_list = [] _nested_param_dict = {} for idx , value in enumerate ( param_value ) : _nested_param_dict [ idx ] = value nested_pg = ParamGen ( _nested_param_dict , self . bucket_name , self . region , self . _boto_client , self . project_name , self . test_name , self . az_excludes , ) nested_pg . transform_parameter () for result_value in nested_pg . results . values () : _results_list . append ( result_value ) self . param_value = _results_list self . results . update ( { param_name : _results_list } ) continue # Setting the instance variables to reflect key / value pair we ' re working on . self . param_name = param_name self . param_value = param_value # Convert from bytes to string . self . convert_to_str () # $ [ taskcat_random-numbers ] self . _regex_replace_param_value ( self . RE_GENNUMB , self . _gen_rand_num ( 20 )) # $ [ taskcat_random-string ] self . _regex_replace_param_value ( self . RE_GENRANDSTR , self . _gen_rand_str ( 20 )) # $ [ taskcat_autobucket ] self . _regex_replace_param_value ( self . RE_GENAUTOBUCKET , self . _gen_autobucket () ) # $ [ taskcat_genpass_X ] self . _gen_password_wrapper ( self . RE_GENPW , self . RE_PWTYPE , self . RE_COUNT ) # $ [ taskcat_ge[nt ] az_ #] self . _gen_az_wrapper ( self . RE_GENAZ , self . RE_COUNT ) # $ [ taskcat_ge[nt ] singleaz_ #] self . _gen_single_az_wrapper ( self . RE_GENAZ_SINGLE ) # $ [ taskcat_getkeypair ] self . _regex_replace_param_value ( self . RE_QSKEYPAIR , \"cikey\" ) # $ [ taskcat_getlicensebucket ] self . _regex_replace_param_value ( self . RE_QSLICBUCKET , \"override_this\" ) # $ [ taskcat_getmediabucket ] self . _regex_replace_param_value ( self . RE_QSMEDIABUCKET , \"override_this\" ) # $ [ taskcat_getlicensecontent ] self . _get_license_content_wrapper ( self . RE_GETLICCONTENT ) # $ [ taskcat_getpresignedurl ] self . _get_license_content_wrapper ( self . RE_GETPRESIGNEDURL ) # $ [ taskcat_getval_X ] self . _getval_wrapper ( self . RE_GETVAL ) # $ [ taskcat_genuuid ] self . _regex_replace_param_value ( self . RE_GENUUID , self . _gen_uuid ()) # $ [ taskcat_ssm_X ] self . _get_ssm_param_value_wrapper ( self . RE_SSM_PARAMETER ) # $ [ taskcat_current_region ] self . _regex_replace_param_value ( self . RE_CURRENT_REGION , self . _gen_current_region () ) self . _regex_replace_param_value ( self . RE_PROJECT_NAME , self . _get_project_name () ) self . _regex_replace_param_value ( self . RE_TEST_NAME , self . _get_test_name ()) self . results . update ( { self . param_name : self . param_value } )","title":"transform_parameter"},{"location":"reference/_tui.html","text":"Module _tui None None View Source import logging import time from reprint import output from taskcat._cfn.threaded import Stacker as TaskcatStacker from taskcat._logger import PrintMsg LOG = logging . getLogger ( __name__ ) class TerminalPrinter : def __init__ ( self , minimalist ): self . minimalist = minimalist if not minimalist : self . _buffer_type = \"list\" self . buffer = self . _add_buffer () def _add_buffer ( self ): with output ( output_type = self . _buffer_type ) as output_buffer : return output_buffer def report_test_progress ( self , stacker : TaskcatStacker , poll_interval = 10 ): if self . minimalist : self . minimalist_progress ( stacker , poll_interval ) return _status_dict = stacker . status () while self . _is_test_in_progress ( _status_dict ): for stack in stacker . stacks : self . _print_stack_tree ( stack , buffer = self . buffer ) time . sleep ( poll_interval ) self . buffer . clear () _status_dict = stacker . status () self . _display_final_status ( stacker ) def minimalist_progress ( self , stacker : TaskcatStacker , poll_interval ): _status_dict = stacker . status () history : dict = {} while self . _is_test_in_progress ( _status_dict ): _status_dict = stacker . status () for stack in stacker . stacks : self . _print_tree_minimal ( stack , history ) time . sleep ( poll_interval ) @staticmethod def _print_tree_minimal ( stack , history ): if stack . id not in history : history [ stack . id ] = \"\" if history [ stack . id ] != stack . status : history [ stack . id ] = stack . status msg = f \" { stack . test_name } { stack . region_name } { stack . status } \" if \"FAILED\" in stack . status : LOG . error ( msg ) for event in stack . error_events ( refresh = True ): LOG . error ( f \" { event . logical_id } { event . status_reason } \" ) else : LOG . info ( msg ) @staticmethod def _print_stack_tree ( stack , buffer ): padding_1 = \" \" buffer . append ( \" {}{} stack {} {} \" . format ( padding_1 , \" \\u250f \" , \" \\u24c2 \" , stack . name ) ) if stack . descendants (): for nested_stack in stack . descendants (): buffer . append ( \" {}{} stack {} {} \" . format ( padding_1 , \" \\u2523 \" , \" \\u24c3 \" , nested_stack . name ) ) buffer . append ( \" {}{} region: {} \" . format ( padding_1 , \" \\u2523 \" , stack . region_name )) buffer . append ( \" {}{} status: {}{}{} \" . format ( padding_1 , \" \\u2517 \" , PrintMsg . white , stack . status , PrintMsg . rst_color ) ) @staticmethod def _display_final_status ( stacker : TaskcatStacker ): for final_stack in stacker . stacks : LOG . info ( \" {} stack {} {} \" . format ( \" \\u250f \" , \" \\u24c2 \" , final_stack . name )) if final_stack . descendants (): for nested_stack in final_stack . descendants (): LOG . info ( \" {} stack {} {} \" . format ( \" \\u2523 \" , \" \\u24c3 \" , nested_stack . name ) ) LOG . info ( \" {} region: {} \" . format ( \" \\u2523 \" , final_stack . region_name )) LOG . info ( \" {} status: {}{} {} \" . format ( \" \\u2517 \" , PrintMsg . white , final_stack . status , PrintMsg . rst_color ) ) @staticmethod def _is_test_in_progress ( status_dict , status_condition = \"IN_PROGRESS\" ): return bool ( len ( status_dict [ status_condition ]) > 0 ) Variables LOG Classes TerminalPrinter class TerminalPrinter ( minimalist ) Methods minimalist_progress def minimalist_progress ( self , stacker : taskcat . _cfn . threaded . Stacker , poll_interval ) View Source def minimalist_progress ( self , stacker : TaskcatStacker , poll_interval ) : _status_dict = stacker . status () history : dict = {} while self . _is_test_in_progress ( _status_dict ) : _status_dict = stacker . status () for stack in stacker . stacks : self . _print_tree_minimal ( stack , history ) time . sleep ( poll_interval ) report_test_progress def report_test_progress ( self , stacker : taskcat . _cfn . threaded . Stacker , poll_interval = 10 ) View Source def report_test_progress ( self , stacker : TaskcatStacker , poll_interval = 10 ) : if self . minimalist : self . minimalist_progress ( stacker , poll_interval ) return _status_dict = stacker . status () while self . _is_test_in_progress ( _status_dict ) : for stack in stacker . stacks : self . _print_stack_tree ( stack , buffer = self . buffer ) time . sleep ( poll_interval ) self . buffer . clear () _status_dict = stacker . status () self . _display_final_status ( stacker )","title":" Tui"},{"location":"reference/_tui.html#module-_tui","text":"None None View Source import logging import time from reprint import output from taskcat._cfn.threaded import Stacker as TaskcatStacker from taskcat._logger import PrintMsg LOG = logging . getLogger ( __name__ ) class TerminalPrinter : def __init__ ( self , minimalist ): self . minimalist = minimalist if not minimalist : self . _buffer_type = \"list\" self . buffer = self . _add_buffer () def _add_buffer ( self ): with output ( output_type = self . _buffer_type ) as output_buffer : return output_buffer def report_test_progress ( self , stacker : TaskcatStacker , poll_interval = 10 ): if self . minimalist : self . minimalist_progress ( stacker , poll_interval ) return _status_dict = stacker . status () while self . _is_test_in_progress ( _status_dict ): for stack in stacker . stacks : self . _print_stack_tree ( stack , buffer = self . buffer ) time . sleep ( poll_interval ) self . buffer . clear () _status_dict = stacker . status () self . _display_final_status ( stacker ) def minimalist_progress ( self , stacker : TaskcatStacker , poll_interval ): _status_dict = stacker . status () history : dict = {} while self . _is_test_in_progress ( _status_dict ): _status_dict = stacker . status () for stack in stacker . stacks : self . _print_tree_minimal ( stack , history ) time . sleep ( poll_interval ) @staticmethod def _print_tree_minimal ( stack , history ): if stack . id not in history : history [ stack . id ] = \"\" if history [ stack . id ] != stack . status : history [ stack . id ] = stack . status msg = f \" { stack . test_name } { stack . region_name } { stack . status } \" if \"FAILED\" in stack . status : LOG . error ( msg ) for event in stack . error_events ( refresh = True ): LOG . error ( f \" { event . logical_id } { event . status_reason } \" ) else : LOG . info ( msg ) @staticmethod def _print_stack_tree ( stack , buffer ): padding_1 = \" \" buffer . append ( \" {}{} stack {} {} \" . format ( padding_1 , \" \\u250f \" , \" \\u24c2 \" , stack . name ) ) if stack . descendants (): for nested_stack in stack . descendants (): buffer . append ( \" {}{} stack {} {} \" . format ( padding_1 , \" \\u2523 \" , \" \\u24c3 \" , nested_stack . name ) ) buffer . append ( \" {}{} region: {} \" . format ( padding_1 , \" \\u2523 \" , stack . region_name )) buffer . append ( \" {}{} status: {}{}{} \" . format ( padding_1 , \" \\u2517 \" , PrintMsg . white , stack . status , PrintMsg . rst_color ) ) @staticmethod def _display_final_status ( stacker : TaskcatStacker ): for final_stack in stacker . stacks : LOG . info ( \" {} stack {} {} \" . format ( \" \\u250f \" , \" \\u24c2 \" , final_stack . name )) if final_stack . descendants (): for nested_stack in final_stack . descendants (): LOG . info ( \" {} stack {} {} \" . format ( \" \\u2523 \" , \" \\u24c3 \" , nested_stack . name ) ) LOG . info ( \" {} region: {} \" . format ( \" \\u2523 \" , final_stack . region_name )) LOG . info ( \" {} status: {}{} {} \" . format ( \" \\u2517 \" , PrintMsg . white , final_stack . status , PrintMsg . rst_color ) ) @staticmethod def _is_test_in_progress ( status_dict , status_condition = \"IN_PROGRESS\" ): return bool ( len ( status_dict [ status_condition ]) > 0 )","title":"Module _tui"},{"location":"reference/_tui.html#variables","text":"LOG","title":"Variables"},{"location":"reference/_tui.html#classes","text":"","title":"Classes"},{"location":"reference/_tui.html#terminalprinter","text":"class TerminalPrinter ( minimalist )","title":"TerminalPrinter"},{"location":"reference/_tui.html#methods","text":"","title":"Methods"},{"location":"reference/_tui.html#minimalist_progress","text":"def minimalist_progress ( self , stacker : taskcat . _cfn . threaded . Stacker , poll_interval ) View Source def minimalist_progress ( self , stacker : TaskcatStacker , poll_interval ) : _status_dict = stacker . status () history : dict = {} while self . _is_test_in_progress ( _status_dict ) : _status_dict = stacker . status () for stack in stacker . stacks : self . _print_tree_minimal ( stack , history ) time . sleep ( poll_interval )","title":"minimalist_progress"},{"location":"reference/_tui.html#report_test_progress","text":"def report_test_progress ( self , stacker : taskcat . _cfn . threaded . Stacker , poll_interval = 10 ) View Source def report_test_progress ( self , stacker : TaskcatStacker , poll_interval = 10 ) : if self . minimalist : self . minimalist_progress ( stacker , poll_interval ) return _status_dict = stacker . status () while self . _is_test_in_progress ( _status_dict ) : for stack in stacker . stacks : self . _print_stack_tree ( stack , buffer = self . buffer ) time . sleep ( poll_interval ) self . buffer . clear () _status_dict = stacker . status () self . _display_final_status ( stacker )","title":"report_test_progress"},{"location":"reference/taskcat_plugin_testhook.html","text":"Module taskcat_plugin_testhook None None View Source from typing import Mapping , Optional from taskcat._config import Config from taskcat._dataclasses import TestObj from taskcat.exceptions import TaskCatException from taskcat.testing._hooks import BaseTaskcatHook class Hook ( BaseTaskcatHook ): def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str , TestObj ], parameters : dict , outputs : Optional [ dict ], ): super () . __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ): raise TaskCatException ( \"generated failure from hook\" ) Classes Hook class Hook ( hook_config : dict , config : taskcat . _config . Config , tests : Mapping [ str , taskcat . _dataclasses . TestObj ], parameters : dict , outputs : Optional [ dict ] ) View Source class Hook ( BaseTaskcatHook ) : def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str, TestObj ] , parameters : dict , outputs : Optional [ dict ] , ) : super (). __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ) : raise TaskCatException ( \"generated failure from hook\" ) Ancestors (in MRO) taskcat.testing._hooks.BaseTaskcatHook","title":"Taskcat Plugin Testhook"},{"location":"reference/taskcat_plugin_testhook.html#module-taskcat_plugin_testhook","text":"None None View Source from typing import Mapping , Optional from taskcat._config import Config from taskcat._dataclasses import TestObj from taskcat.exceptions import TaskCatException from taskcat.testing._hooks import BaseTaskcatHook class Hook ( BaseTaskcatHook ): def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str , TestObj ], parameters : dict , outputs : Optional [ dict ], ): super () . __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ): raise TaskCatException ( \"generated failure from hook\" )","title":"Module taskcat_plugin_testhook"},{"location":"reference/taskcat_plugin_testhook.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat_plugin_testhook.html#hook","text":"class Hook ( hook_config : dict , config : taskcat . _config . Config , tests : Mapping [ str , taskcat . _dataclasses . TestObj ], parameters : dict , outputs : Optional [ dict ] ) View Source class Hook ( BaseTaskcatHook ) : def __init__ ( self , hook_config : dict , config : Config , tests : Mapping [ str, TestObj ] , parameters : dict , outputs : Optional [ dict ] , ) : super (). __init__ ( hook_config , config , tests , parameters , outputs ) if hook_config . get ( \"generate_failure\" ) : raise TaskCatException ( \"generated failure from hook\" )","title":"Hook"},{"location":"reference/taskcat_plugin_testhook.html#ancestors-in-mro","text":"taskcat.testing._hooks.BaseTaskcatHook","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/index.html","text":"Module taskcat taskcat python module None View Source \"\"\" taskcat python module \"\"\" from ._cfn.stack import Stack # noqa: F401 from ._cfn.template import Template # noqa: F401 from ._cli import main # noqa: F401 from ._config import Config # noqa: F401 __all__ = [ \"Stack\" , \"Template\" , \"Config\" , \"main\" ] Sub-modules taskcat.exceptions taskcat.regions_to_partitions taskcat.testing Functions main def main ( cli_core_class =< class ' taskcat . _cli_core . CliCore '>, exit_func =< function exit_with_code at 0x11286c9d0 > ) View Source def main ( cli_core_class = CliCore , exit_func = exit_with_code ): signal . signal ( signal . SIGINT , _sigint_handler ) log_level = _setup_logging ( sys . argv ) args = sys . argv [ 1 :] if not args : args . append ( \"-h\" ) try : _welcome () version = get_installed_version () cli = cli_core_class ( NAME , _cli_modules , DESCRIPTION , version , GLOBAL_ARGS . ARGS ) cli . parse ( args ) _default_profile = cli . parsed_args . __dict__ . get ( \"_profile\" ) if _default_profile : GLOBAL_ARGS . profile = _default_profile cli . run () except TaskCatException as e : LOG . error ( str ( e ), exc_info = _print_tracebacks ( log_level )) exit_func ( 1 ) except Exception as e : # pylint: disable=broad-except LOG . error ( \" %s %s \" , e . __class__ . __name__ , str ( e ), exc_info = _print_tracebacks ( log_level ) ) exit_func ( 1 ) Classes Config class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path ) View Source class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @ staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @ staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @ staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path {file_path}\" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @ staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def get_regions ( self , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = {} for region in test . regions : # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \"{region.account_id}{region.name}\" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \"{test.s3_bucket}-{region.name}\" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj @ staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @ staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests Static methods create def create ( template_file : Optional [ pathlib . Path ] = None , args : Optional [ dict ] = None , global_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat_overrides.yml' ), env_vars : Optional [ dict ] = None , project_root : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) get_params_from_templates def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters Methods get_buckets def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings get_regions def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = {} for region in test . regions : # TODO : comon_utils / determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects get_rendered_parameters def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters get_templates def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates get_tests def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests Stack class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Class variables REMOTE_TEMPLATE_PATH Static methods create def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack delete def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) import_existing def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack Instance variables launch_succeeded status Methods children def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children descendants def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self ) error_events def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors events def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events refresh def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () resources def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources set_stack_properties def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () update def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\") Template class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x103f35850 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Instance variables descendents linesplit s3_key s3_key_prefix Methods parameters def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ int , str ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters url_prefix def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix write def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"Index"},{"location":"reference/taskcat/index.html#module-taskcat","text":"taskcat python module None View Source \"\"\" taskcat python module \"\"\" from ._cfn.stack import Stack # noqa: F401 from ._cfn.template import Template # noqa: F401 from ._cli import main # noqa: F401 from ._config import Config # noqa: F401 __all__ = [ \"Stack\" , \"Template\" , \"Config\" , \"main\" ]","title":"Module taskcat"},{"location":"reference/taskcat/index.html#sub-modules","text":"taskcat.exceptions taskcat.regions_to_partitions taskcat.testing","title":"Sub-modules"},{"location":"reference/taskcat/index.html#functions","text":"","title":"Functions"},{"location":"reference/taskcat/index.html#main","text":"def main ( cli_core_class =< class ' taskcat . _cli_core . CliCore '>, exit_func =< function exit_with_code at 0x11286c9d0 > ) View Source def main ( cli_core_class = CliCore , exit_func = exit_with_code ): signal . signal ( signal . SIGINT , _sigint_handler ) log_level = _setup_logging ( sys . argv ) args = sys . argv [ 1 :] if not args : args . append ( \"-h\" ) try : _welcome () version = get_installed_version () cli = cli_core_class ( NAME , _cli_modules , DESCRIPTION , version , GLOBAL_ARGS . ARGS ) cli . parse ( args ) _default_profile = cli . parsed_args . __dict__ . get ( \"_profile\" ) if _default_profile : GLOBAL_ARGS . profile = _default_profile cli . run () except TaskCatException as e : LOG . error ( str ( e ), exc_info = _print_tracebacks ( log_level )) exit_func ( 1 ) except Exception as e : # pylint: disable=broad-except LOG . error ( \" %s %s \" , e . __class__ . __name__ , str ( e ), exc_info = _print_tracebacks ( log_level ) ) exit_func ( 1 )","title":"main"},{"location":"reference/taskcat/index.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/index.html#config","text":"class Config ( sources : list , uid : uuid . UUID , project_root : pathlib . Path ) View Source class Config : def __init__ ( self , sources : list , uid : uuid . UUID , project_root : Path ): self . config = BaseConfig . from_dict ( DEFAULTS ) self . config . set_source ( \"TASKCAT_DEFAULT\" ) self . project_root = project_root self . uid = uid for source in sources : config_dict : dict = source [ \"config\" ] source_name : str = source [ \"source\" ] source_config = BaseConfig . from_dict ( config_dict ) source_config . set_source ( source_name ) self . config = BaseConfig . merge ( self . config , source_config ) @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root ) # pylint: disable=protected-access,inconsistent-return-statements @ staticmethod def _get_project_source ( base_cls , project_config_path , project_root , template_file ): try : return { \"source\" : str ( project_config_path ), \"config\" : base_cls . _dict_from_file ( project_config_path , fail_ok = False ), } except FileNotFoundError as e : error = e try : legacy_conf = parse_legacy_config ( project_root ) return { \"source\" : str ( project_root / \"ci/taskcat.yml\" ), \"config\" : legacy_conf . to_dict (), } except Exception as e : # pylint: disable=broad-except LOG . debug ( str ( e ), exc_info = True ) if not template_file : # pylint: disable=raise-missing-from raise error @ staticmethod def _dict_from_file ( file_path : Path , fail_ok = True ) -> dict : config_dict = BaseConfig () . to_dict () if not file_path . is_file () and fail_ok : return config_dict try : with open ( str ( file_path ), \"r\" ) as file_handle : config_dict = yaml . safe_load ( file_handle ) return config_dict except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"failed to load config from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) if not fail_ok : raise e return config_dict @ staticmethod def _dict_from_template ( file_path : Path ) -> dict : relative_path = str ( file_path . relative_to ( PROJECT_ROOT )) config_dict = ( BaseConfig () . from_dict ( { \"project\" : { \"template\" : relative_path }, \"tests\" : { \"default\" : {}}} ) . to_dict () ) if not file_path . is_file (): raise TaskCatException ( f \"invalid template path {file_path}\" ) try : template = Template ( str ( file_path ), template_cache = tcat_template_cache ) . template except Exception as e : LOG . warning ( f \"failed to load template from {file_path}\" ) LOG . debug ( str ( e ), exc_info = True ) raise e if not template . get ( \"Metadata\" ): return config_dict if not template [ \"Metadata\" ] . get ( \"taskcat\" ): return config_dict template_config_dict = template [ \"Metadata\" ][ \"taskcat\" ] if not template_config_dict . get ( \"project\" ): template_config_dict [ \"project\" ] = {} template_config_dict [ \"project\" ][ \"template\" ] = relative_path if not template_config_dict . get ( \"tests\" ): template_config_dict [ \"tests\" ] = { \"default\" : {}} return template_config_dict # pylint: disable=protected-access @ staticmethod def _dict_from_env_vars ( env_vars : Optional [ Union [ os . _Environ , Dict [ str , str ]]] = None ): if env_vars is None : env_vars = os . environ config_dict : Dict [ str , Dict [ str , Union [ str , bool , int ]]] = {} for key , value in env_vars . items (): if key . startswith ( \"TASKCAT_\" ): key = key [ 8 :] . lower () sub_key = None key_section = None for section in [ \"general\" , \"project\" , \"tests\" ]: if key . startswith ( section ): sub_key = key [ len ( section ) + 1 :] key_section = section if isinstance ( sub_key , str ) and isinstance ( key_section , str ): if value . isnumeric (): value = int ( value ) elif value . lower () in [ \"true\" , \"false\" ]: value = value . lower () == \"true\" if not config_dict . get ( key_section ): config_dict [ key_section ] = {} config_dict [ key_section ][ sub_key ] = value return config_dict def get_regions ( self , boto3_cache : Boto3Cache = None ): if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str , Dict [ str , RegionObj ]] = {} for test_name , test in self . config . tests . items (): region_objects [ test_name ] = {} for region in test . regions : # TODO: comon_utils/determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects def get_buckets ( self , boto3_cache : Boto3Cache = None ): regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str , S3BucketObj ] = {} bucket_mappings : Dict [ str , Dict [ str , S3BucketObj ]] = {} for test_name , test in self . config . tests . items (): bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items (): if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f \"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region . account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings def _create_legacy_bucket_obj ( self , bucket_objects , region , test ): new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( region . account_id ): name = generate_bucket_name ( self . config . project . name ) auto_generated = True new = True elif bucket_objects . get ( region . account_id ): name = bucket_objects [ region . account_id ] . name auto_generated = bucket_objects [ region . account_id ] . auto_generated else : name = test . s3_bucket auto_generated = False bucket_region = self . _get_bucket_region_for_partition ( region . partition ) bucket_obj = S3BucketObj ( name = name , region = bucket_region , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = bucket_region ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj def _create_regional_bucket_obj ( self , bucket_objects , region , test ): _bucket_obj_key = f \"{region.account_id}{region.name}\" new = False object_acl = ( self . config . project . s3_object_acl if self . config . project . s3_object_acl else \"private\" ) sigv4 = not self . config . project . s3_enable_sig_v2 if not test . s3_bucket and not bucket_objects . get ( _bucket_obj_key ): name = generate_regional_bucket_name ( region ) auto_generated = True new = True elif bucket_objects . get ( _bucket_obj_key ): name = bucket_objects [ _bucket_obj_key ] . name auto_generated = bucket_objects [ _bucket_obj_key ] . auto_generated else : name = f \"{test.s3_bucket}-{region.name}\" auto_generated = False try : region . client ( \"s3\" ) . head_bucket ( Bucket = name ) except ClientError as e : if \"(404)\" in str ( e ): new = True else : raise bucket_obj = S3BucketObj ( name = name , region = region . name , account_id = region . account_id , s3_client = region . session . client ( \"s3\" , region_name = region . name ), auto_generated = auto_generated , object_acl = object_acl , sigv4 = sigv4 , taskcat_id = self . uid , partition = region . partition , regional_buckets = test . s3_regional_buckets , ) if new : bucket_obj . create () return bucket_obj @ staticmethod def _get_bucket_region_for_partition ( partition ): region = \"us-east-1\" if partition == \"aws-us-gov\" : region = \"us-gov-east-1\" elif partition == \"aws-cn\" : region = \"cn-north-1\" return region def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ): parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items (): parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items (): if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ) . results return parameters @ staticmethod def get_params_from_templates ( template_objects ): parameters = {} for test_name , template in template_objects . items (): parameters [ test_name ] = template . parameters () return parameters def get_templates ( self ): templates = {} for test_name , test in self . config . tests . items (): templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates def get_tests ( self , templates , regions , buckets , parameters ): tests = {} for test_name , test in self . config . tests . items (): region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items (): tag_list . append ( Tag ({ \"Key\" : tag_key , \"Value\" : tag_value })) for region_obj in regions [ test_name ] . values (): region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj . name ], parameters [ test_name ][ region_obj . name ], ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ], project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"Config"},{"location":"reference/taskcat/index.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/index.html#create","text":"def create ( template_file : Optional [ pathlib . Path ] = None , args : Optional [ dict ] = None , global_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/.taskcat.yml' ), project_config_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat.yml' ), overrides_path : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat/.taskcat_overrides.yml' ), env_vars : Optional [ dict ] = None , project_root : pathlib . Path = PosixPath ( '/Users/tonynv/work/github/aws-quickstart/python/taskcat' ), uid : uuid . UUID = None ) -> 'Config' View Source @ classmethod # pylint: disable=too-many-locals def create ( cls , template_file : Optional [ Path ] = None , args : Optional [ dict ] = None , global_config_path : Path = GENERAL , project_config_path : Path = PROJECT , overrides_path : Path = OVERRIDES , env_vars : Optional [ dict ] = None , project_root : Path = PROJECT_ROOT , uid : uuid . UUID = None , ) -> \"Config\" : uid = uid if uid else uuid . uuid4 () project_source = cls . _get_project_source ( cls , project_config_path , project_root , template_file ) # general legacy_overrides ( Path ( \"~/.aws/taskcat_global_override.json\" ) . expanduser () . resolve (), global_config_path , \"global\" , ) sources = [ { \"source\" : str ( global_config_path ), \"config\" : cls . _dict_from_file ( global_config_path ), } ] # project config file if project_source : sources . append ( project_source ) # template file if isinstance ( template_file , Path ): sources . append ( { \"source\" : str ( template_file ), \"config\" : cls . _dict_from_template ( template_file ), } ) # override file legacy_overrides ( project_root / \"ci/taskcat_project_override.json\" , overrides_path , \"project\" ) if overrides_path . is_file (): overrides = BaseConfig () . to_dict () with open ( str ( overrides_path ), \"r\" ) as file_handle : override_params = yaml . safe_load ( file_handle ) overrides [ \"project\" ][ \"parameters\" ] = override_params sources . append ({ \"source\" : str ( overrides_path ), \"config\" : overrides }) # environment variables sources . append ( { \"source\" : \"EnvoronmentVariable\" , \"config\" : cls . _dict_from_env_vars ( env_vars ), } ) # cli arguments if args : sources . append ({ \"source\" : \"CliArgument\" , \"config\" : args }) return cls ( sources = sources , uid = uid , project_root = project_root )","title":"create"},{"location":"reference/taskcat/index.html#get_params_from_templates","text":"def get_params_from_templates ( template_objects ) View Source @staticmethod def get_params_from_templates ( template_objects ) : parameters = {} for test_name , template in template_objects . items () : parameters [ test_name ] = template . parameters () return parameters","title":"get_params_from_templates"},{"location":"reference/taskcat/index.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/index.html#get_buckets","text":"def get_buckets ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_buckets ( self , boto3_cache : Boto3Cache = None ) : regions = self . get_regions ( boto3_cache ) bucket_objects : Dict [ str, S3BucketObj ] = {} bucket_mappings : Dict [ str, Dict[str, S3BucketObj ] ] = {} for test_name , test in self . config . tests . items () : bucket_mappings [ test_name ] = {} for region_name , region in regions [ test_name ] . items () : if test . s3_regional_buckets : bucket_obj = self . _create_regional_bucket_obj ( bucket_objects , region , test ) bucket_objects [ f\"{region.account_id}{region.name}\" ] = bucket_obj else : bucket_obj = self . _create_legacy_bucket_obj ( bucket_objects , region , test ) bucket_objects [ region.account_id ] = bucket_obj bucket_mappings [ test_name ][ region_name ] = bucket_obj return bucket_mappings","title":"get_buckets"},{"location":"reference/taskcat/index.html#get_regions","text":"def get_regions ( self , boto3_cache : taskcat . _client_factory . Boto3Cache = None ) View Source def get_regions ( self , boto3_cache : Boto3Cache = None ) : if boto3_cache is None : boto3_cache = Boto3Cache () region_objects : Dict [ str, Dict[str, RegionObj ] ] = {} for test_name , test in self . config . tests . items () : region_objects [ test_name ] = {} for region in test . regions : # TODO : comon_utils / determine_profile_for_region profile = ( test . auth . get ( region , test . auth . get ( \"default\" , \"default\" )) if test . auth else \"default\" ) region_objects [ test_name ][ region ] = RegionObj ( name = region , account_id = boto3_cache . account_id ( profile ), partition = boto3_cache . partition ( profile ), profile = profile , _boto3_cache = boto3_cache , taskcat_id = self . uid , _role_name = test . role_name , ) return region_objects","title":"get_regions"},{"location":"reference/taskcat/index.html#get_rendered_parameters","text":"def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) View Source def get_rendered_parameters ( self , bucket_objects , region_objects , template_objects ) : parameters = {} template_params = self . get_params_from_templates ( template_objects ) for test_name , test in self . config . tests . items () : parameters [ test_name ] = {} for region_name in test . regions : region_params = template_params [ test_name ] . copy () for param_key , param_value in test . parameters . items () : if param_key in region_params : region_params [ param_key ] = param_value region = region_objects [ test_name ][ region_name ] s3bucket = bucket_objects [ test_name ][ region_name ] parameters [ test_name ][ region_name ] = ParamGen ( region_params , s3bucket . name , region . name , region . client , self . config . project . name , test_name , test . az_blacklist , ). results return parameters","title":"get_rendered_parameters"},{"location":"reference/taskcat/index.html#get_templates","text":"def get_templates ( self ) View Source def get_templates ( self ) : templates = {} for test_name , test in self . config . tests . items () : templates [ test_name ] = Template ( template_path = self . project_root / test . template , project_root = self . project_root , s3_key_prefix = f \"{self.config.project.name}/\" , template_cache = tcat_template_cache , ) return templates","title":"get_templates"},{"location":"reference/taskcat/index.html#get_tests","text":"def get_tests ( self , templates , regions , buckets , parameters ) View Source def get_tests ( self , templates , regions , buckets , parameters ) : tests = {} for test_name , test in self . config . tests . items () : region_list = [] tag_list = [] if test . tags : for tag_key , tag_value in test . tags . items () : tag_list . append ( Tag ( { \"Key\" : tag_key , \"Value\" : tag_value } )) for region_obj in regions [ test_name ] . values () : region_list . append ( TestRegion . from_region_obj ( region_obj , buckets [ test_name ][ region_obj.name ] , parameters [ test_name ][ region_obj.name ] , ) ) tests [ test_name ] = TestObj ( name = test_name , template_path = self . project_root / test . template , template = templates [ test_name ] , project_root = self . project_root , regions = region_list , tags = tag_list , uid = self . uid , _project_name = self . config . project . name , _shorten_stack_name = self . config . project . shorten_stack_name , _stack_name = test . stack_name , _stack_name_prefix = test . stack_name_prefix , _stack_name_suffix = test . stack_name_suffix , ) return tests","title":"get_tests"},{"location":"reference/taskcat/index.html#stack","text":"class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Stack"},{"location":"reference/taskcat/index.html#class-variables","text":"REMOTE_TEMPLATE_PATH","title":"Class variables"},{"location":"reference/taskcat/index.html#static-methods_1","text":"","title":"Static methods"},{"location":"reference/taskcat/index.html#create_1","text":"def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack","title":"create"},{"location":"reference/taskcat/index.html#delete","text":"def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" )","title":"delete"},{"location":"reference/taskcat/index.html#import_existing","text":"def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack","title":"import_existing"},{"location":"reference/taskcat/index.html#instance-variables","text":"launch_succeeded status","title":"Instance variables"},{"location":"reference/taskcat/index.html#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/index.html#children","text":"def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children","title":"children"},{"location":"reference/taskcat/index.html#descendants","text":"def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self )","title":"descendants"},{"location":"reference/taskcat/index.html#error_events","text":"def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors","title":"error_events"},{"location":"reference/taskcat/index.html#events","text":"def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events","title":"events"},{"location":"reference/taskcat/index.html#refresh","text":"def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now ()","title":"refresh"},{"location":"reference/taskcat/index.html#resources","text":"def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources","title":"resources"},{"location":"reference/taskcat/index.html#set_stack_properties","text":"def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start ()","title":"set_stack_properties"},{"location":"reference/taskcat/index.html#update","text":"def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\")","title":"update"},{"location":"reference/taskcat/index.html#template","text":"class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x103f35850 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Template"},{"location":"reference/taskcat/index.html#instance-variables_1","text":"descendents linesplit s3_key s3_key_prefix","title":"Instance variables"},{"location":"reference/taskcat/index.html#methods_2","text":"","title":"Methods"},{"location":"reference/taskcat/index.html#parameters","text":"def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ int , str ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters","title":"parameters"},{"location":"reference/taskcat/index.html#url_prefix","text":"def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix","title":"url_prefix"},{"location":"reference/taskcat/index.html#write","text":"def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"write"},{"location":"reference/taskcat/exceptions.html","text":"Module taskcat.exceptions None None View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ () Classes InvalidActionError class InvalidActionError ( expression ) Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred View Source class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ () Ancestors (in MRO) taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. TaskCatException class TaskCatException ( / , * args , ** kwargs ) View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" Ancestors (in MRO) builtins.Exception builtins.BaseException Descendants taskcat.exceptions.InvalidActionError taskcat._s3_stage.S3BucketCreatorException taskcat._amiupdater.AMIUpdaterFatalException taskcat._amiupdater.AMIUpdaterCommitNeededException _amiupdater.AMIUpdaterFatalException _amiupdater.AMIUpdaterCommitNeededException _s3_stage.S3BucketCreatorException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Exceptions"},{"location":"reference/taskcat/exceptions.html#module-taskcatexceptions","text":"None None View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\" class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ ()","title":"Module taskcat.exceptions"},{"location":"reference/taskcat/exceptions.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/exceptions.html#invalidactionerror","text":"class InvalidActionError ( expression ) Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred View Source class InvalidActionError ( TaskCatException ): \"\"\"Exception raised for error when invalid action is supplied Attributes: expression -- input expression in which the error occurred \"\"\" def __init__ ( self , expression ): self . expression = expression super (). __init__ ()","title":"InvalidActionError"},{"location":"reference/taskcat/exceptions.html#ancestors-in-mro","text":"taskcat.exceptions.TaskCatException builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/exceptions.html#class-variables","text":"args","title":"Class variables"},{"location":"reference/taskcat/exceptions.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/exceptions.html#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/taskcat/exceptions.html#taskcatexception","text":"class TaskCatException ( / , * args , ** kwargs ) View Source class TaskCatException ( Exception ): \"\"\"Raised when taskcat experiences a fatal error\"\"\"","title":"TaskCatException"},{"location":"reference/taskcat/exceptions.html#ancestors-in-mro_1","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/exceptions.html#descendants","text":"taskcat.exceptions.InvalidActionError taskcat._s3_stage.S3BucketCreatorException taskcat._amiupdater.AMIUpdaterFatalException taskcat._amiupdater.AMIUpdaterCommitNeededException _amiupdater.AMIUpdaterFatalException _amiupdater.AMIUpdaterCommitNeededException _s3_stage.S3BucketCreatorException","title":"Descendants"},{"location":"reference/taskcat/exceptions.html#class-variables_1","text":"args","title":"Class variables"},{"location":"reference/taskcat/exceptions.html#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/exceptions.html#with_traceback_1","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/taskcat/regions_to_partitions.html","text":"Module taskcat.regions_to_partitions None None View Source REGIONS = { \"af-south-1\": \"aws\", \"ap-east-1\": \"aws\", \"ap-northeast-1\": \"aws\", \"ap-northeast-2\": \"aws\", \"ap-northeast-3\": \"aws\", \"ap-south-1\": \"aws\", \"ap-southeast-1\": \"aws\", \"ap-southeast-2\": \"aws\", \"ca-central-1\": \"aws\", \"eu-central-1\": \"aws\", \"eu-north-1\": \"aws\", \"eu-south-1\": \"aws\", \"eu-west-1\": \"aws\", \"eu-west-2\": \"aws\", \"eu-west-3\": \"aws\", \"me-south-1\": \"aws\", \"sa-east-1\": \"aws\", \"us-east-1\": \"aws\", \"us-east-2\": \"aws\", \"us-west-1\": \"aws\", \"us-west-2\": \"aws\", \"cn-north-1\": \"aws-cn\", \"cn-northwest-1\": \"aws-cn\", \"us-gov-east-1\": \"aws-us-gov\", \"us-gov-west-1\": \"aws-us-gov\", \"us-iso-east-1\": \"aws-iso\", \"us-isob-east-1\": \"aws-iso-b\" } PARTITIONS = { \"aws\": [ \"af-south-1\", \"ap-east-1\", \"ap-northeast-1\", \"ap-northeast-2\", \"ap-northeast-3\", \"ap-south-1\", \"ap-southeast-1\", \"ap-southeast-2\", \"ca-central-1\", \"eu-central-1\", \"eu-north-1\", \"eu-south-1\", \"eu-west-1\", \"eu-west-2\", \"eu-west-3\", \"me-south-1\", \"sa-east-1\", \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ], \"aws-cn\": [ \"cn-north-1\", \"cn-northwest-1\" ], \"aws-us-gov\": [ \"us-gov-east-1\", \"us-gov-west-1\" ], \"aws-iso\": [ \"us-iso-east-1\" ], \"aws-iso-b\": [ \"us-isob-east-1\" ] } Variables PARTITIONS REGIONS","title":"Regions To Partitions"},{"location":"reference/taskcat/regions_to_partitions.html#module-taskcatregions_to_partitions","text":"None None View Source REGIONS = { \"af-south-1\": \"aws\", \"ap-east-1\": \"aws\", \"ap-northeast-1\": \"aws\", \"ap-northeast-2\": \"aws\", \"ap-northeast-3\": \"aws\", \"ap-south-1\": \"aws\", \"ap-southeast-1\": \"aws\", \"ap-southeast-2\": \"aws\", \"ca-central-1\": \"aws\", \"eu-central-1\": \"aws\", \"eu-north-1\": \"aws\", \"eu-south-1\": \"aws\", \"eu-west-1\": \"aws\", \"eu-west-2\": \"aws\", \"eu-west-3\": \"aws\", \"me-south-1\": \"aws\", \"sa-east-1\": \"aws\", \"us-east-1\": \"aws\", \"us-east-2\": \"aws\", \"us-west-1\": \"aws\", \"us-west-2\": \"aws\", \"cn-north-1\": \"aws-cn\", \"cn-northwest-1\": \"aws-cn\", \"us-gov-east-1\": \"aws-us-gov\", \"us-gov-west-1\": \"aws-us-gov\", \"us-iso-east-1\": \"aws-iso\", \"us-isob-east-1\": \"aws-iso-b\" } PARTITIONS = { \"aws\": [ \"af-south-1\", \"ap-east-1\", \"ap-northeast-1\", \"ap-northeast-2\", \"ap-northeast-3\", \"ap-south-1\", \"ap-southeast-1\", \"ap-southeast-2\", \"ca-central-1\", \"eu-central-1\", \"eu-north-1\", \"eu-south-1\", \"eu-west-1\", \"eu-west-2\", \"eu-west-3\", \"me-south-1\", \"sa-east-1\", \"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ], \"aws-cn\": [ \"cn-north-1\", \"cn-northwest-1\" ], \"aws-us-gov\": [ \"us-gov-east-1\", \"us-gov-west-1\" ], \"aws-iso\": [ \"us-iso-east-1\" ], \"aws-iso-b\": [ \"us-isob-east-1\" ] }","title":"Module taskcat.regions_to_partitions"},{"location":"reference/taskcat/regions_to_partitions.html#variables","text":"PARTITIONS REGIONS","title":"Variables"},{"location":"reference/taskcat/_cfn/index.html","text":"Module taskcat._cfn None None Sub-modules taskcat._cfn.stack taskcat._cfn.stack_url_helper taskcat._cfn.template taskcat._cfn.threaded","title":"Index"},{"location":"reference/taskcat/_cfn/index.html#module-taskcat_cfn","text":"None None","title":"Module taskcat._cfn"},{"location":"reference/taskcat/_cfn/index.html#sub-modules","text":"taskcat._cfn.stack taskcat._cfn.stack_url_helper taskcat._cfn.template taskcat._cfn.threaded","title":"Sub-modules"},{"location":"reference/taskcat/_cfn/stack.html","text":"Module taskcat._cfn.stack None None View Source import json import logging import os import random import re import string from datetime import datetime , timedelta from pathlib import Path from threading import Timer from typing import Callable , List , Optional , Tuple from uuid import UUID , uuid4 import boto3 import yaml from taskcat._cfn.template import Template , tcat_template_cache from taskcat._common_utils import ordered_dump , pascal_to_snake , s3_url_maker from taskcat._dataclasses import Tag , TestRegion LOG = logging . getLogger ( __name__ ) GENERIC_ERROR_PATTERNS = [ r \"(The following resource\\(s\\) failed to create: )\" , r \"(^Resource creation cancelled$)\" , ] def criteria_matches ( criteria : dict , instance ): # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ): raise ValueError ( f \" { k } is not a valid property of { type ( instance ) } \" ) for k , v in criteria . items (): # matching is AND for multiple criteria, so as soon as one fails, # it's not a match if getattr ( instance , k ) != v : return False return True class StackStatus : COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] class Capabilities : IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \" {} {} {} \" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {} >\" . format ( self . event_id , hex ( id ( self ))) class Resource : def __init__ ( self , stack_id : str , resource_dict : dict , test_name : str = \"\" , uuid : UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id : str = stack_id self . test_name : str = test_name self . uuid : UUID = uuid self . logical_id : str = resource_dict [ \"LogicalResourceId\" ] self . type : str = resource_dict [ \"ResourceType\" ] self . status : str = resource_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . last_updated_timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {} >\" . format ( self . logical_id , self . status ) class Parameter : def __init__ ( self , param_dict : dict ): self . key : str = param_dict [ \"ParameterKey\" ] self . value : str = \"\" self . raw_value : str = \"\" self . use_previous_value : bool = False self . resolved_value : str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value : self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value : param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value : param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] class FilterableList ( list ): def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ): if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ): flist . append ( item ) return flist class Stacks ( FilterableList ): pass class Resources ( FilterableList ): pass class Events ( FilterableList ): pass class Tags ( FilterableList ): pass class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {} >\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @property def launch_succeeded ( self ): return self . _launch_succeeded @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \" { stack_properties [ 'StackId' ] } . tried \" f \" { parent_stack . template . project_root / relative_path } \" f \" and { absolute_path } \" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: { str ( e ) } \" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: { stack_id } \" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Variables GENERIC_ERROR_PATTERNS LOG Functions criteria_matches def criteria_matches ( criteria : dict , instance ) View Source def criteria_matches ( criteria : dict , instance ) : # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ) : raise ValueError ( f \" {k} is not a valid property of {type(instance)} \" ) for k , v in criteria . items () : # matching is AND for multiple criteria , so as soon as one fails , # it ' s not a match if getattr ( instance , k ) != v : return False return True Classes Capabilities class Capabilities ( / , * args , ** kwargs ) View Source class Capabilities: IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] Class variables ALL AUTO_EXPAND IAM NAMED_IAM Event class Event ( event_dict : dict ) View Source class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \"{} {} {}\" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {}>\" . format ( self . event_id , hex ( id ( self ))) Events class Events ( / , * args , ** kwargs ) View Source class Events ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. FilterableList class FilterableList ( / , * args , ** kwargs ) View Source class FilterableList ( list ) : def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist Ancestors (in MRO) builtins.list Descendants taskcat._cfn.stack.Stacks taskcat._cfn.stack.Resources taskcat._cfn.stack.Events taskcat._cfn.stack.Tags Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Output class Output ( output_dict : dict ) View Source class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] Parameter class Parameter ( param_dict : dict ) View Source class Parameter: def __init__ ( self , param_dict: dict ): self . key: str = param_dict [ \"ParameterKey\" ] self . value: str = \"\" self . raw_value: str = \"\" self . use_previous_value: bool = False self . resolved_value: str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value: self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value: param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value: param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict Methods dump def dump ( self ) View Source def dump ( self ) : param_dict = { \" ParameterKey \" : self . key } if self . value : param_dict [ \" ParameterValue \" ] = self . value if self . use_previous_value : param_dict [ \" UsePreviousValue \" ] = self . use_previous_value return param_dict Resource class Resource ( stack_id : str , resource_dict : dict , test_name : str = '' , uuid : uuid . UUID = None ) View Source class Resource: def __init__ ( self , stack_id: str , resource_dict: dict , test_name: str = \"\" , uuid: UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id: str = stack_id self . test_name: str = test_name self . uuid: UUID = uuid self . logical_id: str = resource_dict [ \"LogicalResourceId\" ] self . type: str = resource_dict [ \"ResourceType\" ] self . status: str = resource_dict [ \"ResourceStatus\" ] self . physical_id: str = \"\" self . last_updated_timestamp: datetime = datetime . fromtimestamp ( 0 ) self . status_reason: str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {}>\" . format ( self . logical_id , self . status ) Resources class Resources ( / , * args , ** kwargs ) View Source class Resources ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Stack class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors Class variables REMOTE_TEMPLATE_PATH Static methods create def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack delete def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) import_existing def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack Instance variables launch_succeeded status Methods children def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children descendants def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self ) error_events def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors events def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events refresh def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () resources def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources set_stack_properties def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () update def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\") StackStatus class StackStatus ( / , * args , ** kwargs ) View Source class StackStatus: COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] Class variables COMPLETE FAILED IN_PROGRESS Stacks class Stacks ( / , * args , ** kwargs ) View Source class Stacks ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. Tags class Tags ( / , * args , ** kwargs ) View Source class Tags ( FilterableList ): pass Ancestors (in MRO) taskcat._cfn.stack.FilterableList builtins.list Methods append def append ( self , object , / ) Append object to the end of the list. clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. filter def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. insert def insert ( self , index , object , / ) Insert object before index. pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"Stack"},{"location":"reference/taskcat/_cfn/stack.html#module-taskcat_cfnstack","text":"None None View Source import json import logging import os import random import re import string from datetime import datetime , timedelta from pathlib import Path from threading import Timer from typing import Callable , List , Optional , Tuple from uuid import UUID , uuid4 import boto3 import yaml from taskcat._cfn.template import Template , tcat_template_cache from taskcat._common_utils import ordered_dump , pascal_to_snake , s3_url_maker from taskcat._dataclasses import Tag , TestRegion LOG = logging . getLogger ( __name__ ) GENERIC_ERROR_PATTERNS = [ r \"(The following resource\\(s\\) failed to create: )\" , r \"(^Resource creation cancelled$)\" , ] def criteria_matches ( criteria : dict , instance ): # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ): raise ValueError ( f \" { k } is not a valid property of { type ( instance ) } \" ) for k , v in criteria . items (): # matching is AND for multiple criteria, so as soon as one fails, # it's not a match if getattr ( instance , k ) != v : return False return True class StackStatus : COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ] class Capabilities : IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ] class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \" {} {} {} \" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {} >\" . format ( self . event_id , hex ( id ( self ))) class Resource : def __init__ ( self , stack_id : str , resource_dict : dict , test_name : str = \"\" , uuid : UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id : str = stack_id self . test_name : str = test_name self . uuid : UUID = uuid self . logical_id : str = resource_dict [ \"LogicalResourceId\" ] self . type : str = resource_dict [ \"ResourceType\" ] self . status : str = resource_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . last_updated_timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {} >\" . format ( self . logical_id , self . status ) class Parameter : def __init__ ( self , param_dict : dict ): self . key : str = param_dict [ \"ParameterKey\" ] self . value : str = \"\" self . raw_value : str = \"\" self . use_previous_value : bool = False self . resolved_value : str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value : self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value : param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value : param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ] class FilterableList ( list ): def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ): if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ): flist . append ( item ) return flist class Stacks ( FilterableList ): pass class Resources ( FilterableList ): pass class Events ( FilterableList ): pass class Tags ( FilterableList ): pass class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {} >\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @property def launch_succeeded ( self ): return self . _launch_succeeded @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \" { stack_properties [ 'StackId' ] } . tried \" f \" { parent_stack . template . project_root / relative_path } \" f \" and { absolute_path } \" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: { str ( e ) } \" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: { stack_id } \" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Module taskcat._cfn.stack"},{"location":"reference/taskcat/_cfn/stack.html#variables","text":"GENERIC_ERROR_PATTERNS LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/stack.html#functions","text":"","title":"Functions"},{"location":"reference/taskcat/_cfn/stack.html#criteria_matches","text":"def criteria_matches ( criteria : dict , instance ) View Source def criteria_matches ( criteria : dict , instance ) : # fail if criteria includes an invalid property for k in criteria : if not hasattr ( instance , k ) : raise ValueError ( f \" {k} is not a valid property of {type(instance)} \" ) for k , v in criteria . items () : # matching is AND for multiple criteria , so as soon as one fails , # it ' s not a match if getattr ( instance , k ) != v : return False return True","title":"criteria_matches"},{"location":"reference/taskcat/_cfn/stack.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/stack.html#capabilities","text":"class Capabilities ( / , * args , ** kwargs ) View Source class Capabilities: IAM = \"CAPABILITY_IAM\" NAMED_IAM = \"CAPABILITY_NAMED_IAM\" AUTO_EXPAND = \"CAPABILITY_AUTO_EXPAND\" ALL = [ IAM , NAMED_IAM , AUTO_EXPAND ]","title":"Capabilities"},{"location":"reference/taskcat/_cfn/stack.html#class-variables","text":"ALL AUTO_EXPAND IAM NAMED_IAM","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack.html#event","text":"class Event ( event_dict : dict ) View Source class Event : def __init__ ( self , event_dict : dict ): self . event_id : str = event_dict [ \"EventId\" ] self . stack_name : str = event_dict [ \"StackName\" ] self . logical_id : str = event_dict [ \"LogicalResourceId\" ] self . type : str = event_dict [ \"ResourceType\" ] self . status : str = event_dict [ \"ResourceStatus\" ] self . physical_id : str = \"\" self . timestamp : datetime = datetime . fromtimestamp ( 0 ) self . status_reason : str = \"\" self . properties : dict = {} if \"PhysicalResourceId\" in event_dict . keys (): self . physical_id = event_dict [ \"PhysicalResourceId\" ] if \"Timestamp\" in event_dict . keys (): self . timestamp = event_dict [ \"Timestamp\" ] if \"ResourceStatusReason\" in event_dict . keys (): self . status_reason = event_dict [ \"ResourceStatusReason\" ] if \"ResourceProperties\" in event_dict . keys (): self . properties = json . loads ( event_dict [ \"ResourceProperties\" ]) def __str__ ( self ): return \"{} {} {}\" . format ( self . timestamp , self . logical_id , self . status ) def __repr__ ( self ): return \"<Event object {} at {}>\" . format ( self . event_id , hex ( id ( self )))","title":"Event"},{"location":"reference/taskcat/_cfn/stack.html#events","text":"class Events ( / , * args , ** kwargs ) View Source class Events ( FilterableList ): pass","title":"Events"},{"location":"reference/taskcat/_cfn/stack.html#ancestors-in-mro","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#append","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack.html#clear","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack.html#copy","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack.html#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack.html#extend","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack.html#filter","text":"def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack.html#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack.html#insert","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack.html#pop","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack.html#remove","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack.html#reverse","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack.html#sort","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack.html#filterablelist","text":"class FilterableList ( / , * args , ** kwargs ) View Source class FilterableList ( list ) : def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"FilterableList"},{"location":"reference/taskcat/_cfn/stack.html#ancestors-in-mro_1","text":"builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack.html#descendants","text":"taskcat._cfn.stack.Stacks taskcat._cfn.stack.Resources taskcat._cfn.stack.Events taskcat._cfn.stack.Tags","title":"Descendants"},{"location":"reference/taskcat/_cfn/stack.html#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#append_1","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack.html#clear_1","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack.html#copy_1","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack.html#count_1","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack.html#extend_1","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack.html#filter_1","text":"def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack.html#index_1","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack.html#insert_1","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack.html#pop_1","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack.html#remove_1","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack.html#reverse_1","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack.html#sort_1","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack.html#output","text":"class Output ( output_dict : dict ) View Source class Output : def __init__ ( self , output_dict : dict ): self . key : str = output_dict [ \"OutputKey\" ] self . value : str = output_dict [ \"OutputValue\" ] self . description : str = \"\" self . export_name : str = \"\" if \"Description\" in output_dict . keys (): self . description = output_dict [ \"Description\" ] if \"ExportName\" in output_dict . keys (): self . export_name = output_dict [ \"ExportName\" ]","title":"Output"},{"location":"reference/taskcat/_cfn/stack.html#parameter","text":"class Parameter ( param_dict : dict ) View Source class Parameter: def __init__ ( self , param_dict: dict ): self . key: str = param_dict [ \"ParameterKey\" ] self . value: str = \"\" self . raw_value: str = \"\" self . use_previous_value: bool = False self . resolved_value: str = \"\" if \"ParameterValue\" in param_dict . keys (): self . value = param_dict [ \"ParameterValue\" ] if \"UsePreviousValue\" in param_dict . keys (): self . use_previous_value = param_dict [ \"UsePreviousValue\" ] if \"ResolvedValue\" in param_dict . keys (): self . resolved_value = param_dict [ \"ResolvedValue\" ] if self . value and not self . raw_value: self . raw_value = self . value def dump ( self ): param_dict = { \"ParameterKey\" : self . key } if self . value: param_dict [ \"ParameterValue\" ] = self . value if self . use_previous_value: param_dict [ \"UsePreviousValue\" ] = self . use_previous_value return param_dict","title":"Parameter"},{"location":"reference/taskcat/_cfn/stack.html#methods_2","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#dump","text":"def dump ( self ) View Source def dump ( self ) : param_dict = { \" ParameterKey \" : self . key } if self . value : param_dict [ \" ParameterValue \" ] = self . value if self . use_previous_value : param_dict [ \" UsePreviousValue \" ] = self . use_previous_value return param_dict","title":"dump"},{"location":"reference/taskcat/_cfn/stack.html#resource","text":"class Resource ( stack_id : str , resource_dict : dict , test_name : str = '' , uuid : uuid . UUID = None ) View Source class Resource: def __init__ ( self , stack_id: str , resource_dict: dict , test_name: str = \"\" , uuid: UUID = None ): uuid = uuid if uuid else uuid4 () self . stack_id: str = stack_id self . test_name: str = test_name self . uuid: UUID = uuid self . logical_id: str = resource_dict [ \"LogicalResourceId\" ] self . type: str = resource_dict [ \"ResourceType\" ] self . status: str = resource_dict [ \"ResourceStatus\" ] self . physical_id: str = \"\" self . last_updated_timestamp: datetime = datetime . fromtimestamp ( 0 ) self . status_reason: str = \"\" if \"PhysicalResourceId\" in resource_dict . keys (): self . physical_id = resource_dict [ \"PhysicalResourceId\" ] if \"LastUpdatedTimestamp\" in resource_dict . keys (): self . last_updated_timestamp = resource_dict [ \"LastUpdatedTimestamp\" ] if \"ResourceStatusReason\" in resource_dict . keys (): self . status_reason = resource_dict [ \"ResourceStatusReason\" ] def __str__ ( self ): return \"<Resource {} {}>\" . format ( self . logical_id , self . status )","title":"Resource"},{"location":"reference/taskcat/_cfn/stack.html#resources","text":"class Resources ( / , * args , ** kwargs ) View Source class Resources ( FilterableList ): pass","title":"Resources"},{"location":"reference/taskcat/_cfn/stack.html#ancestors-in-mro_2","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack.html#methods_3","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#append_2","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack.html#clear_2","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack.html#copy_2","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack.html#count_2","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack.html#extend_2","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack.html#filter_2","text":"def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack.html#index_2","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack.html#insert_2","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack.html#pop_2","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack.html#remove_2","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack.html#reverse_2","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack.html#sort_2","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack.html#stack","text":"class Stack ( region : taskcat . _dataclasses . TestRegion , stack_id : str , template : taskcat . _cfn . template . Template , test_name , uuid : uuid . UUID = None ) View Source class Stack : # pylint: disable=too-many-instance-attributes REMOTE_TEMPLATE_PATH = Path ( \".taskcat/.remote_templates\" ) def __init__ ( self , region : TestRegion , stack_id : str , template : Template , test_name , uuid : UUID = None , ): uuid = uuid if uuid else uuid4 () self . test_name : str = test_name self . uuid : UUID = uuid self . id : str = stack_id self . template : Template = template self . name : str = self . _get_name () self . region : TestRegion = region self . region_name = region . name self . client : boto3 . client = region . client ( \"cloudformation\" ) self . completion_time : timedelta = timedelta ( 0 ) self . role_arn = region . role_arn # properties from additional cfn api calls self . _events : Events = Events () self . _resources : Resources = Resources () self . _children : Stacks = Stacks () # properties from describe_stacks response self . change_set_id : str = \"\" self . parameters : List [ Parameter ] = [] self . creation_time : datetime = datetime . fromtimestamp ( 0 ) self . deletion_time : datetime = datetime . fromtimestamp ( 0 ) self . _status : str = \"\" self . status_reason : str = \"\" self . disable_rollback : bool = False self . timeout_in_minutes : int = 0 self . capabilities : List [ str ] = [] self . outputs : List [ Output ] = [] self . tags : List [ Tag ] = [] self . parent_id : str = \"\" self . root_id : str = \"\" self . _launch_succeeded : bool = False self . _auto_refresh_interval : timedelta = timedelta ( seconds = 60 ) self . _last_event_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_resource_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _last_child_refresh : datetime = datetime . fromtimestamp ( 0 ) self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () def __str__ ( self ): return self . id def __repr__ ( self ): return \"<Stack object {} at {}>\" . format ( self . name , hex ( id ( self ))) def _get_region ( self ) -> str : return self . id . split ( \":\" )[ 3 ] def _get_name ( self ) -> str : return self . id . split ( \":\" )[ 5 ] . split ( \"/\" )[ 1 ] def _auto_refresh ( self , last_refresh ): if datetime . now () - last_refresh > self . _auto_refresh_interval : return True return False @ property def status ( self ): if self . _status in StackStatus . COMPLETE : if not self . launch_succeeded : self . _status = \"OUT_OF_ORDER_EVENT\" self . status_reason = ( \"COMPLETE event not detected. \" + \"Potential out-of-band action against the stack.\" ) return self . _status @ status . setter def status ( self , status ): _complete = StackStatus . COMPLETE . copy () del _complete [ _complete . index ( \"DELETE_COMPLETE\" )] self . _status = status if status in StackStatus . FAILED : self . _launch_succeeded = False return if status in _complete : self . _launch_succeeded = True return return @ property def launch_succeeded ( self ): return self . _launch_succeeded @ classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t . dump () for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options )[ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack @ staticmethod def _cfn_format_parameters ( parameters ): return [{ \"ParameterKey\" : k , \"ParameterValue\" : v } for k , v in parameters . items ()] @ classmethod def _import_child ( # pylint: disable=too-many-locals cls , stack_properties : dict , parent_stack : \"Stack\" ) -> Optional [ \"Stack\" ]: try : url = \"\" for event in parent_stack . events (): if ( event . physical_id == stack_properties [ \"StackId\" ] and event . properties ): url = event . properties [ \"TemplateURL\" ] if url . startswith ( parent_stack . template . url_prefix ()): # Template is part of the project, discovering path relative_path = url . replace ( parent_stack . template . url_prefix (), \"\" ) . lstrip ( \"/\" ) absolute_path = parent_stack . template . project_root / relative_path if not absolute_path . is_file (): # try with the base folder stripped off relative_path2 = Path ( relative_path ) relative_path2 = relative_path2 . relative_to ( * relative_path2 . parts [: 1 ] ) absolute_path = parent_stack . template . project_root / relative_path2 if not absolute_path . is_file (): LOG . warning ( f \"Failed to find template for child stack \" f \"{stack_properties['StackId']}. tried \" f \"{parent_stack.template.project_root / relative_path}\" f \" and {absolute_path}\" ) return None else : # Assuming template is remote to project and downloading it cfn_client = parent_stack . client tempate_body = cfn_client . get_template ( StackName = stack_properties [ \"StackId\" ] )[ \"TemplateBody\" ] path = parent_stack . template . project_root / Stack . REMOTE_TEMPLATE_PATH os . makedirs ( path , exist_ok = True ) fname = ( \"\" . join ( random . choice ( string . ascii_lowercase ) # nosec for _ in range ( 16 ) ) + \".template\" ) absolute_path = path / fname if not isinstance ( tempate_body , str ): tempate_body = ordered_dump ( tempate_body , dumper = yaml . SafeDumper ) if not absolute_path . exists (): with open ( absolute_path , \"w\" ) as fh : fh . write ( tempate_body ) template = Template ( template_path = str ( absolute_path ), project_root = parent_stack . template . project_root , url = url , template_cache = tcat_template_cache , ) stack = cls ( parent_stack . region , stack_properties [ \"StackId\" ], template , parent_stack . name , parent_stack . uuid , ) stack . set_stack_properties ( stack_properties ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to import child stack: {str(e)}\" ) LOG . debug ( \"traceback:\" , exc_info = True ) return None return stack @ classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ], template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now () def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO: get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id )[ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple [ str , Callable ]] = [ ( \"Parameters\" , Parameter ), ( \"Outputs\" , Output ), ( \"Tags\" , Tag ), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , []): item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items (): if key in [ p [ 0 ] for p in iterable_props ]: # noqa: C412 continue key = pascal_to_snake ( key ) . replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start () @ staticmethod def _merge_props ( existing_props , new ): added = False for existing_id , prop in enumerate ( existing_props ): if prop . key == new . key : existing_props [ existing_id ] = new added = True if not added : existing_props . append ( new ) def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ): self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events @ staticmethod def _is_generic ( event : Event ) -> bool : generic = False for regex in GENERIC_ERROR_PATTERNS : if re . search ( regex , event . status_reason ): generic = True return generic def _fetch_stack_events ( self ) -> None : self . _last_event_refresh = datetime . now () events = Events () for page in self . client . get_paginator ( \"describe_stack_events\" ) . paginate ( StackName = self . id ): for event in page [ \"StackEvents\" ]: events . append ( Event ( event )) self . _events = events def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ): self . _fetch_stack_resources () return self . _resources def _fetch_stack_resources ( self ) -> None : self . _last_resource_refresh = datetime . now () resources = Resources () for page in self . client . get_paginator ( \"list_stack_resources\" ) . paginate ( StackName = self . id ): for resource in page [ \"StackResourceSummaries\" ]: resources . append ( Resource ( self . id , resource , self . test_name , self . uuid )) self . _resources = resources @ staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" ) def update ( self , * args , ** kwargs ): raise NotImplementedError ( \"Stack updates not implemented\" ) def _fetch_children ( self ) -> None : self . _last_child_refresh = datetime . now () for page in self . client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack in page [ \"Stacks\" ]: if self . _children . filter ( id = stack [ \"StackId\" ]): continue if \"ParentId\" in stack . keys (): if self . id == stack [ \"ParentId\" ]: stack_obj = Stack . _import_child ( stack , self ) if stack_obj : self . _children . append ( stack_obj ) def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ): self . _fetch_children () return self . _children def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ): descendants += stack . children () for child in stack . children (): descendants = recurse ( child , descendants ) return descendants return recurse ( self ) def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ([ self ]) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ) . filter ({ \"status\" : status }) return errors","title":"Stack"},{"location":"reference/taskcat/_cfn/stack.html#class-variables_1","text":"REMOTE_TEMPLATE_PATH","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/stack.html#create","text":"def create ( region : taskcat . _dataclasses . TestRegion , stack_name : str , template : taskcat . _cfn . template . Template , tags : List [ taskcat . _dataclasses . Tag ] = None , disable_rollback : bool = True , test_name : str = '' , uuid : uuid . UUID = None ) -> 'Stack' View Source @classmethod def create ( cls , region : TestRegion , stack_name : str , template : Template , tags : List [ Tag ] = None , disable_rollback : bool = True , test_name : str = \"\" , uuid : UUID = None , ) -> \"Stack\" : parameters = cls . _cfn_format_parameters ( region . parameters ) uuid = uuid if uuid else uuid4 () cfn_client = region . client ( \"cloudformation\" ) tags = [ t.dump() for t in tags ] if tags else [] template = Template ( template_path = template . template_path , project_root = template . project_root , s3_key_prefix = template . s3_key_prefix , url = s3_url_maker ( region . s3_bucket . name , template . s3_key , region . client ( \"s3\" ), region . s3_bucket . auto_generated , ), template_cache = tcat_template_cache , ) create_options = { \"StackName\" : stack_name , \"TemplateURL\" : template . url , \"Parameters\" : parameters , \"DisableRollback\" : disable_rollback , \"Tags\" : tags , \"Capabilities\" : Capabilities . ALL , } if region . role_arn : create_options [ \"RoleARN\" ] = region . role_arn stack_id = cfn_client . create_stack ( ** create_options ) [ \"StackId\" ] stack = cls ( region , stack_id , template , test_name , uuid ) # fetch property values from cfn stack . refresh () return stack","title":"create"},{"location":"reference/taskcat/_cfn/stack.html#delete","text":"def delete ( client , stack_id ) -> None View Source @staticmethod def delete ( client , stack_id ) -> None : client . delete_stack ( StackName = stack_id ) LOG . info ( f \"Deleting stack: {stack_id}\" )","title":"delete"},{"location":"reference/taskcat/_cfn/stack.html#import_existing","text":"def import_existing ( stack_properties : dict , template : taskcat . _cfn . template . Template , region : taskcat . _dataclasses . TestRegion , test_name : str , uid : uuid . UUID ) -> 'Stack' View Source @classmethod def import_existing ( cls , stack_properties : dict , template : Template , region : TestRegion , test_name : str , uid : UUID , ) -> \"Stack\" : stack = cls ( region , stack_properties [ \"StackId\" ] , template , test_name , uid ) stack . set_stack_properties ( stack_properties ) return stack","title":"import_existing"},{"location":"reference/taskcat/_cfn/stack.html#instance-variables","text":"launch_succeeded status","title":"Instance variables"},{"location":"reference/taskcat/_cfn/stack.html#methods_4","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#children","text":"def children ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def children ( self , refresh = False ) -> Stacks : if ( refresh or not self . _children or self . _auto_refresh ( self . _last_child_refresh ) ) : self . _fetch_children () return self . _children","title":"children"},{"location":"reference/taskcat/_cfn/stack.html#descendants_1","text":"def descendants ( self , refresh = False ) -> taskcat . _cfn . stack . Stacks View Source def descendants ( self , refresh = False ) -> Stacks : if refresh or not self . _children : self . _fetch_children () def recurse ( stack : Stack , descendants : Stacks = None ) -> Stacks : descendants = descendants if descendants else Stacks () if stack . children ( refresh = refresh ) : descendants += stack . children () for child in stack . children () : descendants = recurse ( child , descendants ) return descendants return recurse ( self )","title":"descendants"},{"location":"reference/taskcat/_cfn/stack.html#error_events","text":"def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> taskcat . _cfn . stack . Events View Source def error_events ( self , recurse : bool = True , include_generic : bool = False , refresh = False ) -> Events : errors = Events () stacks = Stacks ( [ self ] ) if recurse : stacks += self . descendants () for stack in stacks : for status in StackStatus . FAILED : errors += stack . events ( refresh = refresh , include_generic = include_generic ). filter ( { \"status\" : status } ) return errors","title":"error_events"},{"location":"reference/taskcat/_cfn/stack.html#events_1","text":"def events ( self , refresh : bool = False , include_generic : bool = True ) -> taskcat . _cfn . stack . Events View Source def events ( self , refresh : bool = False , include_generic : bool = True ) -> Events : if refresh or not self . _events or self . _auto_refresh ( self . _last_event_refresh ) : self . _fetch_stack_events () events = self . _events if not include_generic : events = Events ([ event for event in events if not self . _is_generic ( event )]) return events","title":"events"},{"location":"reference/taskcat/_cfn/stack.html#refresh","text":"def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False ) -> None View Source def refresh ( self , properties : bool = True , events : bool = False , resources : bool = False , children : bool = False , ) -> None : if properties : self . set_stack_properties () if events : self . _fetch_stack_events () self . _last_event_refresh = datetime . now () if resources : self . _fetch_stack_resources () self . _last_resource_refresh = datetime . now () if children : self . _fetch_children () self . _last_child_refresh = datetime . now ()","title":"refresh"},{"location":"reference/taskcat/_cfn/stack.html#resources_1","text":"def resources ( self , refresh : bool = False ) -> taskcat . _cfn . stack . Resources View Source def resources ( self , refresh : bool = False ) -> Resources : if ( refresh or not self . _resources or self . _auto_refresh ( self . _last_resource_refresh ) ) : self . _fetch_stack_resources () return self . _resources","title":"resources"},{"location":"reference/taskcat/_cfn/stack.html#set_stack_properties","text":"def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None View Source def set_stack_properties ( self , stack_properties : Optional [ dict ] = None ) -> None : # TODO : get time to complete for complete stacks and % complete props : dict = stack_properties if stack_properties else {} self . _timer . cancel () if not props : describe_stacks = self . client . describe_stacks props = describe_stacks ( StackName = self . id ) [ \"Stacks\" ][ 0 ] iterable_props : List [ Tuple[str, Callable ] ] = [ (\"Parameters\", Parameter), (\"Outputs\", Output), (\"Tags\", Tag), ] for prop_name , prop_class in iterable_props : for item in props . get ( prop_name , [] ) : item = prop_class ( item ) self . _merge_props ( getattr ( self , prop_name . lower ()), item ) for key , value in props . items () : if key in [ p[0 ] for p in iterable_props ]: # noqa : C412 continue key = pascal_to_snake ( key ). replace ( \"stack_\" , \"\" ) setattr ( self , key , value ) if self . status in StackStatus . IN_PROGRESS : self . _timer = Timer ( self . _auto_refresh_interval . total_seconds (), self . refresh ) self . _timer . start ()","title":"set_stack_properties"},{"location":"reference/taskcat/_cfn/stack.html#update","text":"def update ( self , * args , ** kwargs ) View Source def update(self, *args, **kwargs): raise NotImplementedError(\"Stack updates not implemented\")","title":"update"},{"location":"reference/taskcat/_cfn/stack.html#stackstatus","text":"class StackStatus ( / , * args , ** kwargs ) View Source class StackStatus: COMPLETE = [ \"CREATE_COMPLETE\" , \"UPDATE_COMPLETE\" , \"DELETE_COMPLETE\" ] IN_PROGRESS = [ \"CREATE_IN_PROGRESS\" , \"DELETE_IN_PROGRESS\" , \"UPDATE_IN_PROGRESS\" , \"UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\" , ] FAILED = [ \"DELETE_FAILED\" , \"CREATE_FAILED\" , \"ROLLBACK_IN_PROGRESS\" , \"ROLLBACK_FAILED\" , \"ROLLBACK_COMPLETE\" , \"UPDATE_ROLLBACK_IN_PROGRESS\" , \"UPDATE_ROLLBACK_FAILED\" , \"UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS\" , \"UPDATE_ROLLBACK_COMPLETE\" , \"OUT_OF_ORDER_EVENT\" , ]","title":"StackStatus"},{"location":"reference/taskcat/_cfn/stack.html#class-variables_2","text":"COMPLETE FAILED IN_PROGRESS","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack.html#stacks","text":"class Stacks ( / , * args , ** kwargs ) View Source class Stacks ( FilterableList ): pass","title":"Stacks"},{"location":"reference/taskcat/_cfn/stack.html#ancestors-in-mro_3","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack.html#methods_5","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#append_3","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack.html#clear_3","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack.html#copy_3","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack.html#count_3","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack.html#extend_3","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack.html#filter_3","text":"def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack.html#index_3","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack.html#insert_3","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack.html#pop_3","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack.html#remove_3","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack.html#reverse_3","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack.html#sort_3","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack.html#tags","text":"class Tags ( / , * args , ** kwargs ) View Source class Tags ( FilterableList ): pass","title":"Tags"},{"location":"reference/taskcat/_cfn/stack.html#ancestors-in-mro_4","text":"taskcat._cfn.stack.FilterableList builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/_cfn/stack.html#methods_6","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack.html#append_4","text":"def append ( self , object , / ) Append object to the end of the list.","title":"append"},{"location":"reference/taskcat/_cfn/stack.html#clear_4","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/taskcat/_cfn/stack.html#copy_4","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/taskcat/_cfn/stack.html#count_4","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/taskcat/_cfn/stack.html#extend_4","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/taskcat/_cfn/stack.html#filter_4","text":"def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) View Source def filter ( self , criteria : Optional [ dict ] = None , ** kwargs ) : if not criteria and not kwargs : return self if not criteria : criteria = kwargs flist = FilterableList () for item in self : if criteria_matches ( criteria , item ) : flist . append ( item ) return flist","title":"filter"},{"location":"reference/taskcat/_cfn/stack.html#index_4","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/taskcat/_cfn/stack.html#insert_4","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/taskcat/_cfn/stack.html#pop_4","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/taskcat/_cfn/stack.html#remove_4","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/taskcat/_cfn/stack.html#reverse_4","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/taskcat/_cfn/stack.html#sort_4","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/taskcat/_cfn/stack_url_helper.html","text":"Module taskcat._cfn.stack_url_helper Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. View Source \"\"\" Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \"\"\" import json import logging import os from pathlib import Path from urllib.parse import urlparse LOG = logging . getLogger ( __name__ ) class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @staticmethod def evaluate_fn_join ( expression ): \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @staticmethod def evaluate_fn_if ( expression ): \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\" Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths Variables LOG Classes StackURLHelper class StackURLHelper ( template_mappings = None , template_parameters = None , parameter_values = None ) View Source class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @ staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @ staticmethod def evaluate_fn_join ( expression ): \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @ staticmethod def evaluate_fn_if ( expression ): \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @ staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @ staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\" Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @ staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths Class variables MAX_DEPTH SUBSTITUTION Static methods evaluate_fn_getatt def evaluate_fn_getatt ( expression ) View Source @staticmethod def evaluate_fn_getatt ( expression ) : raise Exception ( \"Fn::GetAtt: not supported\" ) evaluate_fn_if def evaluate_fn_if ( expression ) Return both possible parts of the expression View Source @staticmethod def evaluate_fn_if ( expression ) : \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" ) [ 1 ] . strip () value_false = expression . split ( \",\" ) [ 2 ] . strip (). strip ( \"]\" ) # if we don 't have '' this can break things results.append(\"' \" + value_true.strip(\" '\") + \"' \") results.append(\" '\" + value_false.strip(\"' \") + \" '\" ) return results evaluate_fn_join def evaluate_fn_join ( expression ) Return the joined stuff View Source @staticmethod def evaluate_fn_join ( expression ) : \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" ) [ 1 ] delimiter = temp . split ( \",\" ) [ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" ) [ 2 ] values = values . split ( \"]]\" ) [ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results evaluate_fn_split def evaluate_fn_split ( expression ) View Source @staticmethod def evaluate_fn_split ( expression ) : raise Exception ( \"Fn::Split: not supported\" ) rewrite_sub_vars_with_values def rewrite_sub_vars_with_values ( expression , values ) Rewrite sub vars with actual variable values View Source @staticmethod def rewrite_sub_vars_with_values ( expression , values ) : \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ] ) + \"\" result = result . replace ( rep_text , rep_with ) return result values_to_dict def values_to_dict ( values ) Rewrite sub vars with actual variable values View Source @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict Methods evaluate_expression_controller def evaluate_expression_controller ( self , expression ) Figure out what type of expression and pass off to handler View Source def evaluate_expression_controller ( self , expression ) : \"\"\" Figure out what type of expression and pass off to handler \"\"\" results = [] if \" Fn::If \" in expression : results = self . evaluate_fn_if ( expression ) elif \" Fn::Sub \" in expression : results = self . evaluate_fn_sub ( expression ) elif \" Fn::Join \" in expression : results = self . evaluate_fn_join ( expression ) elif \" Ref \" in expression : results = self . evaluate_fn_ref ( expression ) elif \" Fn::FindInMap \" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \" Fn::GetAtt \" in expression : results = self . evaluate_fn_getatt ( expression ) elif \" Fn::Split \" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \" ( \" + expression + \" ) \" ) return results evaluate_fn_findinmap def evaluate_fn_findinmap ( self , expression ) View Source def evaluate_fn_findinmap ( self , expression ) : result = [] mappings_map = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 0 ]. strip () first_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 1 ]. strip () final_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 2 ]. strip () result . append ( \" ' \" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \" ' \" ) return result evaluate_fn_ref def evaluate_fn_ref ( self , expression ) Since this is runtime data the best we can do is the name in place View Source def evaluate_fn_ref ( self , expression ) : \"\"\" Since this is runtime data the best we can do is the name in place \"\"\" # TODO : Allow user to inject RunTime values for these results = [] temp = expression . split ( \" : \" ) [ 1 ] if temp . strip ( \" ' \" ) in self . SUBSTITUTION . keys () : temp = self . SUBSTITUTION [ temp . strip ( \" ' \" ) ] temp = \" ' \" + temp + \" ' \" results . append ( temp ) return results evaluate_fn_sub def evaluate_fn_sub ( self , expression ) Return expression with values replaced View Source def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results evaluate_string def evaluate_string ( self , template_url , depth = 0 ) Recursively find expressions in the URL and send them to be evaluated View Source def evaluate_string ( self , template_url , depth = 0 ) : \"\"\" Recursively find expressions in the URL and send them to be evaluated \"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \" Template URL contains more than {} levels or nesting \" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \" { \" in template_url : parts = template_url . split ( \" { \" ) parts = parts [ - 1 ]. split ( \" } \" ) # Last open bracket # This function will handle Fn :: Sub Fn :: If etc . replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \" { \" + parts [ 0 ] + \" } \" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls find_in_map_lookup def find_in_map_lookup ( self , mappings_map , first_key , final_key ) View Source def find_in_map_lookup ( self , mappings_map , first_key , final_key ) : step1 = self . mappings [ mappings_map . strip ( \" ' \" ) ] step2 = step1 [ first_key . strip ( \" ' \" ) ] result = step2 [ final_key . strip ( \" ' \" ) ] return result find_local_child_template def find_local_child_template ( self , parent_template_path , child_template_path ) View Source def find_local_child_template ( self , parent_template_path , child_template_path ) : final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \" /../ \" ) ) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) return \"\" flatten_template_url def flatten_template_url ( self , template_url ) Flatten template_url and return all permutations View Source def flatten_template_url ( self , template_url ) : \"\"\" Flatten template_url and return all permutations \"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO : figure where the ' is coming from output = urlparse ( str ( url . strip ( \" ' \" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print ( url_list ) # print ( path_list ) return path_list rewrite_sub_vars def rewrite_sub_vars ( self , original_string , depth = 1 ) Replace the '##var##' placeholders with 'var' View Source def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result rewrite_vars def rewrite_vars ( self , original_string , depth = 1 ) Replace the ${var} placeholders with ##var## View Source def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result template_url_to_path def template_url_to_path ( self , current_template_path , template_url ) View Source def template_url_to_path ( self , current_template_path , template_url , ) : child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO : Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"Stack Url Helper"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#module-taskcat_cfnstack_url_helper","text":"Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. View Source \"\"\" Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \"\"\" import json import logging import os from pathlib import Path from urllib.parse import urlparse LOG = logging . getLogger ( __name__ ) class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @staticmethod def evaluate_fn_join ( expression ): \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @staticmethod def evaluate_fn_if ( expression ): \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\" Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"Module taskcat._cfn.stack_url_helper"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#stackurlhelper","text":"class StackURLHelper ( template_mappings = None , template_parameters = None , parameter_values = None ) View Source class StackURLHelper : MAX_DEPTH = 20 # Handle at most 20 levels of nesting in TemplateURL expressions # TODO: Allow user to inject this # SUBSTITUTION = { # \"QSS3BucketName\": \"aws-quickstart\", # \"QSS3KeyPrefix\": \"QSS3KeyPrefix/\", # \"qss3KeyPrefix\": \"qss3KeyPrefix/\", # \"AWS::Region\": \"us-east-1\", # \"AWS::AccountId\": \"8888XXXX9999\", # } SUBSTITUTION = { \"AWS::Region\" : \"us-east-1\" , \"AWS::URLSuffix\" : \"amazonaws.com\" , \"AWS::AccountId\" : \"8888XXXX9999\" , } def __init__ ( self , template_mappings = None , template_parameters = None , parameter_values = None , ): if template_mappings : self . mappings = template_mappings else : self . mappings = {} if template_parameters : self . template_parameters = template_parameters else : self . template_parameters = {} if parameter_values : self . parameter_values = parameter_values else : self . parameter_values = {} default_parameters : dict = {} for parameter in self . template_parameters : properties = self . template_parameters . get ( parameter ) if \"Default\" in properties . keys (): default_parameters [ parameter ] = properties [ \"Default\" ] self . SUBSTITUTION . update ( default_parameters ) self . SUBSTITUTION . update ( self . parameter_values ) def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result @ staticmethod def rewrite_sub_vars_with_values ( expression , values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ]) + \"\" result = result . replace ( rep_text , rep_with ) return result @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results @ staticmethod def evaluate_fn_join ( expression ): \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" )[ 1 ] delimiter = temp . split ( \",\" )[ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" )[ 2 ] values = values . split ( \"]]\" )[ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results @ staticmethod def evaluate_fn_if ( expression ): \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" )[ 1 ] . strip () value_false = expression . split ( \",\" )[ 2 ] . strip () . strip ( \"]\" ) # if we don't have '' this can break things results . append ( \"'\" + value_true . strip ( \"'\" ) + \"'\" ) results . append ( \"'\" + value_false . strip ( \"'\" ) + \"'\" ) return results def evaluate_fn_ref ( self , expression ): \"\"\"Since this is runtime data the best we can do is the name in place\"\"\" # TODO: Allow user to inject RunTime values for these results = [] temp = expression . split ( \": \" )[ 1 ] if temp . strip ( \"'\" ) in self . SUBSTITUTION . keys (): temp = self . SUBSTITUTION [ temp . strip ( \"'\" )] temp = \"'\" + temp + \"'\" results . append ( temp ) return results def find_in_map_lookup ( self , mappings_map , first_key , final_key ): step1 = self . mappings [ mappings_map . strip ( \"'\" )] step2 = step1 [ first_key . strip ( \"'\" )] result = step2 [ final_key . strip ( \"'\" )] return result def evaluate_fn_findinmap ( self , expression ): result = [] mappings_map = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 0 ] . strip () first_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 1 ] . strip () final_key = expression . split ( \"[\" )[ 1 ] . split ( \"]\" )[ 0 ] . split ( \",\" )[ 2 ] . strip () result . append ( \"'\" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \"'\" ) return result @ staticmethod def evaluate_fn_getatt ( expression ): raise Exception ( \"Fn::GetAtt: not supported\" ) @ staticmethod def evaluate_fn_split ( expression ): raise Exception ( \"Fn::Split: not supported\" ) def evaluate_expression_controller ( self , expression ): \"\"\"Figure out what type of expression and pass off to handler\"\"\" results = [] if \"Fn::If\" in expression : results = self . evaluate_fn_if ( expression ) elif \"Fn::Sub\" in expression : results = self . evaluate_fn_sub ( expression ) elif \"Fn::Join\" in expression : results = self . evaluate_fn_join ( expression ) elif \"Ref\" in expression : results = self . evaluate_fn_ref ( expression ) elif \"Fn::FindInMap\" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \"Fn::GetAtt\" in expression : results = self . evaluate_fn_getatt ( expression ) elif \"Fn::Split\" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \"(\" + expression + \")\" ) return results def evaluate_string ( self , template_url , depth = 0 ): \"\"\"Recursively find expressions in the URL and send them to be evaluated\"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \"Template URL contains more than {} levels or nesting\" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \"{\" in template_url : parts = template_url . split ( \"{\" ) parts = parts [ - 1 ] . split ( \"}\" ) # Last open bracket # This function will handle Fn::Sub Fn::If etc. replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \"{\" + parts [ 0 ] + \"}\" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls def _flatten_template_controller ( self , template_url ): \"\"\" Recursively evaluate subs/ifs\"\"\" url_list = [] # Replace ${SOMEVAR} with ##SOMEVAR## so finding actual \"expressions\" is easier template_url_string = str ( template_url ) parts = template_url_string . split ( \"${\" ) if len ( parts ) > 1 : template_url_string = self . rewrite_vars ( template_url_string ) # Evaluate expressions recursively if \"{\" in template_url_string : replacements = self . evaluate_string ( template_url_string ) # first closed bracket for replacement in replacements : url_list . append ( replacement ) else : url_list . append ( template_url ) return url_list def flatten_template_url ( self , template_url ): \"\"\"Flatten template_url and return all permutations\"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO: figure where the ' is coming from output = urlparse ( str ( url . strip ( \"'\" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print(url_list) # print(path_list) return path_list @ staticmethod def _remove_one_level ( path_string ): result = path_string result = result . find ( \"/\" , 0 ) result = path_string [ result + 1 : len ( path_string )] return result def find_local_child_template ( self , parent_template_path , child_template_path ): final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \"/../\" ) ) # Get rid of any \"//\" child_template_path_tmp = os . path . normpath ( child_template_path ) while \"/\" in str ( child_template_path_tmp ): child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \"/\" . join ([ str ( project_root ), str ( child_template_path_tmp )]) ) if final_template_path . exists () and final_template_path . is_file (): return str ( final_template_path ) return \"\" def template_url_to_path ( self , current_template_path , template_url , ): child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO: Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"StackURLHelper"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#class-variables","text":"MAX_DEPTH SUBSTITUTION","title":"Class variables"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_getatt","text":"def evaluate_fn_getatt ( expression ) View Source @staticmethod def evaluate_fn_getatt ( expression ) : raise Exception ( \"Fn::GetAtt: not supported\" )","title":"evaluate_fn_getatt"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_if","text":"def evaluate_fn_if ( expression ) Return both possible parts of the expression View Source @staticmethod def evaluate_fn_if ( expression ) : \"\"\" Return both possible parts of the expression \"\"\" results = [] value_true = expression . split ( \",\" ) [ 1 ] . strip () value_false = expression . split ( \",\" ) [ 2 ] . strip (). strip ( \"]\" ) # if we don 't have '' this can break things results.append(\"' \" + value_true.strip(\" '\") + \"' \") results.append(\" '\" + value_false.strip(\"' \") + \" '\" ) return results","title":"evaluate_fn_if"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_join","text":"def evaluate_fn_join ( expression ) Return the joined stuff View Source @staticmethod def evaluate_fn_join ( expression ) : \"\"\" Return the joined stuff \"\"\" results = [] new_values_list = [] temp = expression . split ( \"[\" ) [ 1 ] delimiter = temp . split ( \",\" ) [ 0 ] . strip ( \"'\" ) values = expression . split ( \"[\" ) [ 2 ] values = values . split ( \"]]\" ) [ 0 ] values_list = values . split ( \", \" ) for value in values_list : new_values_list . append ( value . strip ( \"'\" )) result = delimiter . join ( new_values_list ) results . append ( result ) return results","title":"evaluate_fn_join"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_split","text":"def evaluate_fn_split ( expression ) View Source @staticmethod def evaluate_fn_split ( expression ) : raise Exception ( \"Fn::Split: not supported\" )","title":"evaluate_fn_split"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#rewrite_sub_vars_with_values","text":"def rewrite_sub_vars_with_values ( expression , values ) Rewrite sub vars with actual variable values View Source @staticmethod def rewrite_sub_vars_with_values ( expression , values ) : \"\"\"Rewrite sub vars with actual variable values\"\"\" result = expression # replace each key we have a value for for key in values : rep_text = \"##\" + key + \"##\" rep_with = \"\" + str ( values [ key ] ) + \"\" result = result . replace ( rep_text , rep_with ) return result","title":"rewrite_sub_vars_with_values"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#values_to_dict","text":"def values_to_dict ( values ) Rewrite sub vars with actual variable values View Source @ staticmethod def values_to_dict ( values ): \"\"\"Rewrite sub vars with actual variable values\"\"\" # Create dictionary of values values_dict_string = values . replace ( \"(\" , \"{\" ) values_dict_string = values_dict_string . replace ( \")\" , \"}\" ) values_dict_string = values_dict_string . replace ( \"'\" , '\"' ) # for values or keys not quoted # Split by : values_split_string = values_dict_string # Trim stuff so we can get the key values values_split_string = values_split_string . replace ( \" \" , \"\" ) values_split_string = values_split_string . replace ( \"{\" , \"\" ) values_split_string = values_split_string . replace ( \"}\" , \"\" ) values_split = values_split_string . split ( \",\" ) values_split_final = [] for value in values_split : values = value . split ( \":\" ) values_split_final . extend ( values ) for value in values_split_final : if value [ 0 ] != \"'\" and value [ - 1 ] != \"'\" : if value [ 0 ] != '\"' and value [ - 1 ] != '\"' : values_dict_string = values_dict_string . replace ( value , '\"' + value + '\"' ) values_dict = json . loads ( values_dict_string ) return values_dict","title":"values_to_dict"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_expression_controller","text":"def evaluate_expression_controller ( self , expression ) Figure out what type of expression and pass off to handler View Source def evaluate_expression_controller ( self , expression ) : \"\"\" Figure out what type of expression and pass off to handler \"\"\" results = [] if \" Fn::If \" in expression : results = self . evaluate_fn_if ( expression ) elif \" Fn::Sub \" in expression : results = self . evaluate_fn_sub ( expression ) elif \" Fn::Join \" in expression : results = self . evaluate_fn_join ( expression ) elif \" Ref \" in expression : results = self . evaluate_fn_ref ( expression ) elif \" Fn::FindInMap \" in expression : results = self . evaluate_fn_findinmap ( expression ) elif \" Fn::GetAtt \" in expression : results = self . evaluate_fn_getatt ( expression ) elif \" Fn::Split \" in expression : results = self . evaluate_fn_split ( expression ) else : # This is a NON expression repl { and } with ( and ) to break recursion results . append ( \" ( \" + expression + \" ) \" ) return results","title":"evaluate_expression_controller"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_findinmap","text":"def evaluate_fn_findinmap ( self , expression ) View Source def evaluate_fn_findinmap ( self , expression ) : result = [] mappings_map = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 0 ]. strip () first_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 1 ]. strip () final_key = expression . split ( \" [ \" ) [ 1 ]. split ( \" ] \" ) [ 0 ]. split ( \" , \" ) [ 2 ]. strip () result . append ( \" ' \" + self . find_in_map_lookup ( mappings_map , first_key , final_key ) + \" ' \" ) return result","title":"evaluate_fn_findinmap"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_ref","text":"def evaluate_fn_ref ( self , expression ) Since this is runtime data the best we can do is the name in place View Source def evaluate_fn_ref ( self , expression ) : \"\"\" Since this is runtime data the best we can do is the name in place \"\"\" # TODO : Allow user to inject RunTime values for these results = [] temp = expression . split ( \" : \" ) [ 1 ] if temp . strip ( \" ' \" ) in self . SUBSTITUTION . keys () : temp = self . SUBSTITUTION [ temp . strip ( \" ' \" ) ] temp = \" ' \" + temp + \" ' \" results . append ( temp ) return results","title":"evaluate_fn_ref"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_fn_sub","text":"def evaluate_fn_sub ( self , expression ) Return expression with values replaced View Source def evaluate_fn_sub ( self , expression ): \"\"\" Return expression with values replaced \"\"\" results = [] # Builtins - Fudge some defaults here since we don't have runtime info # ${AWS::Region} ${AWS::AccountId} expression = self . rewrite_sub_vars_with_values ( expression , self . SUBSTITUTION ) # Handle Sub of form [ StringToSub, { \"key\" : \"value\", \"key\": \"value\" }] if \"[\" in expression : temp_expression = expression . split ( \"[\" )[ 1 ] . split ( \",\" )[ 0 ] values = expression . split ( \"[\" )[ 1 ] . split ( \"(\" )[ 1 ] . split ( \")\" )[ 0 ] values = self . values_to_dict ( \"(\" + values + \")\" ) temp_expression = self . rewrite_sub_vars_with_values ( temp_expression , values ) else : temp_expression = expression . split ( \"': '\" )[ 1 ] . split ( \"'\" )[ 0 ] # if we still have them we just use their values (ie: Parameters) result = self . rewrite_sub_vars ( temp_expression ) results . append ( result ) return results","title":"evaluate_fn_sub"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#evaluate_string","text":"def evaluate_string ( self , template_url , depth = 0 ) Recursively find expressions in the URL and send them to be evaluated View Source def evaluate_string ( self , template_url , depth = 0 ) : \"\"\" Recursively find expressions in the URL and send them to be evaluated \"\"\" # Recursion bail out if depth > self . MAX_DEPTH : raise Exception ( \" Template URL contains more than {} levels or nesting \" . format ( self . MAX_DEPTH ) ) template_urls = [] # Evaluate expressions if \" { \" in template_url : parts = template_url . split ( \" { \" ) parts = parts [ - 1 ]. split ( \" } \" ) # Last open bracket # This function will handle Fn :: Sub Fn :: If etc . replacements = self . evaluate_expression_controller ( parts [ 0 ] ) # First closed bracket after for replacement in replacements : template_url_temp = template_url template_url_temp = template_url_temp . replace ( \" { \" + parts [ 0 ] + \" } \" , replacement ) evaluated_strings = self . evaluate_string ( template_url_temp , depth = ( depth + 1 ) ) for evaluated_string in evaluated_strings : template_urls . append ( evaluated_string ) else : template_urls . append ( template_url ) return template_urls","title":"evaluate_string"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#find_in_map_lookup","text":"def find_in_map_lookup ( self , mappings_map , first_key , final_key ) View Source def find_in_map_lookup ( self , mappings_map , first_key , final_key ) : step1 = self . mappings [ mappings_map . strip ( \" ' \" ) ] step2 = step1 [ first_key . strip ( \" ' \" ) ] result = step2 [ final_key . strip ( \" ' \" ) ] return result","title":"find_in_map_lookup"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#find_local_child_template","text":"def find_local_child_template ( self , parent_template_path , child_template_path ) View Source def find_local_child_template ( self , parent_template_path , child_template_path ) : final_template_path = \"\" # Start where the Parent template is project_root = Path ( os . path . dirname ( parent_template_path )) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) # Take the path piece by piece and try in current folder while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) # Take the path piece by piece and try in one folder up folder project_root = Path ( os . path . normpath ( os . path . dirname ( parent_template_path ) + \" /../ \" ) ) # Get rid of any \" // \" child_template_path_tmp = os . path . normpath ( child_template_path ) while \" / \" in str ( child_template_path_tmp ) : child_template_path_tmp = self . _remove_one_level ( child_template_path_tmp ) final_template_path = Path ( \" / \" . join ( [ str ( project_root ) , str ( child_template_path_tmp ) ] ) ) if final_template_path . exists () and final_template_path . is_file () : return str ( final_template_path ) return \"\"","title":"find_local_child_template"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#flatten_template_url","text":"def flatten_template_url ( self , template_url ) Flatten template_url and return all permutations View Source def flatten_template_url ( self , template_url ) : \"\"\" Flatten template_url and return all permutations \"\"\" path_list = [] url_list = self . _flatten_template_controller ( template_url ) # Extract the path portion from the URL for url in url_list : # TODO : figure where the ' is coming from output = urlparse ( str ( url . strip ( \" ' \" ))) path_list . append ( output . path ) path_list = list ( dict . fromkeys ( path_list )) # print ( url_list ) # print ( path_list ) return path_list","title":"flatten_template_url"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#rewrite_sub_vars","text":"def rewrite_sub_vars ( self , original_string , depth = 1 ) Replace the '##var##' placeholders with 'var' View Source def rewrite_sub_vars ( self , original_string , depth = 1 ): \"\"\"Replace the '##var##' placeholders with 'var'\"\"\" if \"##\" not in original_string : return original_string parts = original_string . split ( \"##\" ) parts = parts [ 1 ] . split ( \"##\" ) rep_text = \"##\" + parts [ 0 ] + \"##\" rep_with = \"\" + parts [ 0 ] + \"\" result = original_string . replace ( rep_text , rep_with ) if \"##\" in result : # Recurse if we have more variables result = self . rewrite_sub_vars ( result , depth = ( depth + 1 )) return result","title":"rewrite_sub_vars"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#rewrite_vars","text":"def rewrite_vars ( self , original_string , depth = 1 ) Replace the ${var} placeholders with ##var## View Source def rewrite_vars ( self , original_string , depth = 1 ): \"\"\"Replace the ${var} placeholders with ##var##\"\"\" parts = original_string . split ( \"${\" ) parts = parts [ 1 ] . split ( \"}\" ) rep_text = \"${\" + parts [ 0 ] + \"}\" rep_with = \"##\" + parts [ 0 ] + \"##\" result = original_string . replace ( rep_text , rep_with ) if len ( result . split ( \"${\" )) > 1 : result = self . rewrite_vars ( result , depth = ( depth + 1 )) return result","title":"rewrite_vars"},{"location":"reference/taskcat/_cfn/stack_url_helper.html#template_url_to_path","text":"def template_url_to_path ( self , current_template_path , template_url ) View Source def template_url_to_path ( self , current_template_path , template_url , ) : child_local_paths = [] child_template_paths = self . flatten_template_url ( template_url ) # TODO : Add logic to try for S3 paths for child_template_path in child_template_paths : child_local_paths . append ( self . find_local_child_template ( current_template_path , child_template_path ) ) return child_local_paths","title":"template_url_to_path"},{"location":"reference/taskcat/_cfn/template.html","text":"Module taskcat._cfn.template None None View Source import logging import re from pathlib import Path from time import sleep from typing import Dict , List , Union from yaml . scanner import ScannerError import cfnlint from taskcat . _ cfn . stack_url_helper import StackURLHelper from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class TemplateCache : def __ init__ ( self , store : dict = None ) : self . _ templates = store if store else {} self . _ lock : Dict [ str , bool ] = {} def get ( self , template_path: str ) -> cfnlint . Template : while self . _ lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _ templates : try : self . _ lock [ template_path ] = True try : self . _ templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _ lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _ lock [ template_path ] = False raise return self . _ templates [ template_path ] template_cache_store: Dict [ str , cfnlint . Template ] = {} tcat_template_cache = TemplateCache ( template_cache_store ) # pylint : disable = C0103 class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Variables LOG tcat_template_cache template_cache_store Classes Template class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x103f35850 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters Instance variables descendents linesplit s3_key s3_key_prefix Methods parameters def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ int , str ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters url_prefix def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix write def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children () TemplateCache class TemplateCache ( store : dict = None ) View Source class TemplateCache : def __init__ ( self , store : dict = None ) : self . _templates = store if store else {} self . _lock : Dict [ str, bool ] = {} def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ] Methods get def get ( self , template_path : str ) -> cfnlint . decorators . refactored . refactored .< locals >. cls_wrapper .< locals >. Wrapped View Source def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"Template"},{"location":"reference/taskcat/_cfn/template.html#module-taskcat_cfntemplate","text":"None None View Source import logging import re from pathlib import Path from time import sleep from typing import Dict , List , Union from yaml . scanner import ScannerError import cfnlint from taskcat . _ cfn . stack_url_helper import StackURLHelper from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class TemplateCache : def __ init__ ( self , store : dict = None ) : self . _ templates = store if store else {} self . _ lock : Dict [ str , bool ] = {} def get ( self , template_path: str ) -> cfnlint . Template : while self . _ lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _ templates : try : self . _ lock [ template_path ] = True try : self . _ templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _ lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _ lock [ template_path ] = False raise return self . _ templates [ template_path ] template_cache_store: Dict [ str , cfnlint . Template ] = {} tcat_template_cache = TemplateCache ( template_cache_store ) # pylint : disable = C0103 class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Module taskcat._cfn.template"},{"location":"reference/taskcat/_cfn/template.html#variables","text":"LOG tcat_template_cache template_cache_store","title":"Variables"},{"location":"reference/taskcat/_cfn/template.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/template.html#template","text":"class Template ( template_path : Union [ str , pathlib . Path ], project_root : Union [ str , pathlib . Path ] = '' , url : str = '' , s3_key_prefix : str = '' , template_cache : taskcat . _cfn . template . TemplateCache = < taskcat . _cfn . template . TemplateCache object at 0x103f35850 > ) View Source class Template : def __ init__ ( self , template_path: Union [ str , Path ], project_root: Union [ str , Path ] = \"\" , url : str = \"\" , s3_key_prefix: str = \"\" , template_cache: TemplateCache = tcat_template_cache , ) : self . template_cache = template_cache self . template_path: Path = Path ( template_path ). expanduser (). resolve () self . template = self . template_cache . get ( str ( self . template_path )) with open ( template_path , \"r\" ) as file_handle: self . raw_template = file_handle . read () project_root = ( project_root if project_root else self . template_path . parent . parent ) self . project_root = Path ( project_root ). expanduser (). resolve () self . url = url self . _ s3_key_prefix = s3_key_prefix self . children : List [ Template ] = [] self . _ find_children () def __ str__ ( self ) : return str ( self . template ) def __ repr__ ( self ) : return f \"<Template {self.template_path} at {hex(id(self))}>\" @property def s3_key ( self ) : suffix = str ( self . template_path . relative_to ( self . project_root ). as_posix ()) return self . _ s3_key_prefix + suffix @property def s3_key_prefix ( self ) : return self . _ s3_key_prefix @property def linesplit ( self ) : return self . raw_template . split ( \"\\n\" ) def write ( self ) : \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle: file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _ find_children () def _ template_url_to_path ( self , template_url , template_mappings = None , ) : try : helper = StackURLHelper ( template_mappings = template_mappings , template_parameters = self . template . get ( \"Parameters\" ), ) urls = helper . template_url_to_path ( current_template_path = self . template_path , template_url = template_url ) if len ( urls ) > 0 : return urls [ 0 ] except Exception as e : # pylint : disable = broad - except LOG . debug ( \"Traceback:\" , exc_info = True ) LOG . error ( \"TemplateURL parsing error: %s \" % str(e)) LOG . warning ( \"Failed to discover path for %s, path %s does not exist\" , template_url , None , ) return \"\" def _ get_relative_url ( self , path : str ) -> str : suffix = str ( path ). replace ( str ( self . project_root ), \"\" ) url = self . url_prefix () + suffix return url def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \")[0:-suffix_length]) return url_prefix def _find_children(self) -> None: # noqa: C901 children = set() if \" Resources \" not in self.template: raise TaskCatException( f\" did not receive a valid template : { self . template_path } does not \" f\" have a Resources section \" ) for resource in self.template[\" Resources \"].keys(): resource = self.template[\" Resources \"][resource] if resource[\" Type \"] == \" AWS :: CloudFormation :: Stack \": child_name = self._template_url_to_path( template_url=resource[\" Properties \"][\" TemplateURL \"], ) # print(child_name) if child_name: # for child_url in child_name: children.add(child_name) for child in children: child_template_instance = None for descendent in self.descendents: if str(descendent.template_path) == str(child): child_template_instance = descendent if not child_template_instance: try: child_template_instance = Template( child, self.project_root, self._get_relative_url(child), self._s3_key_prefix, tcat_template_cache, ) except Exception: # pylint: disable=broad-except LOG.debug(\" Traceback : \", exc_info=True) LOG.error(f\" Failed to add child template { child } \") if isinstance(child_template_instance, Template): self.children.append(child_template_instance) @property def descendents(self) -> List[\" Template \"]: desc_map = {} def recurse(template): for child in template.children: desc_map[str(child.template_path)] = child recurse(child) recurse(self) return list(desc_map.values()) def parameters( self, ) -> Dict[str, Union[None, str, int, bool, List[Union[int, str]]]]: parameters = {} for param_key, param in self.template.get(\" Parameters \", {}).items(): parameters[param_key] = param.get(\" Default \" ) return parameters","title":"Template"},{"location":"reference/taskcat/_cfn/template.html#instance-variables","text":"descendents linesplit s3_key s3_key_prefix","title":"Instance variables"},{"location":"reference/taskcat/_cfn/template.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/template.html#parameters","text":"def parameters ( self ) -> Dict [ str , Union [ NoneType , str , int , bool , List [ Union [ int , str ]]]] View Source def parameters ( self , ) -> Dict [ str, Union[None, str, int, bool, List[Union[int, str ] ]]]: parameters = {} for param_key , param in self . template . get ( \"Parameters\" , {} ). items () : parameters [ param_key ] = param . get ( \"Default\" ) return parameters","title":"parameters"},{"location":"reference/taskcat/_cfn/template.html#url_prefix","text":"def url_prefix ( self ) -> str View Source def url_prefix ( self ) -> str : if not self . url : return \"\" regionless_url = re . sub ( r \" \\.s3\\. (. * ) \\.amazonaws\\.com \", \" . s3 . amazonaws . com \", self.url, ) suffix = str(self.template_path).replace(str(self.project_root), \"\") suffix_length = len(suffix.lstrip(\" / \").split(\" / \")) url_prefix = \" / \".join(regionless_url.split(\" / \" )[ 0 :- suffix_length ]) return url_prefix","title":"url_prefix"},{"location":"reference/taskcat/_cfn/template.html#write","text":"def write ( self ) writes raw_template back to file, and reloads decoded template, useful if the template has been modified View Source def write ( self ): \"\"\"writes raw_template back to file, and reloads decoded template, useful if the template has been modified\"\"\" with open ( str ( self . template_path ), \"w\" ) as file_handle : file_handle . write ( self . raw_template ) self . template = cfnlint . decode . cfn_yaml . load ( self . template_path ) self . _find_children ()","title":"write"},{"location":"reference/taskcat/_cfn/template.html#templatecache","text":"class TemplateCache ( store : dict = None ) View Source class TemplateCache : def __init__ ( self , store : dict = None ) : self . _templates = store if store else {} self . _lock : Dict [ str, bool ] = {} def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"TemplateCache"},{"location":"reference/taskcat/_cfn/template.html#methods_1","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/template.html#get","text":"def get ( self , template_path : str ) -> cfnlint . decorators . refactored . refactored .< locals >. cls_wrapper .< locals >. Wrapped View Source def get ( self , template_path : str ) -> cfnlint . Template : while self . _lock . get ( template_path ) : sleep ( 0.1 ) if template_path not in self . _templates : try : self . _lock [ template_path ] = True try : self . _templates [ template_path ] = cfnlint . decode . cfn_yaml . load ( template_path ) except ScannerError as e : LOG . error ( f \"Failed to parse template {template_path} {e.problem} at \" f \"{e.problem_mark}\" ) raise self . _lock [ template_path ] = False except Exception : # pylint : disable = broad - except self . _lock [ template_path ] = False raise return self . _templates [ template_path ]","title":"get"},{"location":"reference/taskcat/_cfn/threaded.html","text":"Module taskcat._cfn.threaded None None View Source import logging import uuid from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List import boto3 from taskcat._cfn.stack import Stack , Stacks , StackStatus from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_dicts from taskcat._dataclasses import Tag , TestObj , TestRegion from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str , TestObj ], uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ): self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str , TestObj ]): return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ): if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ({ \"Key\" : \"taskcat-id\" , \"Value\" : self . uid . hex })] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \"taskcat-project-name\" , \"taskcat-test-name\" , \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags }, tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ): stack_name = test . stack_name tags . append ( Tag ({ \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name })) tags . append ( Tag ({ \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name })) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ): raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ): if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ): fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _delete_stack ( stack : Stack ): stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results : for status in region : statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ): return fan_out ( self . _status , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _status ( stack : Stack ): for status_group in [ \"COMPLETE\" , \"IN_PROGRESS\" , \"FAILED\" ]: if stack . status in getattr ( StackStatus , status_group ): return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack { stack } \" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ): return { stack . id : stack . events () . filter ( criteria )} def resources ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs }, self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _resources , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ): return { stack . id : stack . resources () . filter ( criteria )} @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str , TestObj ], include_deleted = False , recurse = False , threads = 32 , ): if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3 . client , List [ TestRegion ]] = {} for test in tests . values (): for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests }, clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ([ item for sublist in results for item in sublist ]) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ): # pylint: disable=too-many-locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ], test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ]: stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client [ r ] for r in stacks_by_client ] @staticmethod def list_stacks ( profiles , regions ): stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache ()}, profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ): stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile }, regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ): stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" )[ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k . startswith ( \"taskcat-\" ): stack [ k ] = v if stack . get ( \"taskcat-id\" ): stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ]) stacks . append ( stack ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to fetch stacks for region { region } using profile \" f \" { profile } { type ( e ) } { e } \" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks Variables LOG Functions fan_out def fan_out ( func , partial_kwargs , payload , threads ) View Source def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results Classes Stacker class Stacker ( project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], uid : uuid . UUID = UUID ( '00000000-0000-0000-0000-000000000000' ), stack_name_prefix : str = 'tCaT' , shorten_stack_name : bool = False , tags : list = None ) View Source class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str, TestObj ] , uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ) : self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str, TestObj ] ) : return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag({\"Key\": \"taskcat-id\", \"Value\": self.uid.hex}) ] tags += [ Tag(t) for t in self.tags if t.key not in [\"taskcat-project-name\", \"taskcat-test-name\", \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags } , tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ) : stack_name = test . stack_name tags . append ( Tag ( { \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name } )) tags . append ( Tag ( { \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name } )) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ) : raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ) : fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _delete_stack ( stack : Stack ) : stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str, dict ] = { \"IN_PROGRESS\" : {} , \"COMPLETE\" : {} , \"FAILED\" : {}} for region in results : for status in region : statuses [ status[1 ] ] [ status[0 ] ] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ) : return fan_out ( self . _status , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _status ( stack : Stack ) : for status_group in [ \"COMPLETE\", \"IN_PROGRESS\", \"FAILED\" ] : if stack . status in getattr ( StackStatus , status_group ) : return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack {stack}\" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs } , per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ) : return { stack . id : stack . events (). filter ( criteria ) } def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs } , self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _resources , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ) : return { stack . id : stack . resources (). filter ( criteria ) } @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ) : # pylint : disable = too - many - locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ] , test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ] : stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client[r ] for r in stacks_by_client ] @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ) : stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile } , regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ) : stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" ) [ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k . startswith ( \"taskcat-\" ) : stack [ k ] = v if stack . get ( \"taskcat-id\" ) : stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ] ) stacks . append ( stack ) except Exception as e : # pylint : disable = broad - except LOG . warning ( f \"Failed to fetch stacks for region {region} using profile \" f \"{profile} {type(e)} {e}\" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks Class variables NULL_UUID Static methods from_existing def from_existing ( uid : uuid . UUID , project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], include_deleted = False , recurse = False , threads = 32 ) View Source @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker list_stacks def list_stacks ( profiles , regions ) View Source @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] Methods create_stacks def create_stacks ( self , threads : int = 8 ) View Source def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \" Stacker already initialised with stack objects \" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ( { \" Key \" : \" taskcat-id \" , \" Value \" : self . uid . hex } ) ] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \" taskcat-project-name \" , \" taskcat-test-name \" , \" taskcat-id \" ] ] fan_out ( self . _create_stacks_for_test , { \" tags \" : tags }, tests , threads ) delete_stacks def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) View Source def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \" deep delete not yet implemented \" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )) , threads , ) events def events ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def events ( self , recurse = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) resources def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \" recurse not implemented \" ) results = fan_out ( self . _resources_per_client , { \" criteria \" : kwargs }, self . _group_stacks ( self . stacks ) , threads , ) return merge_dicts ( results ) status def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) View Source def status ( self , recurse: bool = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses: Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results: for status in region: statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses update_stacks def update_stacks ( self ) View Source def update_stacks(self): raise NotImplementedError()","title":"Threaded"},{"location":"reference/taskcat/_cfn/threaded.html#module-taskcat_cfnthreaded","text":"None None View Source import logging import uuid from functools import partial from multiprocessing.dummy import Pool as ThreadPool from typing import Dict , List import boto3 from taskcat._cfn.stack import Stack , Stacks , StackStatus from taskcat._client_factory import Boto3Cache from taskcat._common_utils import merge_dicts from taskcat._dataclasses import Tag , TestObj , TestRegion from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str , TestObj ], uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ): self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str , TestObj ]): return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ): if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ({ \"Key\" : \"taskcat-id\" , \"Value\" : self . uid . hex })] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \"taskcat-project-name\" , \"taskcat-test-name\" , \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags }, tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ): stack_name = test . stack_name tags . append ( Tag ({ \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name })) tags . append ( Tag ({ \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name })) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ): raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ): if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ): fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _delete_stack ( stack : Stack ): stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results : for status in region : statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ): return fan_out ( self . _status , None , stacks [ \"Stacks\" ], threads ) @staticmethod def _status ( stack : Stack ): for status_group in [ \"COMPLETE\" , \"IN_PROGRESS\" , \"FAILED\" ]: if stack . status in getattr ( StackStatus , status_group ): return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack { stack } \" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ): return { stack . id : stack . events () . filter ( criteria )} def resources ( self , recurse = False , threads : int = 32 , ** kwargs ): if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs }, self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ): results = fan_out ( self . _resources , { \"criteria\" : criteria }, stacks [ \"Stacks\" ], threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ): return { stack . id : stack . resources () . filter ( criteria )} @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str , TestObj ], include_deleted = False , recurse = False , threads = 32 , ): if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3 . client , List [ TestRegion ]] = {} for test in tests . values (): for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests }, clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ([ item for sublist in results for item in sublist ]) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ): # pylint: disable=too-many-locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ], test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ]: stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client [ r ] for r in stacks_by_client ] @staticmethod def list_stacks ( profiles , regions ): stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache ()}, profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ): stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile }, regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ): stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ) . paginate (): for stack_props in page [ \"Stacks\" ]: if stack_props . get ( \"ParentId\" ): continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" )[ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ]: k , v = ( tag [ \"Key\" ], tag [ \"Value\" ]) if k . startswith ( \"taskcat-\" ): stack [ k ] = v if stack . get ( \"taskcat-id\" ): stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ]) stacks . append ( stack ) except Exception as e : # pylint: disable=broad-except LOG . warning ( f \"Failed to fetch stacks for region { region } using profile \" f \" { profile } { type ( e ) } { e } \" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks","title":"Module taskcat._cfn.threaded"},{"location":"reference/taskcat/_cfn/threaded.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cfn/threaded.html#functions","text":"","title":"Functions"},{"location":"reference/taskcat/_cfn/threaded.html#fan_out","text":"def fan_out ( func , partial_kwargs , payload , threads ) View Source def fan_out ( func , partial_kwargs , payload , threads ): pool = ThreadPool ( threads ) if partial_kwargs : func = partial ( func , ** partial_kwargs ) results = pool . map ( func , payload ) pool . close () pool . join () return results","title":"fan_out"},{"location":"reference/taskcat/_cfn/threaded.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cfn/threaded.html#stacker","text":"class Stacker ( project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], uid : uuid . UUID = UUID ( '00000000-0000-0000-0000-000000000000' ), stack_name_prefix : str = 'tCaT' , shorten_stack_name : bool = False , tags : list = None ) View Source class Stacker : NULL_UUID = uuid . UUID ( int = 0 ) def __init__ ( self , project_name : str , tests : Dict [ str, TestObj ] , uid : uuid . UUID = NULL_UUID , stack_name_prefix : str = \"tCaT\" , shorten_stack_name : bool = False , tags : list = None , ) : self . tests = tests self . project_name = project_name self . stack_name_prefix = stack_name_prefix self . shorten_stack_name = shorten_stack_name self . tags = tags if tags else [] self . uid = uuid . uuid4 () if uid == Stacker . NULL_UUID else uid self . stacks : Stacks = Stacks () @staticmethod def _tests_to_list ( tests : Dict [ str, TestObj ] ) : return list ( tests . values ()) def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \"Stacker already initialised with stack objects\" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag({\"Key\": \"taskcat-id\", \"Value\": self.uid.hex}) ] tags += [ Tag(t) for t in self.tags if t.key not in [\"taskcat-project-name\", \"taskcat-test-name\", \"taskcat-id\" ] ] fan_out ( self . _create_stacks_for_test , { \"tags\" : tags } , tests , threads ) def _create_stacks_for_test ( self , test , tags , threads : int = 32 ) : stack_name = test . stack_name tags . append ( Tag ( { \"Key\" : \"taskcat-project-name\" , \"Value\" : self . project_name } )) tags . append ( Tag ( { \"Key\" : \"taskcat-test-name\" , \"Value\" : test . name } )) tags += test . tags partial_kwargs = { \"stack_name\" : stack_name , \"template\" : test . template , \"tags\" : tags , \"test_name\" : test . name , } stacks = fan_out ( Stack . create , partial_kwargs , test . regions , threads ) self . stacks += stacks # Not used by tCat at present def update_stacks ( self ) : raise NotImplementedError () def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \"deep delete not yet implemented\" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )), threads , ) def _delete_stacks_per_client ( self , stacks , threads = 8 ) : fan_out ( self . _delete_stack , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _delete_stack ( stack : Stack ) : stack . delete ( stack_id = stack . id , client = stack . client ) stack . refresh () def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses : Dict [ str, dict ] = { \"IN_PROGRESS\" : {} , \"COMPLETE\" : {} , \"FAILED\" : {}} for region in results : for status in region : statuses [ status[1 ] ] [ status[0 ] ] = status [ 2 ] return statuses def _status_per_client ( self , stacks , threads : int = 8 ) : return fan_out ( self . _status , None , stacks [ \"Stacks\" ] , threads ) @staticmethod def _status ( stack : Stack ) : for status_group in [ \"COMPLETE\", \"IN_PROGRESS\", \"FAILED\" ] : if stack . status in getattr ( StackStatus , status_group ) : return stack . id , status_group , stack . status_reason raise TaskCatException ( f \"Invalid stack {stack}\" ) def events ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs } , per_region_stacks , threads ) return merge_dicts ( results ) def _events_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _describe_stack_events , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads , ) return merge_dicts ( results ) @staticmethod def _describe_stack_events ( stack : Stack , criteria ) : return { stack . id : stack . events (). filter ( criteria ) } def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \"recurse not implemented\" ) results = fan_out ( self . _resources_per_client , { \"criteria\" : kwargs } , self . _group_stacks ( self . stacks ), threads , ) return merge_dicts ( results ) def _resources_per_client ( self , stacks , criteria , threads : int = 8 ) : results = fan_out ( self . _resources , { \"criteria\" : criteria } , stacks [ \"Stacks\" ] , threads ) return merge_dicts ( results ) @staticmethod def _resources ( stack : Stack , criteria ) : return { stack . id : stack . resources (). filter ( criteria ) } @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker @staticmethod def _import_stacks_per_client ( clients , uid , project_name , tests ) : # pylint : disable = too - many - locals stacks = Stacks () client , region = clients for page in client . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue match = False project = \"\" test = \"\" for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k == \"taskcat-id\" and v == uid . hex : match = True elif k == \"taskcat-test-name\" and v in tests : test = v elif k == \"taskcat-project-name\" and v == project_name : project = v if match and test and project : stack = Stack . import_existing ( stack_props , tests [ test ] . template , region [ 0 ] , test , uid , ) stacks . append ( stack ) return stacks @staticmethod def _group_stacks ( stacks : Stacks ) -> List [ dict ] : stacks_by_client : dict = {} for stack in stacks : client = stack . client if client not in stacks_by_client : stacks_by_client [ client ] = { \"Client\" : client , \"Stacks\" : []} stacks_by_client [ client ][ \"Stacks\" ] . append ( stack ) return [ stacks_by_client[r ] for r in stacks_by_client ] @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _list_per_profile ( profile , regions , boto_cache ) : stacks = fan_out ( Stacker . _get_taskcat_stacks , { \"boto_cache\" : boto_cache , \"profile\" : profile } , regions , threads = len ( regions ), ) return [ stack for sublist in stacks for stack in sublist ] @staticmethod def _get_taskcat_stacks ( region , boto_cache : Boto3Cache , profile : str ) : stacks = [] try : cfn = boto_cache . client ( \"cloudformation\" , profile = profile , region = region ) for page in cfn . get_paginator ( \"describe_stacks\" ). paginate () : for stack_props in page [ \"Stacks\" ] : if stack_props . get ( \"ParentId\" ) : continue stack_id = stack_props [ \"StackId\" ] stack_name = stack_id . split ( \"/\" ) [ 1 ] stack = { \"region\" : region , \"profile\" : profile , \"stack-id\" : stack_id , \"stack-name\" : stack_name , } for tag in stack_props [ \"Tags\" ] : k , v = ( tag [ \"Key\" ] , tag [ \"Value\" ] ) if k . startswith ( \"taskcat-\" ) : stack [ k ] = v if stack . get ( \"taskcat-id\" ) : stack [ \"taskcat-id\" ] = uuid . UUID ( stack [ \"taskcat-id\" ] ) stacks . append ( stack ) except Exception as e : # pylint : disable = broad - except LOG . warning ( f \"Failed to fetch stacks for region {region} using profile \" f \"{profile} {type(e)} {e}\" ) LOG . debug ( \"Traceback:\" , exc_info = True ) return stacks","title":"Stacker"},{"location":"reference/taskcat/_cfn/threaded.html#class-variables","text":"NULL_UUID","title":"Class variables"},{"location":"reference/taskcat/_cfn/threaded.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cfn/threaded.html#from_existing","text":"def from_existing ( uid : uuid . UUID , project_name : str , tests : Dict [ str , taskcat . _dataclasses . TestObj ], include_deleted = False , recurse = False , threads = 32 ) View Source @classmethod def from_existing ( cls , uid : uuid . UUID , project_name : str , tests : Dict [ str, TestObj ] , include_deleted = False , recurse = False , threads = 32 , ) : if include_deleted : raise NotImplementedError ( \"including deleted stacks not implemented\" ) if recurse : raise NotImplementedError ( \"recurse not implemented\" ) clients : Dict [ boto3.client, List[TestRegion ] ] = {} for test in tests . values () : for region in test . regions : client = region . client ( \"cloudformation\" ) if client not in clients : clients [ client ] = [] clients [ client ] . append ( region ) results = fan_out ( Stacker . _import_stacks_per_client , { \"uid\" : uid , \"project_name\" : project_name , \"tests\" : tests } , clients . items (), threads , ) stacker = Stacker ( project_name , tests , uid ) stacker . stacks = Stacks ( [ item for sublist in results for item in sublist ] ) return stacker","title":"from_existing"},{"location":"reference/taskcat/_cfn/threaded.html#list_stacks","text":"def list_stacks ( profiles , regions ) View Source @staticmethod def list_stacks ( profiles , regions ) : stacks = fan_out ( Stacker . _list_per_profile , { \"regions\" : regions , \"boto_cache\" : Boto3Cache () } , profiles , threads = 8 , ) return [ stack for sublist in stacks for stack in sublist ]","title":"list_stacks"},{"location":"reference/taskcat/_cfn/threaded.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cfn/threaded.html#create_stacks","text":"def create_stacks ( self , threads : int = 8 ) View Source def create_stacks ( self , threads : int = 8 ) : if self . stacks : raise TaskCatException ( \" Stacker already initialised with stack objects \" ) tests = self . _tests_to_list ( self . tests ) tags = [ Tag ( { \" Key \" : \" taskcat-id \" , \" Value \" : self . uid . hex } ) ] tags += [ Tag ( t ) for t in self . tags if t . key not in [ \" taskcat-project-name \" , \" taskcat-test-name \" , \" taskcat-id \" ] ] fan_out ( self . _create_stacks_for_test , { \" tags \" : tags }, tests , threads )","title":"create_stacks"},{"location":"reference/taskcat/_cfn/threaded.html#delete_stacks","text":"def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) View Source def delete_stacks ( self , criteria : dict = None , deep = False , threads = 32 ) : if deep : raise NotImplementedError ( \" deep delete not yet implemented \" ) fan_out ( self . _delete_stacks_per_client , None , self . _group_stacks ( self . stacks . filter ( criteria )) , threads , )","title":"delete_stacks"},{"location":"reference/taskcat/_cfn/threaded.html#events","text":"def events ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def events ( self , recurse = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) per_region_stacks = self . _group_stacks ( self . stacks ) results = fan_out ( self . _events_per_client , { \"criteria\" : kwargs }, per_region_stacks , threads ) return merge_dicts ( results )","title":"events"},{"location":"reference/taskcat/_cfn/threaded.html#resources","text":"def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) View Source def resources ( self , recurse = False , threads : int = 32 , ** kwargs ) : if recurse : raise NotImplementedError ( \" recurse not implemented \" ) results = fan_out ( self . _resources_per_client , { \" criteria \" : kwargs }, self . _group_stacks ( self . stacks ) , threads , ) return merge_dicts ( results )","title":"resources"},{"location":"reference/taskcat/_cfn/threaded.html#status","text":"def status ( self , recurse : bool = False , threads : int = 32 , ** kwargs ) View Source def status ( self , recurse: bool = False , threads: int = 32 , ** kwargs ) : if recurse: raise NotImplementedError ( \"recurse not implemented\" ) stacks = self . stacks . filter ( kwargs ) per_region_stacks = self . _group_stacks ( stacks ) results = fan_out ( self . _status_per_client , None , per_region_stacks , threads ) statuses: Dict [ str , dict ] = { \"IN_PROGRESS\" : {}, \"COMPLETE\" : {}, \"FAILED\" : {}} for region in results: for status in region: statuses [ status [ 1 ]][ status [ 0 ]] = status [ 2 ] return statuses","title":"status"},{"location":"reference/taskcat/_cfn/threaded.html#update_stacks","text":"def update_stacks ( self ) View Source def update_stacks(self): raise NotImplementedError()","title":"update_stacks"},{"location":"reference/taskcat/_cli_modules/index.html","text":"Module taskcat._cli_modules None None View Source from .delete import Delete # noqa: F401 from .deploy import Deploy # noqa: F401 from .lint import Lint # noqa: F401 from .list import List # noqa: F401 from .package import Package # noqa: F401 from .test import Test # noqa: F401 from .update_ami import UpdateAMI # noqa: F401 from .upload import Upload # noqa: F401 Sub-modules taskcat._cli_modules.delete taskcat._cli_modules.deploy taskcat._cli_modules.lint taskcat._cli_modules.list taskcat._cli_modules.package taskcat._cli_modules.test taskcat._cli_modules.update_ami taskcat._cli_modules.upload","title":"Index"},{"location":"reference/taskcat/_cli_modules/index.html#module-taskcat_cli_modules","text":"None None View Source from .delete import Delete # noqa: F401 from .deploy import Deploy # noqa: F401 from .lint import Lint # noqa: F401 from .list import List # noqa: F401 from .package import Package # noqa: F401 from .test import Test # noqa: F401 from .update_ami import UpdateAMI # noqa: F401 from .upload import Upload # noqa: F401","title":"Module taskcat._cli_modules"},{"location":"reference/taskcat/_cli_modules/index.html#sub-modules","text":"taskcat._cli_modules.delete taskcat._cli_modules.deploy taskcat._cli_modules.lint taskcat._cli_modules.list taskcat._cli_modules.package taskcat._cli_modules.test taskcat._cli_modules.update_ami taskcat._cli_modules.upload","title":"Sub-modules"},{"location":"reference/taskcat/_cli_modules/delete.html","text":"Module taskcat._cli_modules.delete None None View Source # pylint: disable=duplicate-code import logging from taskcat._cfn.stack import Stack from taskcat._cfn.threaded import Stacker from taskcat._client_factory import Boto3Cache LOG = logging . getLogger ( __name__ ) class Delete : \"\"\"[ALPHA] Deletes an installed package in an AWS account/region\"\"\" def __init__ ( self , package : str , aws_profile : str = \"default\" , region = \"default\" , _stack_type = \"package\" , ): \"\"\" :param package: installed package to delete, can be an install name or uuid :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will use aws cli configured default \"\"\" LOG . warning ( \"delete is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if region == \"default\" : region = boto3_cache . get_default_region ( aws_profile ) if isinstance ( region , str ): region = [ region ] stacks = Stacker . list_stacks ([ aws_profile ], region ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ]) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ], \"test_name\" : stack [ \"taskcat-test-name\" ], \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ], \"type\" : \"package\" if stack . get ( \"taskcat-installer\" ) else \"test\" , \"stack_id\" : stack [ \"stack-id\" ], } if _stack_type == job [ \"type\" ]: if package in [ job [ \"name\" ], job [ \"taskcat_id\" ], \"ALL\" ]: jobs . append ( job ) # TODO: concurrency and wait for complete for job in jobs : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ]) Variables LOG Classes Delete class Delete ( package : str , aws_profile : str = 'default' , region = 'default' , _stack_type = 'package' ) View Source class Delete : \"\"\"[ALPHA] Deletes an installed package in an AWS account/region\"\"\" def __init__ ( self , package : str , aws_profile : str = \"default\" , region = \"default\" , _stack_type = \"package\" , ) : \"\"\" :param package: installed package to delete, can be an install name or uuid :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will use aws cli configured default \"\"\" LOG . warning ( \"delete is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if region == \"default\" : region = boto3_cache . get_default_region ( aws_profile ) if isinstance ( region , str ) : region = [ region ] stacks = Stacker . list_stacks ( [ aws_profile ] , region ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ] ) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"test_name\" : stack [ \"taskcat-test-name\" ] , \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ] , \"type\" : \"package\" if stack . get ( \"taskcat-installer\" ) else \"test\" , \"stack_id\" : stack [ \"stack-id\" ] , } if _stack_type == job [ \"type\" ] : if package in [ job[\"name\" ] , job [ \"taskcat_id\" ] , \"ALL\" ]: jobs . append ( job ) # TODO : concurrency and wait for complete for job in jobs : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ] )","title":"Delete"},{"location":"reference/taskcat/_cli_modules/delete.html#module-taskcat_cli_modulesdelete","text":"None None View Source # pylint: disable=duplicate-code import logging from taskcat._cfn.stack import Stack from taskcat._cfn.threaded import Stacker from taskcat._client_factory import Boto3Cache LOG = logging . getLogger ( __name__ ) class Delete : \"\"\"[ALPHA] Deletes an installed package in an AWS account/region\"\"\" def __init__ ( self , package : str , aws_profile : str = \"default\" , region = \"default\" , _stack_type = \"package\" , ): \"\"\" :param package: installed package to delete, can be an install name or uuid :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will use aws cli configured default \"\"\" LOG . warning ( \"delete is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if region == \"default\" : region = boto3_cache . get_default_region ( aws_profile ) if isinstance ( region , str ): region = [ region ] stacks = Stacker . list_stacks ([ aws_profile ], region ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ]) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ], \"test_name\" : stack [ \"taskcat-test-name\" ], \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ], \"type\" : \"package\" if stack . get ( \"taskcat-installer\" ) else \"test\" , \"stack_id\" : stack [ \"stack-id\" ], } if _stack_type == job [ \"type\" ]: if package in [ job [ \"name\" ], job [ \"taskcat_id\" ], \"ALL\" ]: jobs . append ( job ) # TODO: concurrency and wait for complete for job in jobs : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ])","title":"Module taskcat._cli_modules.delete"},{"location":"reference/taskcat/_cli_modules/delete.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/delete.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/delete.html#delete","text":"class Delete ( package : str , aws_profile : str = 'default' , region = 'default' , _stack_type = 'package' ) View Source class Delete : \"\"\"[ALPHA] Deletes an installed package in an AWS account/region\"\"\" def __init__ ( self , package : str , aws_profile : str = \"default\" , region = \"default\" , _stack_type = \"package\" , ) : \"\"\" :param package: installed package to delete, can be an install name or uuid :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will use aws cli configured default \"\"\" LOG . warning ( \"delete is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if region == \"default\" : region = boto3_cache . get_default_region ( aws_profile ) if isinstance ( region , str ) : region = [ region ] stacks = Stacker . list_stacks ( [ aws_profile ] , region ) jobs = [] for stack in stacks : name = stack . get ( \"taskcat-installer\" , stack [ \"taskcat-project-name\" ] ) job = { \"name\" : name , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"test_name\" : stack [ \"taskcat-test-name\" ] , \"taskcat_id\" : stack [ \"taskcat-id\" ] . hex , \"region\" : stack [ \"region\" ] , \"type\" : \"package\" if stack . get ( \"taskcat-installer\" ) else \"test\" , \"stack_id\" : stack [ \"stack-id\" ] , } if _stack_type == job [ \"type\" ] : if package in [ job[\"name\" ] , job [ \"taskcat_id\" ] , \"ALL\" ]: jobs . append ( job ) # TODO : concurrency and wait for complete for job in jobs : client = boto3_cache . client ( \"cloudformation\" , profile = aws_profile , region = job [ \"region\" ] ) Stack . delete ( client = client , stack_id = job [ \"stack_id\" ] )","title":"Delete"},{"location":"reference/taskcat/_cli_modules/deploy.html","text":"Module taskcat._cli_modules.deploy None None View Source # pylint : disable = duplicate - code import logging from io import BytesIO from pathlib import Path from time import sleep from dulwich import porcelain from dulwich . config import ConfigFile , parse_submodules from taskcat . _ cfn . threaded import Stacker from taskcat . _ client_factory import Boto3Cache from taskcat . _ config import Config from taskcat . _ dataclasses import Tag from taskcat . _ name_generator import generate_name from taskcat . _ s3_stage import stage_in_s3 from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/region\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def __ init__ ( # noqa : C901 self , package : str , aws_profile: str = \"default\" , region= \"default\" , parameters= \"\" , name= \"\" , wait = False , ) : \"\"\" :param package: name of package to install can be a path to a local package, a github org/repo, or an AWS Quick Start name :param aws_profile: aws profile to use for installation :param region: regions to install into, default will use aws cli configured default :param parameters: parameters to pass to the stack, in the format Key=Value,AnotherKey=AnotherValue or providing a path to a json or yaml file containing the parameters :param name: stack name to use, if not specified one will be automatically generated :param wait: if enabled, taskcat will wait for stack to complete before exiting \"\"\" LOG . warning ( \"deploy is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if not name : name = generate_name () if region == \"default\" : region = boto3_cache . get_default_region ( profile_name = aws_profile ) path = Path ( package ). resolve () if Path ( package ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in package : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" package = f\"aws-quickstart/quickstart-{package}\" if package_type == \"github\": if package.startswith(\"https://\") or package.startswith(\"git@\"): url = package org, repo = ( package.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = package.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) config = Config.create( args={\"project\": {\"regions\": [region]}}, project_config_path=(path / \".taskcat.yml\"), project_root=path, ) # only use one region for test_name in config.config.tests: config.config.tests[test_name].regions = config.config.project.regions # if there's no test called default , take the 1 st in the list if \"default\" not in config . config . tests : config . config . tests [ \"default\" ] = config . config . tests [ list ( config . config . tests . keys ())[ 0 ] ] # until install offers a way to run different \"plans\" we only need one test for test_name in list ( config . config . tests . keys ()) : if test_name ! = \"default\" : del config . config . tests [ test_name ] buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , path ) regions = config . get_regions ( boto3_cache ) templates = config . get_templates () parameters = config . get_rendered_parameters ( buckets , regions , templates ) tests = config . get_tests ( templates , regions , buckets , parameters ) tags = [ Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name })] stacks = Stacker ( config . config . project . name , tests , tags = tags ) stacks . create_stacks () LOG . error ( f \" {stacks.uid.hex}\" , extra= { \"nametag\" : \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_ID ] \\x1b [ 0 m \"}, ) LOG.error(f\" { name } \", extra={\" nametag \": \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_NAME ] \\x1b [ 0 m \"}) if wait: LOG.info( f\" waiting for stack { stacks . stacks [ 0 ]. name } to complete in \" f\" { stacks . stacks [ 0 ]. region_name } \" ) while stacks.status()[\" IN_PROGRESS \"]: sleep(5) if stacks.status()[\" FAILED \"]: LOG.error(\" Install failed : \") for error in stacks.stacks[0].error_events(): LOG.error(f\" { error . logical_id } : { error . status_reason } \") raise TaskCatException(\" Stack creation failed \") @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \" path already exists , updating from remote is not yet implemented \" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\" utf - 8 \")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \" . gitmodules \" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\" utf - 8 \") url = url.decode(\" utf - 8 \") name = name.decode(\" utf - 8 \") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule... sha = None try: porcelain.get_object_by_path(str(path), sub_path) except KeyError as e: sha = e.args[0].decode(\" utf - 8 \") if not sha: raise ValueError(f\" Could not find sha for submodule { name } \") if url.startswith(\" .. / \"): base_url = parent_url for _ in range(url.count(\" .. / \")): base_url = \" / \".join(base_url.split(\" / \")[:-1]) url = base_url + \" / \" + url.replace(\" .. / \", \"\") outp = BytesIO() if not (path / sub_path / \" . git \").is_dir(): LOG.info(f\" fetching git submodule { url } \") porcelain.clone( url, str(path / sub_path), checkout=sha, errstream=outp, outstream=outp, ) LOG.debug(outp.getvalue().decode(\" utf - 8 \" )) self . _ recurse_submodules (( path / sub_path ), url ) Variables LOG Classes Deploy class Deploy ( package : str , aws_profile : str = 'default' , region = 'default' , parameters = '' , name = '' , wait = False ) View Source class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/region\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def __ init__ ( # noqa : C901 self , package : str , aws_profile: str = \"default\" , region= \"default\" , parameters= \"\" , name= \"\" , wait = False , ) : \"\"\" :param package: name of package to install can be a path to a local package, a github org/repo, or an AWS Quick Start name :param aws_profile: aws profile to use for installation :param region: regions to install into, default will use aws cli configured default :param parameters: parameters to pass to the stack, in the format Key=Value,AnotherKey=AnotherValue or providing a path to a json or yaml file containing the parameters :param name: stack name to use, if not specified one will be automatically generated :param wait: if enabled, taskcat will wait for stack to complete before exiting \"\"\" LOG . warning ( \"deploy is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if not name : name = generate_name () if region == \"default\" : region = boto3_cache . get_default_region ( profile_name = aws_profile ) path = Path ( package ). resolve () if Path ( package ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in package : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" package = f\"aws-quickstart/quickstart-{package}\" if package_type == \"github\": if package.startswith(\"https://\") or package.startswith(\"git@\"): url = package org, repo = ( package.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = package.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) config = Config.create( args={\"project\": {\"regions\": [region]}}, project_config_path=(path / \".taskcat.yml\"), project_root=path, ) # only use one region for test_name in config.config.tests: config.config.tests[test_name].regions = config.config.project.regions # if there's no test called default , take the 1 st in the list if \"default\" not in config . config . tests : config . config . tests [ \"default\" ] = config . config . tests [ list ( config . config . tests . keys ())[ 0 ] ] # until install offers a way to run different \"plans\" we only need one test for test_name in list ( config . config . tests . keys ()) : if test_name ! = \"default\" : del config . config . tests [ test_name ] buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , path ) regions = config . get_regions ( boto3_cache ) templates = config . get_templates () parameters = config . get_rendered_parameters ( buckets , regions , templates ) tests = config . get_tests ( templates , regions , buckets , parameters ) tags = [ Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name })] stacks = Stacker ( config . config . project . name , tests , tags = tags ) stacks . create_stacks () LOG . error ( f \" {stacks.uid.hex}\" , extra= { \"nametag\" : \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_ID ] \\x1b [ 0 m \"}, ) LOG.error(f\" { name } \", extra={\" nametag \": \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_NAME ] \\x1b [ 0 m \"}) if wait: LOG.info( f\" waiting for stack { stacks . stacks [ 0 ]. name } to complete in \" f\" { stacks . stacks [ 0 ]. region_name } \" ) while stacks.status()[\" IN_PROGRESS \"]: sleep(5) if stacks.status()[\" FAILED \"]: LOG.error(\" Install failed : \") for error in stacks.stacks[0].error_events(): LOG.error(f\" { error . logical_id } : { error . status_reason } \") raise TaskCatException(\" Stack creation failed \") @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \" path already exists , updating from remote is not yet implemented \" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\" utf - 8 \")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \" . gitmodules \" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\" utf - 8 \") url = url.decode(\" utf - 8 \") name = name.decode(\" utf - 8 \") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule... sha = None try: porcelain.get_object_by_path(str(path), sub_path) except KeyError as e: sha = e.args[0].decode(\" utf - 8 \") if not sha: raise ValueError(f\" Could not find sha for submodule { name } \") if url.startswith(\" .. / \"): base_url = parent_url for _ in range(url.count(\" .. / \")): base_url = \" / \".join(base_url.split(\" / \")[:-1]) url = base_url + \" / \" + url.replace(\" .. / \", \"\") outp = BytesIO() if not (path / sub_path / \" . git \").is_dir(): LOG.info(f\" fetching git submodule { url } \") porcelain.clone( url, str(path / sub_path), checkout=sha, errstream=outp, outstream=outp, ) LOG.debug(outp.getvalue().decode(\" utf - 8 \" )) self . _ recurse_submodules (( path / sub_path ), url ) Class variables PKG_CACHE_PATH","title":"Deploy"},{"location":"reference/taskcat/_cli_modules/deploy.html#module-taskcat_cli_modulesdeploy","text":"None None View Source # pylint : disable = duplicate - code import logging from io import BytesIO from pathlib import Path from time import sleep from dulwich import porcelain from dulwich . config import ConfigFile , parse_submodules from taskcat . _ cfn . threaded import Stacker from taskcat . _ client_factory import Boto3Cache from taskcat . _ config import Config from taskcat . _ dataclasses import Tag from taskcat . _ name_generator import generate_name from taskcat . _ s3_stage import stage_in_s3 from taskcat . exceptions import TaskCatException LOG = logging . getLogger ( __ name__ ) class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/region\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def __ init__ ( # noqa : C901 self , package : str , aws_profile: str = \"default\" , region= \"default\" , parameters= \"\" , name= \"\" , wait = False , ) : \"\"\" :param package: name of package to install can be a path to a local package, a github org/repo, or an AWS Quick Start name :param aws_profile: aws profile to use for installation :param region: regions to install into, default will use aws cli configured default :param parameters: parameters to pass to the stack, in the format Key=Value,AnotherKey=AnotherValue or providing a path to a json or yaml file containing the parameters :param name: stack name to use, if not specified one will be automatically generated :param wait: if enabled, taskcat will wait for stack to complete before exiting \"\"\" LOG . warning ( \"deploy is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if not name : name = generate_name () if region == \"default\" : region = boto3_cache . get_default_region ( profile_name = aws_profile ) path = Path ( package ). resolve () if Path ( package ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in package : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" package = f\"aws-quickstart/quickstart-{package}\" if package_type == \"github\": if package.startswith(\"https://\") or package.startswith(\"git@\"): url = package org, repo = ( package.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = package.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) config = Config.create( args={\"project\": {\"regions\": [region]}}, project_config_path=(path / \".taskcat.yml\"), project_root=path, ) # only use one region for test_name in config.config.tests: config.config.tests[test_name].regions = config.config.project.regions # if there's no test called default , take the 1 st in the list if \"default\" not in config . config . tests : config . config . tests [ \"default\" ] = config . config . tests [ list ( config . config . tests . keys ())[ 0 ] ] # until install offers a way to run different \"plans\" we only need one test for test_name in list ( config . config . tests . keys ()) : if test_name ! = \"default\" : del config . config . tests [ test_name ] buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , path ) regions = config . get_regions ( boto3_cache ) templates = config . get_templates () parameters = config . get_rendered_parameters ( buckets , regions , templates ) tests = config . get_tests ( templates , regions , buckets , parameters ) tags = [ Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name })] stacks = Stacker ( config . config . project . name , tests , tags = tags ) stacks . create_stacks () LOG . error ( f \" {stacks.uid.hex}\" , extra= { \"nametag\" : \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_ID ] \\x1b [ 0 m \"}, ) LOG.error(f\" { name } \", extra={\" nametag \": \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_NAME ] \\x1b [ 0 m \"}) if wait: LOG.info( f\" waiting for stack { stacks . stacks [ 0 ]. name } to complete in \" f\" { stacks . stacks [ 0 ]. region_name } \" ) while stacks.status()[\" IN_PROGRESS \"]: sleep(5) if stacks.status()[\" FAILED \"]: LOG.error(\" Install failed : \") for error in stacks.stacks[0].error_events(): LOG.error(f\" { error . logical_id } : { error . status_reason } \") raise TaskCatException(\" Stack creation failed \") @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \" path already exists , updating from remote is not yet implemented \" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\" utf - 8 \")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \" . gitmodules \" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\" utf - 8 \") url = url.decode(\" utf - 8 \") name = name.decode(\" utf - 8 \") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule... sha = None try: porcelain.get_object_by_path(str(path), sub_path) except KeyError as e: sha = e.args[0].decode(\" utf - 8 \") if not sha: raise ValueError(f\" Could not find sha for submodule { name } \") if url.startswith(\" .. / \"): base_url = parent_url for _ in range(url.count(\" .. / \")): base_url = \" / \".join(base_url.split(\" / \")[:-1]) url = base_url + \" / \" + url.replace(\" .. / \", \"\") outp = BytesIO() if not (path / sub_path / \" . git \").is_dir(): LOG.info(f\" fetching git submodule { url } \") porcelain.clone( url, str(path / sub_path), checkout=sha, errstream=outp, outstream=outp, ) LOG.debug(outp.getvalue().decode(\" utf - 8 \" )) self . _ recurse_submodules (( path / sub_path ), url )","title":"Module taskcat._cli_modules.deploy"},{"location":"reference/taskcat/_cli_modules/deploy.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/deploy.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/deploy.html#deploy","text":"class Deploy ( package : str , aws_profile : str = 'default' , region = 'default' , parameters = '' , name = '' , wait = False ) View Source class Deploy : \"\"\"[ALPHA] installs a stack into an AWS account/region\"\"\" PKG_CACHE_PATH = Path ( \"~/.taskcat_package_cache/\" ). expanduser (). resolve () # pylint : disable = too - many - branches , too - many - locals def __ init__ ( # noqa : C901 self , package : str , aws_profile: str = \"default\" , region= \"default\" , parameters= \"\" , name= \"\" , wait = False , ) : \"\"\" :param package: name of package to install can be a path to a local package, a github org/repo, or an AWS Quick Start name :param aws_profile: aws profile to use for installation :param region: regions to install into, default will use aws cli configured default :param parameters: parameters to pass to the stack, in the format Key=Value,AnotherKey=AnotherValue or providing a path to a json or yaml file containing the parameters :param name: stack name to use, if not specified one will be automatically generated :param wait: if enabled, taskcat will wait for stack to complete before exiting \"\"\" LOG . warning ( \"deploy is in alpha feature, use with caution\" ) boto3_cache = Boto3Cache () if not name : name = generate_name () if region == \"default\" : region = boto3_cache . get_default_region ( profile_name = aws_profile ) path = Path ( package ). resolve () if Path ( package ). resolve (). is_dir () : package_type = \"local\" elif \"/\" in package : package_type = \"github\" else : # assuming it's an AWS Quick Start package_type = \"github\" package = f\"aws-quickstart/quickstart-{package}\" if package_type == \"github\": if package.startswith(\"https://\") or package.startswith(\"git@\"): url = package org, repo = ( package.replace(\".git\", \"\").replace(\":\", \"/\").split(\"/\")[-2:] ) else: org, repo = package.split(\"/\") url = f\"https://github.com/{org}/{repo}.git\" path = Deploy.PKG_CACHE_PATH / org / repo LOG.info(f\"fetching git repo {url}\") self._git_clone(url, path) self._recurse_submodules(path, url) config = Config.create( args={\"project\": {\"regions\": [region]}}, project_config_path=(path / \".taskcat.yml\"), project_root=path, ) # only use one region for test_name in config.config.tests: config.config.tests[test_name].regions = config.config.project.regions # if there's no test called default , take the 1 st in the list if \"default\" not in config . config . tests : config . config . tests [ \"default\" ] = config . config . tests [ list ( config . config . tests . keys ())[ 0 ] ] # until install offers a way to run different \"plans\" we only need one test for test_name in list ( config . config . tests . keys ()) : if test_name ! = \"default\" : del config . config . tests [ test_name ] buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , path ) regions = config . get_regions ( boto3_cache ) templates = config . get_templates () parameters = config . get_rendered_parameters ( buckets , regions , templates ) tests = config . get_tests ( templates , regions , buckets , parameters ) tags = [ Tag ({ \"Key\" : \"taskcat-installer\" , \"Value\" : name })] stacks = Stacker ( config . config . project . name , tests , tags = tags ) stacks . create_stacks () LOG . error ( f \" {stacks.uid.hex}\" , extra= { \"nametag\" : \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_ID ] \\x1b [ 0 m \"}, ) LOG.error(f\" { name } \", extra={\" nametag \": \" \\x1b [ 0 ; 30 ; 47 m [ INSTALL_NAME ] \\x1b [ 0 m \"}) if wait: LOG.info( f\" waiting for stack { stacks . stacks [ 0 ]. name } to complete in \" f\" { stacks . stacks [ 0 ]. region_name } \" ) while stacks.status()[\" IN_PROGRESS \"]: sleep(5) if stacks.status()[\" FAILED \"]: LOG.error(\" Install failed : \") for error in stacks.stacks[0].error_events(): LOG.error(f\" { error . logical_id } : { error . status_reason } \") raise TaskCatException(\" Stack creation failed \") @staticmethod def _git_clone(url, path): outp = BytesIO() if path.exists(): # TODO: handle updating existing repo LOG.warning( \" path already exists , updating from remote is not yet implemented \" ) # shutil.rmtree(path) if not path.exists(): path.mkdir(parents=True) porcelain.clone( url, str(path), checkout=True, errstream=outp, outstream=outp ) LOG.debug(outp.getvalue().decode(\" utf - 8 \")) def _recurse_submodules(self, path: Path, parent_url): gitmodule_path = path / \" . gitmodules \" if not gitmodule_path.is_file(): return conf = ConfigFile.from_path(str(gitmodule_path)) for sub_path, url, name in parse_submodules(conf): sub_path = sub_path.decode(\" utf - 8 \") url = url.decode(\" utf - 8 \") name = name.decode(\" utf - 8 \") if not (path / sub_path).is_dir(): (path / sub_path).mkdir(parents=True) # bizarre process here, but I don't know how else to get the sha for the # submodule... sha = None try: porcelain.get_object_by_path(str(path), sub_path) except KeyError as e: sha = e.args[0].decode(\" utf - 8 \") if not sha: raise ValueError(f\" Could not find sha for submodule { name } \") if url.startswith(\" .. / \"): base_url = parent_url for _ in range(url.count(\" .. / \")): base_url = \" / \".join(base_url.split(\" / \")[:-1]) url = base_url + \" / \" + url.replace(\" .. / \", \"\") outp = BytesIO() if not (path / sub_path / \" . git \").is_dir(): LOG.info(f\" fetching git submodule { url } \") porcelain.clone( url, str(path / sub_path), checkout=sha, errstream=outp, outstream=outp, ) LOG.debug(outp.getvalue().decode(\" utf - 8 \" )) self . _ recurse_submodules (( path / sub_path ), url )","title":"Deploy"},{"location":"reference/taskcat/_cli_modules/deploy.html#class-variables","text":"PKG_CACHE_PATH","title":"Class variables"},{"location":"reference/taskcat/_cli_modules/lint.html","text":"Module taskcat._cli_modules.lint None None View Source import logging from pathlib import Path from taskcat._cfn_lint import Lint as TaskCatLint from taskcat._config import Config from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class Lint : \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file : str = \".taskcat.yml\" , project_root : str = \"./\" , strict : bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) Variables LOG Classes Lint class Lint ( input_file : str = '.taskcat.yml' , project_root : str = './' , strict : bool = False ) View Source class Lint: \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file: str = \".taskcat.yml\" , project_root: str = \"./\" , strict: bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () input_file_path: Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed: raise TaskCatException ( \"Lint failed with errors\" )","title":"Lint"},{"location":"reference/taskcat/_cli_modules/lint.html#module-taskcat_cli_moduleslint","text":"None None View Source import logging from pathlib import Path from taskcat._cfn_lint import Lint as TaskCatLint from taskcat._config import Config from taskcat.exceptions import TaskCatException LOG = logging . getLogger ( __name__ ) class Lint : \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file : str = \".taskcat.yml\" , project_root : str = \"./\" , strict : bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" )","title":"Module taskcat._cli_modules.lint"},{"location":"reference/taskcat/_cli_modules/lint.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/lint.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/lint.html#lint","text":"class Lint ( input_file : str = '.taskcat.yml' , project_root : str = './' , strict : bool = False ) View Source class Lint: \"\"\"checks CloudFormation templates for issues using cfn-python-lint\"\"\" def __init__ ( self , input_file: str = \".taskcat.yml\" , project_root: str = \"./\" , strict: bool = False , ): \"\"\" :param input_file: path to project config or CloudFormation template :param project_root: base path for project :param strict: fail on lint warnings as well as errors \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () input_file_path: Path = project_root_path / input_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) templates = config . get_templates () lint = TaskCatLint ( config , templates , strict ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed: raise TaskCatException ( \"Lint failed with errors\" )","title":"Lint"},{"location":"reference/taskcat/_cli_modules/list.html","text":"Module taskcat._cli_modules.list None None View Source # pylint: disable=duplicate-code import logging from typing import List as ListType , Union import boto3 from taskcat._cfn.threaded import Stacker LOG = logging . getLogger ( __name__ ) class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint: disable=too-many-locals,too-many-branches def __init__ ( # noqa: C901 self , profiles : Union [ str , ListType [ str ]] = \"default\" , regions = \"ALL\" , _stack_type = \"package\" , ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" LOG . warning ( \"list is in alpha feature, use with caution\" ) if isinstance ( profiles , str ): profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if _stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif name and _stack_type == \"package\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 def longest ( things : list ): lengths = [ len ( thing ) for thing in things ] return sorted ( lengths )[ - 1 ] if lengths else 0 def spaces ( number ): ret = \"\" for _ in range ( number ): ret += \" \" return ret def pad ( string , length ): while len ( string ) < length : string += \" \" return string longest_name = longest ([ v [ \"name\" ] for _ , v in jobs . items ()]) longest_project_name = longest ([ v [ \"project_name\" ] for _ , v in jobs . items ()]) if not jobs : LOG . info ( \"no stacks found\" ) return if _stack_type != \"test\" : header = ( f \"NAME { spaces ( longest_name ) } PROJECT { spaces ( longest_project_name ) } \" f \"ID { spaces ( 34 ) } REGION\" ) column = \" {} {} {} {} \" else : header = f \"NAME { spaces ( longest_name ) } ID { spaces ( 34 ) } REGION\" column = \" {} {} {} \" LOG . error ( header , extra = { \"nametag\" : \"\" }) for job in jobs . values (): args = [ pad ( job [ \"name\" ], longest_name ), pad ( job [ \"project_name\" ], longest_project_name ), job [ \"id\" ], job [ \"region\" ], ] if _stack_type == \"test\" : args = [ pad ( job [ \"name\" ], longest_name ), job [ \"id\" ], job [ \"region\" ]] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" }) Variables LOG Classes List class List ( profiles : Union [ str , List [ str ]] = 'default' , regions = 'ALL' , _stack_type = 'package' ) View Source class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint : disable = too - many - locals , too - many - branches def __init__ ( # noqa : C901 self , profiles : Union [ str, ListType[str ] ] = \"default\" , regions = \"ALL\" , _stack_type = \"package\" , ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" LOG . warning ( \"list is in alpha feature, use with caution\" ) if isinstance ( profiles , str ) : profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if _stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif name and _stack_type == \"package\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 def longest ( things : list ) : lengths = [ len(thing) for thing in things ] return sorted ( lengths ) [ -1 ] if lengths else 0 def spaces ( number ) : ret = \"\" for _ in range ( number ) : ret += \" \" return ret def pad ( string , length ) : while len ( string ) < length : string += \" \" return string longest_name = longest ( [ v[\"name\" ] for _ , v in jobs . items () ] ) longest_project_name = longest ( [ v[\"project_name\" ] for _ , v in jobs . items () ] ) if not jobs : LOG . info ( \"no stacks found\" ) return if _stack_type != \"test\" : header = ( f \"NAME{spaces(longest_name)}PROJECT{spaces(longest_project_name)}\" f \"ID{spaces(34)}REGION\" ) column = \"{} {} {} {}\" else : header = f \"NAME{spaces(longest_name)}ID{spaces(34)}REGION\" column = \"{} {} {}\" LOG . error ( header , extra = { \"nametag\" : \"\" } ) for job in jobs . values () : args = [ pad(job[\"name\" ] , longest_name ), pad ( job [ \"project_name\" ] , longest_project_name ), job [ \"id\" ] , job [ \"region\" ] , ] if _stack_type == \"test\" : args = [ pad(job[\"name\" ] , longest_name ), job [ \"id\" ] , job [ \"region\" ] ] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" } )","title":"List"},{"location":"reference/taskcat/_cli_modules/list.html#module-taskcat_cli_moduleslist","text":"None None View Source # pylint: disable=duplicate-code import logging from typing import List as ListType , Union import boto3 from taskcat._cfn.threaded import Stacker LOG = logging . getLogger ( __name__ ) class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint: disable=too-many-locals,too-many-branches def __init__ ( # noqa: C901 self , profiles : Union [ str , ListType [ str ]] = \"default\" , regions = \"ALL\" , _stack_type = \"package\" , ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" LOG . warning ( \"list is in alpha feature, use with caution\" ) if isinstance ( profiles , str ): profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if _stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } elif name and _stack_type == \"package\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ], \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ], } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 def longest ( things : list ): lengths = [ len ( thing ) for thing in things ] return sorted ( lengths )[ - 1 ] if lengths else 0 def spaces ( number ): ret = \"\" for _ in range ( number ): ret += \" \" return ret def pad ( string , length ): while len ( string ) < length : string += \" \" return string longest_name = longest ([ v [ \"name\" ] for _ , v in jobs . items ()]) longest_project_name = longest ([ v [ \"project_name\" ] for _ , v in jobs . items ()]) if not jobs : LOG . info ( \"no stacks found\" ) return if _stack_type != \"test\" : header = ( f \"NAME { spaces ( longest_name ) } PROJECT { spaces ( longest_project_name ) } \" f \"ID { spaces ( 34 ) } REGION\" ) column = \" {} {} {} {} \" else : header = f \"NAME { spaces ( longest_name ) } ID { spaces ( 34 ) } REGION\" column = \" {} {} {} \" LOG . error ( header , extra = { \"nametag\" : \"\" }) for job in jobs . values (): args = [ pad ( job [ \"name\" ], longest_name ), pad ( job [ \"project_name\" ], longest_project_name ), job [ \"id\" ], job [ \"region\" ], ] if _stack_type == \"test\" : args = [ pad ( job [ \"name\" ], longest_name ), job [ \"id\" ], job [ \"region\" ]] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" })","title":"Module taskcat._cli_modules.list"},{"location":"reference/taskcat/_cli_modules/list.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/list.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/list.html#list","text":"class List ( profiles : Union [ str , List [ str ]] = 'default' , regions = 'ALL' , _stack_type = 'package' ) View Source class List : \"\"\"[ALPHA] lists taskcat jobs with active stacks\"\"\" # pylint : disable = too - many - locals , too - many - branches def __init__ ( # noqa : C901 self , profiles : Union [ str, ListType[str ] ] = \"default\" , regions = \"ALL\" , _stack_type = \"package\" , ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" LOG . warning ( \"list is in alpha feature, use with caution\" ) if isinstance ( profiles , str ) : profiles = profiles . split ( \",\" ) if regions == \"ALL\" : region_set : set = set () for profile in profiles : region_set = region_set . union ( set ( boto3 . Session ( profile_name = profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = regions . split ( \",\" ) stacks = Stacker . list_stacks ( profiles , regions ) jobs : dict = {} for stack in stacks : stack_key = stack [ \"taskcat-id\" ] . hex + \"-\" + stack [ \"region\" ] if stack_key not in jobs : name = stack . get ( \"taskcat-installer\" ) if _stack_type == \"test\" and not name : name = stack [ \"taskcat-project-name\" ] jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } elif name and _stack_type == \"package\" : jobs [ stack_key ] = { \"name\" : name , \"id\" : stack [ \"taskcat-id\" ] . hex , \"project_name\" : stack [ \"taskcat-project-name\" ] , \"active_stacks\" : 1 , \"region\" : stack [ \"region\" ] , } else : jobs [ stack_key ][ \"active_stacks\" ] += 1 def longest ( things : list ) : lengths = [ len(thing) for thing in things ] return sorted ( lengths ) [ -1 ] if lengths else 0 def spaces ( number ) : ret = \"\" for _ in range ( number ) : ret += \" \" return ret def pad ( string , length ) : while len ( string ) < length : string += \" \" return string longest_name = longest ( [ v[\"name\" ] for _ , v in jobs . items () ] ) longest_project_name = longest ( [ v[\"project_name\" ] for _ , v in jobs . items () ] ) if not jobs : LOG . info ( \"no stacks found\" ) return if _stack_type != \"test\" : header = ( f \"NAME{spaces(longest_name)}PROJECT{spaces(longest_project_name)}\" f \"ID{spaces(34)}REGION\" ) column = \"{} {} {} {}\" else : header = f \"NAME{spaces(longest_name)}ID{spaces(34)}REGION\" column = \"{} {} {}\" LOG . error ( header , extra = { \"nametag\" : \"\" } ) for job in jobs . values () : args = [ pad(job[\"name\" ] , longest_name ), pad ( job [ \"project_name\" ] , longest_project_name ), job [ \"id\" ] , job [ \"region\" ] , ] if _stack_type == \"test\" : args = [ pad(job[\"name\" ] , longest_name ), job [ \"id\" ] , job [ \"region\" ] ] LOG . error ( column . format ( * args ), extra = { \"nametag\" : \"\" } )","title":"List"},{"location":"reference/taskcat/_cli_modules/package.html","text":"Module taskcat._cli_modules.package None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat._lambda_build import LambdaBuild LOG = logging . getLogger ( __name__ ) class Package : \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root : str = \"./\" , source_folder : str = \"lambda_functions/source\" , zip_folder : str = \"lambda_functions/packages\" , config_file : str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () project_config : Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args = { \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda : LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path ) Variables LOG Classes Package class Package ( project_root : str = './' , source_folder : str = 'lambda_functions/source' , zip_folder : str = 'lambda_functions/packages' , config_file : str = '.taskcat.yml' ) View Source class Package: \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root: str = \"./\" , source_folder: str = \"lambda_functions/source\" , zip_folder: str = \"lambda_functions/packages\" , config_file: str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () project_config: Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args ={ \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda: LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Package"},{"location":"reference/taskcat/_cli_modules/package.html#module-taskcat_cli_modulespackage","text":"None None View Source import logging from pathlib import Path from taskcat._config import Config from taskcat._lambda_build import LambdaBuild LOG = logging . getLogger ( __name__ ) class Package : \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root : str = \"./\" , source_folder : str = \"lambda_functions/source\" , zip_folder : str = \"lambda_functions/packages\" , config_file : str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () project_config : Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args = { \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda : LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Module taskcat._cli_modules.package"},{"location":"reference/taskcat/_cli_modules/package.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/package.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/package.html#package","text":"class Package ( project_root : str = './' , source_folder : str = 'lambda_functions/source' , zip_folder : str = 'lambda_functions/packages' , config_file : str = '.taskcat.yml' ) View Source class Package: \"\"\"packages lambda source files into zip files. If a dockerfile is present in a source folder, it will be run prior to zipping the contents\"\"\" def __init__ ( self , project_root: str = \"./\" , source_folder: str = \"lambda_functions/source\" , zip_folder: str = \"lambda_functions/packages\" , config_file: str = \".taskcat.yml\" , ): \"\"\" :param project_root: base path for project :param source_folder: folder containing the lambda source files, relative to the project_root :param zip_folder: folder to output zip files, relative to the project root :param config_file: path to taskcat project config file \"\"\" project_root_path: Path = Path ( project_root ). expanduser (). resolve () project_config: Path = project_root_path / config_file config = Config . create ( project_config_path = project_config , project_root = project_root_path , args ={ \"project\" : { \"lambda_zip_path\" : zip_folder , \"lambda_source_path\" : source_folder , } }, ) if not config . config . project . package_lambda: LOG . info ( \"Lambda packaging disabled by config\" ) return LambdaBuild ( config , project_root_path )","title":"Package"},{"location":"reference/taskcat/_cli_modules/test.html","text":"Module taskcat._cli_modules.test None None View Source # pylint: disable=duplicate-code # noqa: B950,F841 import inspect import logging from pathlib import Path import boto3 import yaml from taskcat._common_utils import determine_profile_for_region from taskcat._config import Config from taskcat._tui import TerminalPrinter from taskcat.testing import CFNTest from .delete import Delete from .list import List LOG = logging . getLogger ( __name__ ) class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint: disable=too-many-locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ): \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ) . client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name )[ \"StackEvents\" ] resource = [ i for i in events if i [ \"LogicalResourceId\" ] == resource_name ][ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ]) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" )[ 4 :] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ]: cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ]) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ) . wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) @staticmethod # pylint: disable=too-many-arguments,W0613,line-too-long def run ( # noqa: C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ): \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa: B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ): setattr ( test , i , values [ i ]) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory ) def resume ( self , run_id ): # pylint: disable=no-self-use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ): \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint: disable=duplicate-code set ( boto3 . Session ( profile_name = aws_profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" ) Variables LOG Classes Test class Test ( / , * args , ** kwargs ) View Source class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint : disable = too - many - locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory ) def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" ) Static methods clean def clean ( project : str , aws_profile : str = 'default' , region = 'ALL' ) Parameters: Name Type Description Default project None project to delete, can be an name or uuid, or ALL to clean all tests None aws_profile None aws profile to use for deletion None region None region to delete from, default will scan all regions None View Source @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" ) list def list ( profiles : str = 'default' , regions = 'ALL' , _stack_type = 'package' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" ) retry def retry ( region : str , stack_name : str , resource_name : str , config_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False ) [ALPHA] re-launches a child stack using the same parameters as previous launch Parameters: Name Type Description Default region None region stack is in None stack_name None name of parent stack None resource_name None logical id of child stack that will be re-launched None config_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None keep_failed None do not delete failed stacks None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None View Source @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) run def run ( test_names : str = 'ALL' , regions : str = 'ALL' , input_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = './taskcat_outputs' , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False ) tests whether CloudFormation templates are able to successfully launch Parameters: Name Type Description Default test_names None comma separated list of tests to run None regions None comma separated list of regions to test in None input_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None lint_disable None disable cfn-lint checks None enable_sig_v2 None enable legacy sigv2 requests for auto-created buckets None keep_failed None do not delete failed stacks None output_directory None Where to store generated logfiles None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None skip_upload None Use templates in an existing cloudformation bucket. None View Source @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory ) Methods resume def resume ( self , run_id ) resumes a monitoring of a previously started test run View Source def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\" resumes a monitoring of a previously started test run \"\"\" # do some stuff raise NotImplementedError ()","title":"Test"},{"location":"reference/taskcat/_cli_modules/test.html#module-taskcat_cli_modulestest","text":"None None View Source # pylint: disable=duplicate-code # noqa: B950,F841 import inspect import logging from pathlib import Path import boto3 import yaml from taskcat._common_utils import determine_profile_for_region from taskcat._config import Config from taskcat._tui import TerminalPrinter from taskcat.testing import CFNTest from .delete import Delete from .list import List LOG = logging . getLogger ( __name__ ) class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint: disable=too-many-locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ): \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ) . client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name )[ \"StackEvents\" ] resource = [ i for i in events if i [ \"LogicalResourceId\" ] == resource_name ][ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ]) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" )[ 4 :] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ]: cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ]) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ) . wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) @staticmethod # pylint: disable=too-many-arguments,W0613,line-too-long def run ( # noqa: C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ): \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa: B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ): setattr ( test , i , values [ i ]) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory ) def resume ( self , run_id ): # pylint: disable=no-self-use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ): \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ): \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint: disable=duplicate-code set ( boto3 . Session ( profile_name = aws_profile ) . get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" )","title":"Module taskcat._cli_modules.test"},{"location":"reference/taskcat/_cli_modules/test.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/test.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/test.html#test","text":"class Test ( / , * args , ** kwargs ) View Source class Test : \"\"\" Performs functional tests on CloudFormation templates. \"\"\" # pylint : disable = too - many - locals @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , ) @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory ) def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\"resumes a monitoring of a previously started test run\"\"\" # do some stuff raise NotImplementedError () @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" ) @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" )","title":"Test"},{"location":"reference/taskcat/_cli_modules/test.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/_cli_modules/test.html#clean","text":"def clean ( project : str , aws_profile : str = 'default' , region = 'ALL' ) Parameters: Name Type Description Default project None project to delete, can be an name or uuid, or ALL to clean all tests None aws_profile None aws profile to use for deletion None region None region to delete from, default will scan all regions None View Source @staticmethod def clean ( project : str , aws_profile : str = \"default\" , region = \"ALL\" ) : \"\"\" :param project: project to delete, can be an name or uuid, or ALL to clean all tests :param aws_profile: aws profile to use for deletion :param region: region to delete from, default will scan all regions \"\"\" if region == \"ALL\" : region_set : set = set () region_set = region_set . union ( # pylint : disable = duplicate - code set ( boto3 . Session ( profile_name = aws_profile ). get_available_regions ( \"cloudformation\" ) ) ) regions = list ( region_set ) else : regions = [ region ] Delete ( package = project , aws_profile = aws_profile , region = regions , _stack_type = \"test\" )","title":"clean"},{"location":"reference/taskcat/_cli_modules/test.html#list","text":"def list ( profiles : str = 'default' , regions = 'ALL' , _stack_type = 'package' ) Parameters: Name Type Description Default profiles None comma separated list of aws profiles to search None regions None comma separated list of regions to search, default is to check all commercial regions None View Source @staticmethod def list ( profiles : str = \"default\" , regions = \"ALL\" , _stack_type = \"package\" ) : \"\"\" :param profiles: comma separated list of aws profiles to search :param regions: comma separated list of regions to search, default is to check all commercial regions \"\"\" List ( profiles = profiles , regions = regions , _stack_type = \"test\" )","title":"list"},{"location":"reference/taskcat/_cli_modules/test.html#retry","text":"def retry ( region : str , stack_name : str , resource_name : str , config_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False ) [ALPHA] re-launches a child stack using the same parameters as previous launch Parameters: Name Type Description Default region None region stack is in None stack_name None name of parent stack None resource_name None logical id of child stack that will be re-launched None config_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None keep_failed None do not delete failed stacks None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None View Source @staticmethod def retry ( region : str , stack_name : str , resource_name : str , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , keep_failed : bool = False , minimal_output : bool = False , dont_wait_for_delete : bool = False , ) : \"\"\"[ALPHA] re-launches a child stack using the same parameters as previous launch :param region: region stack is in :param stack_name: name of parent stack :param resource_name: logical id of child stack that will be re-launched :param config_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param keep_failed: do not delete failed stacks :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete \"\"\" LOG . warning ( \"test retry is in alpha feature, use with caution\" ) project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file config = Config . create ( project_root = project_root_path , project_config_path = input_file_path ) profile = determine_profile_for_region ( config . config . general . auth , region ) cfn = boto3 . Session ( profile_name = profile ). client ( \"cloudformation\" , region_name = region ) events = cfn . describe_stack_events ( StackName = stack_name ) [ \"StackEvents\" ] resource = [ i for i in events if i[\"LogicalResourceId\" ] == resource_name ] [ 0 ] properties = yaml . safe_load ( resource [ \"ResourceProperties\" ] ) with open ( str ( input_file_path ), \"r\" ) as filepointer : config_yaml = yaml . safe_load ( filepointer ) config_yaml [ \"project\" ][ \"regions\" ] = [ region ] config_yaml [ \"project\" ][ \"parameters\" ] = properties [ \"Parameters\" ] config_yaml [ \"project\" ][ \"template\" ] = \"/\" . join ( properties [ \"TemplateURL\" ] . split ( \"/\" ) [ 4: ] ) config_yaml [ \"tests\" ] = { \"default\" : {}} with open ( \"/tmp/.taskcat.yml.temp\" , \"w\" ) as filepointer : # nosec yaml . safe_dump ( config_yaml , filepointer ) if resource [ \"PhysicalResourceId\" ] : cfn . delete_stack ( StackName = resource [ \"PhysicalResourceId\" ] ) LOG . info ( \"waiting for old stack to delete...\" ) cfn . get_waiter ( \"stack_delete_complete\" ). wait ( StackName = resource [ \"PhysicalResourceId\" ] ) Test . run ( input_file = \"/tmp/.taskcat.yml.temp\" , # nosec project_root = project_root , lint_disable = True , no_delete = no_delete , keep_failed = keep_failed , minimal_output = minimal_output , dont_wait_for_delete = dont_wait_for_delete , )","title":"retry"},{"location":"reference/taskcat/_cli_modules/test.html#run","text":"def run ( test_names : str = 'ALL' , regions : str = 'ALL' , input_file : str = './.taskcat.yml' , project_root : str = './' , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = './taskcat_outputs' , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False ) tests whether CloudFormation templates are able to successfully launch Parameters: Name Type Description Default test_names None comma separated list of tests to run None regions None comma separated list of regions to test in None input_file None path to either a taskat project config file or a CloudFormation template None project_root None root path of the project relative to input_file None no_delete None don't delete stacks after test is complete None lint_disable None disable cfn-lint checks None enable_sig_v2 None enable legacy sigv2 requests for auto-created buckets None keep_failed None do not delete failed stacks None output_directory None Where to store generated logfiles None minimal_output None Reduces output during test runs None dont_wait_for_delete None Exits immediately after calling stack_delete None skip_upload None Use templates in an existing cloudformation bucket. None View Source @staticmethod # pylint : disable = too - many - arguments , W0613 , line - too - long def run ( # noqa : C901 test_names : str = \"ALL\" , regions : str = \"ALL\" , input_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , no_delete : bool = False , lint_disable : bool = False , enable_sig_v2 : bool = False , keep_failed : bool = False , output_directory : str = \"./taskcat_outputs\" , minimal_output : bool = False , dont_wait_for_delete : bool = False , skip_upload : bool = False , ) : \"\"\"tests whether CloudFormation templates are able to successfully launch :param test_names: comma separated list of tests to run :param regions: comma separated list of regions to test in :param input_file: path to either a taskat project config file or a CloudFormation template :param project_root: root path of the project relative to input_file :param no_delete: don't delete stacks after test is complete :param lint_disable: disable cfn-lint checks :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param keep_failed: do not delete failed stacks :param output_directory: Where to store generated logfiles :param minimal_output: Reduces output during test runs :param dont_wait_for_delete: Exits immediately after calling stack_delete :param skip_upload: Use templates in an existing cloudformation bucket. \"\"\" # noqa : B950 test = CFNTest . from_file ( project_root = project_root , input_file = input_file , regions = regions , enable_sig_v2 = enable_sig_v2 , ) # This code is temporary and should be removed once its easier # to create a config object frame = inspect . currentframe () if frame is not None : args , _ , _ , values = inspect . getargvalues ( frame ) for i in args : if hasattr ( test , i ) : setattr ( test , i , values [ i ] ) terminal_printer = TerminalPrinter ( minimalist = minimal_output ) test . printer = terminal_printer with test : test . report ( output_directory )","title":"run"},{"location":"reference/taskcat/_cli_modules/test.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/_cli_modules/test.html#resume","text":"def resume ( self , run_id ) resumes a monitoring of a previously started test run View Source def resume ( self , run_id ) : # pylint : disable = no - self - use \"\"\" resumes a monitoring of a previously started test run \"\"\" # do some stuff raise NotImplementedError ()","title":"resume"},{"location":"reference/taskcat/_cli_modules/update_ami.html","text":"Module taskcat._cli_modules.update_ami None None View Source import logging import os from pathlib import Path from taskcat._amiupdater import ( AMIUpdater , AMIUpdaterCommitNeededException , AMIUpdaterFatalException , ) from taskcat._common_utils import exit_with_code from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class UpdateAMI : \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root : str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else : _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try : amiupdater . update_amis () except AMIUpdaterCommitNeededException : exit_with_code ( 100 ) except AMIUpdaterFatalException : exit_with_code ( 1 ) Variables LOG Classes UpdateAMI class UpdateAMI ( project_root : str = './' ) View Source class UpdateAMI: \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root: str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else: _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try: amiupdater . update_amis () except AMIUpdaterCommitNeededException: exit_with_code ( 100 ) except AMIUpdaterFatalException: exit_with_code ( 1 ) Class variables CLINAME","title":"Update Ami"},{"location":"reference/taskcat/_cli_modules/update_ami.html#module-taskcat_cli_modulesupdate_ami","text":"None None View Source import logging import os from pathlib import Path from taskcat._amiupdater import ( AMIUpdater , AMIUpdaterCommitNeededException , AMIUpdaterFatalException , ) from taskcat._common_utils import exit_with_code from taskcat._config import Config LOG = logging . getLogger ( __name__ ) class UpdateAMI : \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root : str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else : _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try : amiupdater . update_amis () except AMIUpdaterCommitNeededException : exit_with_code ( 100 ) except AMIUpdaterFatalException : exit_with_code ( 1 )","title":"Module taskcat._cli_modules.update_ami"},{"location":"reference/taskcat/_cli_modules/update_ami.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/update_ami.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/update_ami.html#updateami","text":"class UpdateAMI ( project_root : str = './' ) View Source class UpdateAMI: \"\"\" Updates AMI IDs within CloudFormation templates \"\"\" CLINAME = \"update-ami\" def __init__ ( self , project_root: str = \"./\" ): \"\"\" :param project_root: base path for project \"\"\" if project_root == \"./\" : _project_root = Path ( os . getcwd ()) else: _project_root = Path ( project_root ) config = Config . create ( project_root = _project_root , project_config_path = Path ( _project_root / \".taskcat.yml\" ), ) amiupdater = AMIUpdater ( config = config ) try: amiupdater . update_amis () except AMIUpdaterCommitNeededException: exit_with_code ( 100 ) except AMIUpdaterFatalException: exit_with_code ( 1 )","title":"UpdateAMI"},{"location":"reference/taskcat/_cli_modules/update_ami.html#class-variables","text":"CLINAME","title":"Class variables"},{"location":"reference/taskcat/_cli_modules/upload.html","text":"Module taskcat._cli_modules.upload None None View Source import logging from pathlib import Path from typing import Any , Dict from taskcat._cli_core import CliCore from taskcat._client_factory import Boto3Cache from taskcat._config import Config from taskcat._lambda_build import LambdaBuild from taskcat._s3_stage import stage_in_s3 LOG = logging . getLogger ( __name__ ) class Upload : \"\"\" Uploads project to S3. \"\"\" @CliCore . longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , ): \"\"\"does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ): LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , dry_run ) Variables LOG Classes Upload class Upload ( config_file : str = './.taskcat.yml' , project_root : str = './' , enable_sig_v2 : bool = False , bucket_name : str = '' , disable_lambda_packaging : bool = False , key_prefix : str = '' , dry_run : bool = False ) View Source class Upload : \" \"\" Uploads project to S3. \"\" \" @CliCore.longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , ) : \" \"\" does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\" \" project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ) : LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , dry_run )","title":"Upload"},{"location":"reference/taskcat/_cli_modules/upload.html#module-taskcat_cli_modulesupload","text":"None None View Source import logging from pathlib import Path from typing import Any , Dict from taskcat._cli_core import CliCore from taskcat._client_factory import Boto3Cache from taskcat._config import Config from taskcat._lambda_build import LambdaBuild from taskcat._s3_stage import stage_in_s3 LOG = logging . getLogger ( __name__ ) class Upload : \"\"\" Uploads project to S3. \"\"\" @CliCore . longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , ): \"\"\"does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\"\" project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ): LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , dry_run )","title":"Module taskcat._cli_modules.upload"},{"location":"reference/taskcat/_cli_modules/upload.html#variables","text":"LOG","title":"Variables"},{"location":"reference/taskcat/_cli_modules/upload.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/_cli_modules/upload.html#upload","text":"class Upload ( config_file : str = './.taskcat.yml' , project_root : str = './' , enable_sig_v2 : bool = False , bucket_name : str = '' , disable_lambda_packaging : bool = False , key_prefix : str = '' , dry_run : bool = False ) View Source class Upload : \" \"\" Uploads project to S3. \"\" \" @CliCore.longform_param_required ( \"dry_run\" ) def __init__ ( self , config_file : str = \"./.taskcat.yml\" , project_root : str = \"./\" , enable_sig_v2 : bool = False , bucket_name : str = \"\" , disable_lambda_packaging : bool = False , key_prefix : str = \"\" , dry_run : bool = False , ) : \" \"\" does lambda packaging and uploads to s3 :param config_file: path to taskat project config file :param enable_sig_v2: enable legacy sigv2 requests for auto-created buckets :param bucket_name: set bucket name instead of generating it. If regional buckets are enabled, will use this as a prefix :param disable_lambda_packaging: skip packaging step :param key_prefix: provide a custom key-prefix for uploading to S3. This will be used instead of `project` => `name` in the config :param dry_run: identify changes needed but do not upload to S3. \"\" \" project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / config_file args : Dict [ str , Any ] = { \"project\" : { \"s3_enable_sig_v2\" : enable_sig_v2 }} if bucket_name : args [ \"project\" ][ \"bucket_name\" ] = bucket_name if key_prefix : args [ \"project\" ][ \"name\" ] = key_prefix config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args , ) boto3_cache = Boto3Cache () if ( config . config . project . package_lambda and disable_lambda_packaging is not True ) : LambdaBuild ( config , project_root_path ) buckets = config . get_buckets ( boto3_cache ) stage_in_s3 ( buckets , config . config . project . name , config . project_root , dry_run )","title":"Upload"},{"location":"reference/taskcat/testing/index.html","text":"Module taskcat.testing None None View Source from ._cfn_test import CFNTest # noqa: F401 from ._lint_test import LintTest # noqa: F401 from ._unit_test import UnitTest # noqa: F401 __all__ = [ \"CFNTest\" ] Sub-modules taskcat.testing.base_test Classes CFNTest class CFNTest ( config : taskcat . _config . Config , printer : Optional [ taskcat . _tui . TerminalPrinter ] = None , test_names : str = 'ALL' , regions : str = 'ALL' , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True ) View Source class CFNTest ( BaseTest ): # pylint: disable=too-many-instance-attributes \"\"\" Tests Cloudformation template by making sure the stack can properly deploy in the specified regions. \"\"\" def __init__ ( self , config : Config , printer : Union [ TerminalPrinter , None ] = None , test_names : str = \"ALL\" , regions : str = \"ALL\" , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , ): \"\"\"The constructor creates a test from the given Config object. Args: config (Config): A pre-configured Taskcat Config instance. printer (Union[TerminalPrinter, None], optional): A printer object that will handle Test output. Defaults to TerminalPrinter. test_names (str, optional): A comma separated list of tests to run. Defaults to \"ALL\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". skip_upload (bool, optional): Use templates in an existing cloudformation bucket. Defaults to False. lint_disable (bool, optional): Disable linting with cfn-lint. Defaults to False. no_delete (bool, optional): Don't delete stacks after test is complete. Defaults to False. keep_failed (bool, optional): Don't delete failed stacks. Defaults to False. dont_wait_for_delete (bool, optional): Exits immediately after calling stack_delete. Defaults to True. \"\"\" # noqa: B950 super () . __init__ ( config ) self . test_definition : Stacker self . test_names = test_names self . regions = regions self . skip_upload = skip_upload self . lint_disable = lint_disable self . no_delete = no_delete self . keep_failed = keep_failed self . dont_wait_for_delete = dont_wait_for_delete if printer is None : self . printer = TerminalPrinter ( minimalist = True ) else : self . printer = printer def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks def clean_up ( self ) -> None : # noqa: C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ): LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ]) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ({ \"status\" : \"CREATE_COMPLETE\" }) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO: summarise stack statusses (did they complete/delete ok) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ]) == 0 ): deleted : ListType [ str ] = [] for test in buckets . values (): for bucket in test . values (): if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK. status = self . test_definition . status () if len ( status [ \"FAILED\" ]) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) def report ( self , output_directory : str = \"./taskcat_outputs\" , ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path ( output_directory ) . resolve () report_path . mkdir ( exist_ok = True ) cfn_logs = _CfnLogTools () cfn_logs . createcfnlogs ( self . test_definition , report_path ) ReportBuilder ( self . test_definition , report_path / \"index.html\" ) . generate_report () Ancestors (in MRO) taskcat.testing.base_test.BaseTest taskcat.testing._abstract_test.Test abc.ABC Static methods from_dict def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) from_file def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) Instance variables config passed result Methods clean_up def clean_up ( self ) -> None Deletes the Test related resources in AWS. Raises: Type Description TaskCatException If one or more stacks failed to create. View Source def clean_up ( self ) -> None : # noqa : C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ) : LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ] ) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ( { \"status\" : \"CREATE_COMPLETE\" } ) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO : summarise stack statusses ( did they complete / delete ok ) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ] ) == 0 ) : deleted : ListType [ str ] = [] for test in buckets . values () : for bucket in test . values () : if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK . status = self . test_definition . status () if len ( status [ \"FAILED\" ] ) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) report def report ( self , output_directory : str = './taskcat_outputs' ) Generates a report of the status of Cloudformation stacks. Parameters: Name Type Description Default output_directory str The directory to save the report in. Defaults to \"./taskcat_outputs\". \"./taskcat_outputs\" View Source def report( self, output_directory: str = \"./taskcat_outputs\", ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path(output_directory).resolve() report_path.mkdir(exist_ok=True) cfn_logs = _CfnLogTools() cfn_logs.createcfnlogs(self.test_definition, report_path) ReportBuilder( self.test_definition, report_path / \"index.html\" ).generate_report() run def run ( self ) -> None Deploys the required Test resources in AWS. Raises: Type Description TaskCatException If skip_upload is set without specifying s3_bucket in config. TaskCatException If linting fails with errors. View Source def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks","title":"Index"},{"location":"reference/taskcat/testing/index.html#module-taskcattesting","text":"None None View Source from ._cfn_test import CFNTest # noqa: F401 from ._lint_test import LintTest # noqa: F401 from ._unit_test import UnitTest # noqa: F401 __all__ = [ \"CFNTest\" ]","title":"Module taskcat.testing"},{"location":"reference/taskcat/testing/index.html#sub-modules","text":"taskcat.testing.base_test","title":"Sub-modules"},{"location":"reference/taskcat/testing/index.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/testing/index.html#cfntest","text":"class CFNTest ( config : taskcat . _config . Config , printer : Optional [ taskcat . _tui . TerminalPrinter ] = None , test_names : str = 'ALL' , regions : str = 'ALL' , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True ) View Source class CFNTest ( BaseTest ): # pylint: disable=too-many-instance-attributes \"\"\" Tests Cloudformation template by making sure the stack can properly deploy in the specified regions. \"\"\" def __init__ ( self , config : Config , printer : Union [ TerminalPrinter , None ] = None , test_names : str = \"ALL\" , regions : str = \"ALL\" , skip_upload : bool = False , lint_disable : bool = False , no_delete : bool = False , keep_failed : bool = False , dont_wait_for_delete : bool = True , ): \"\"\"The constructor creates a test from the given Config object. Args: config (Config): A pre-configured Taskcat Config instance. printer (Union[TerminalPrinter, None], optional): A printer object that will handle Test output. Defaults to TerminalPrinter. test_names (str, optional): A comma separated list of tests to run. Defaults to \"ALL\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". skip_upload (bool, optional): Use templates in an existing cloudformation bucket. Defaults to False. lint_disable (bool, optional): Disable linting with cfn-lint. Defaults to False. no_delete (bool, optional): Don't delete stacks after test is complete. Defaults to False. keep_failed (bool, optional): Don't delete failed stacks. Defaults to False. dont_wait_for_delete (bool, optional): Exits immediately after calling stack_delete. Defaults to True. \"\"\" # noqa: B950 super () . __init__ ( config ) self . test_definition : Stacker self . test_names = test_names self . regions = regions self . skip_upload = skip_upload self . lint_disable = lint_disable self . no_delete = no_delete self . keep_failed = keep_failed self . dont_wait_for_delete = dont_wait_for_delete if printer is None : self . printer = TerminalPrinter ( minimalist = True ) else : self . printer = printer def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks def clean_up ( self ) -> None : # noqa: C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ): LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ]) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ({ \"status\" : \"CREATE_COMPLETE\" }) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO: summarise stack statusses (did they complete/delete ok) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ]) == 0 ): deleted : ListType [ str ] = [] for test in buckets . values (): for bucket in test . values (): if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK. status = self . test_definition . status () if len ( status [ \"FAILED\" ]) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' ) def report ( self , output_directory : str = \"./taskcat_outputs\" , ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path ( output_directory ) . resolve () report_path . mkdir ( exist_ok = True ) cfn_logs = _CfnLogTools () cfn_logs . createcfnlogs ( self . test_definition , report_path ) ReportBuilder ( self . test_definition , report_path / \"index.html\" ) . generate_report ()","title":"CFNTest"},{"location":"reference/taskcat/testing/index.html#ancestors-in-mro","text":"taskcat.testing.base_test.BaseTest taskcat.testing._abstract_test.Test abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/testing/index.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/testing/index.html#from_dict","text":"def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"from_dict"},{"location":"reference/taskcat/testing/index.html#from_file","text":"def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config )","title":"from_file"},{"location":"reference/taskcat/testing/index.html#instance-variables","text":"config passed result","title":"Instance variables"},{"location":"reference/taskcat/testing/index.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/testing/index.html#clean_up","text":"def clean_up ( self ) -> None Deletes the Test related resources in AWS. Raises: Type Description TaskCatException If one or more stacks failed to create. View Source def clean_up ( self ) -> None : # noqa : C901 \"\"\"Deletes the Test related resources in AWS. Raises: TaskCatException: If one or more stacks failed to create. \"\"\" if not hasattr ( self , \"test_definition\" ) : LOG . warning ( \"No stacks were created... skipping cleanup.\" ) return status = self . test_definition . status () # Delete Stacks if self . no_delete : LOG . info ( \"Skipping delete due to cli argument\" ) elif self . keep_failed : if len ( status [ \"COMPLETE\" ] ) > 0 : LOG . info ( \"deleting successful stacks\" ) self . test_definition . delete_stacks ( { \"status\" : \"CREATE_COMPLETE\" } ) else : self . test_definition . delete_stacks () if not self . dont_wait_for_delete : self . printer . report_test_progress ( stacker = self . test_definition ) # TODO : summarise stack statusses ( did they complete / delete ok ) and print any # error events # Delete Templates and Buckets buckets = self . config . get_buckets () if not self . no_delete or ( self . keep_failed is True and len ( status [ \"FAILED\" ] ) == 0 ) : deleted : ListType [ str ] = [] for test in buckets . values () : for bucket in test . values () : if ( bucket . name not in deleted ) and not bucket . regional_buckets : bucket . delete ( delete_objects = True ) deleted . append ( bucket . name ) # 9. raise if something failed # - grabbing the status again to ensure everything deleted OK . status = self . test_definition . status () if len ( status [ \"FAILED\" ] ) > 0 : raise TaskCatException ( f 'One or more stacks failed to create: {status[\"FAILED\"]}' )","title":"clean_up"},{"location":"reference/taskcat/testing/index.html#report","text":"def report ( self , output_directory : str = './taskcat_outputs' ) Generates a report of the status of Cloudformation stacks. Parameters: Name Type Description Default output_directory str The directory to save the report in. Defaults to \"./taskcat_outputs\". \"./taskcat_outputs\" View Source def report( self, output_directory: str = \"./taskcat_outputs\", ): \"\"\"Generates a report of the status of Cloudformation stacks. Args: output_directory (str, optional): The directory to save the report in. Defaults to \"./taskcat_outputs\". \"\"\" # noqa: B950 report_path = Path(output_directory).resolve() report_path.mkdir(exist_ok=True) cfn_logs = _CfnLogTools() cfn_logs.createcfnlogs(self.test_definition, report_path) ReportBuilder( self.test_definition, report_path / \"index.html\" ).generate_report()","title":"report"},{"location":"reference/taskcat/testing/index.html#run","text":"def run ( self ) -> None Deploys the required Test resources in AWS. Raises: Type Description TaskCatException If skip_upload is set without specifying s3_bucket in config. TaskCatException If linting fails with errors. View Source def run ( self ) -> None : \"\"\"Deploys the required Test resources in AWS. Raises: TaskCatException: If skip_upload is set without specifying s3_bucket in config. TaskCatException: If linting fails with errors. \"\"\" _trim_regions ( self . regions , self . config ) _trim_tests ( self . test_names , self . config ) boto3_cache = Boto3Cache () templates = self . config . get_templates () if self . skip_upload and not self . config . config . project . s3_bucket : raise TaskCatException ( \"cannot skip_buckets without specifying s3_bucket in config\" ) buckets = self . config . get_buckets ( boto3_cache ) if not self . skip_upload : # 1. lint if not self . lint_disable : lint = TaskCatLint ( self . config , templates ) errors = lint . lints [ 1 ] lint . output_results () if errors or not lint . passed : raise TaskCatException ( \"Lint failed with errors\" ) # 2. build lambdas if self . config . config . project . package_lambda : LambdaBuild ( self . config , self . config . project_root ) # 3. s3 sync stage_in_s3 ( buckets , self . config . config . project . name , self . config . project_root ) regions = self . config . get_regions ( boto3_cache ) parameters = self . config . get_rendered_parameters ( buckets , regions , templates ) tests = self . config . get_tests ( templates , regions , buckets , parameters ) # pre-hooks execute_hooks ( \"prehooks\" , self . config , tests , parameters ) self . test_definition = Stacker ( self . config . config . project . name , tests , shorten_stack_name = self . config . config . project . shorten_stack_name , ) self . test_definition . create_stacks () # post-hooks # TODO: pass in outputs, once there is a standard interface for a test_definition execute_hooks ( \"posthooks\" , self . config , tests , parameters ) self . printer . report_test_progress ( stacker = self . test_definition ) self . passed = True self . result = self . test_definition . stacks","title":"run"},{"location":"reference/taskcat/testing/base_test.html","text":"Module taskcat.testing.base_test None None View Source # pylint: disable=line-too-long import uuid from pathlib import Path from typing import Any , Dict , Type , TypeVar from taskcat._cli_core import GLOBAL_ARGS from taskcat._config import Config from ._abstract_test import Test T = TypeVar ( \"T\" , bound = \"BaseTest\" ) # pylint: disable=invalid-name class BaseTest ( Test ): \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ): self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception. Might be needed since # child objects rely on the configs uid. self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ): try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ): # we could optionally call self.report() on exiting. self . clean_up () @classmethod def from_file ( cls : Type [ T ], project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". input_file (str, optional): The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO: detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ], input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ { \"source\" : \"Manual\" , \"config\" : input_config }, { \"source\" : \"CliArgument\" , \"config\" : args }, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) def _build_args ( enable_sig_v2 , regions , default_profile ): args : Dict [ str , Any ] = {} if enable_sig_v2 : args [ \"project\" ] = { \"s3_enable_sig_v2\" : enable_sig_v2 } if regions != \"ALL\" : if \"project\" not in args : args [ \"project\" ] = {} args [ \"project\" ][ \"regions\" ] = regions . split ( \",\" ) if default_profile : _auth_dict = { \"default\" : default_profile } if not args . get ( \"project\" ): args [ \"project\" ] = { \"auth\" : _auth_dict } else : args [ \"project\" ][ \"auth\" ] = _auth_dict return args Variables T Classes BaseTest class BaseTest ( config : taskcat . _config . Config ) View Source class BaseTest ( Test ) : \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ) : self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception . Might be needed since # child objects rely on the configs uid . self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ) : try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ) : # we could optionally call self . report () on exiting . self . clean_up () @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) Ancestors (in MRO) taskcat.testing._abstract_test.Test abc.ABC Descendants taskcat.testing._cfn_test.CFNTest taskcat.testing._lint_test.LintTest taskcat.testing._unit_test.UnitTest Static methods from_dict def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) from_file def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) Instance variables config passed result Methods clean_up def clean_up ( self ) -> None Clean up after the Test. View Source @abstractmethod def clean_up ( self ) -> None : \"\"\"Clean up after the Test.\"\"\" run def run ( self ) -> None Run the Test. View Source @abstractmethod def run ( self ) -> None : \"\"\"Run the Test.\"\"\"","title":"Base Test"},{"location":"reference/taskcat/testing/base_test.html#module-taskcattestingbase_test","text":"None None View Source # pylint: disable=line-too-long import uuid from pathlib import Path from typing import Any , Dict , Type , TypeVar from taskcat._cli_core import GLOBAL_ARGS from taskcat._config import Config from ._abstract_test import Test T = TypeVar ( \"T\" , bound = \"BaseTest\" ) # pylint: disable=invalid-name class BaseTest ( Test ): \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ): self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception. Might be needed since # child objects rely on the configs uid. self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ): try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ): # we could optionally call self.report() on exiting. self . clean_up () @classmethod def from_file ( cls : Type [ T ], project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". input_file (str, optional): The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () input_file_path : Path = project_root_path / input_file # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO: detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ], input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \"./\". regions (str, optional): A comma separated list of regions to test in. Defaults to \"ALL\". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa: B950 project_root_path : Path = Path ( project_root ) . expanduser () . resolve () # pylint: disable=too-many-arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ { \"source\" : \"Manual\" , \"config\" : input_config }, { \"source\" : \"CliArgument\" , \"config\" : args }, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config ) def _build_args ( enable_sig_v2 , regions , default_profile ): args : Dict [ str , Any ] = {} if enable_sig_v2 : args [ \"project\" ] = { \"s3_enable_sig_v2\" : enable_sig_v2 } if regions != \"ALL\" : if \"project\" not in args : args [ \"project\" ] = {} args [ \"project\" ][ \"regions\" ] = regions . split ( \",\" ) if default_profile : _auth_dict = { \"default\" : default_profile } if not args . get ( \"project\" ): args [ \"project\" ] = { \"auth\" : _auth_dict } else : args [ \"project\" ][ \"auth\" ] = _auth_dict return args","title":"Module taskcat.testing.base_test"},{"location":"reference/taskcat/testing/base_test.html#variables","text":"T","title":"Variables"},{"location":"reference/taskcat/testing/base_test.html#classes","text":"","title":"Classes"},{"location":"reference/taskcat/testing/base_test.html#basetest","text":"class BaseTest ( config : taskcat . _config . Config ) View Source class BaseTest ( Test ) : \"\"\"A Generic Test Class that implements the passed and uid properties. Any subclass will still need to implement the the run and clean_up methods. \"\"\" def __init__ ( self , config : Config ) : self . config : Config = config self . passed : bool = False self . result : Any = None @property def config ( self ) -> Config : return self . _config @config . setter def config ( self , config : Config ) -> None : # It should be possible to check if config is already set # and if it is throw an exception . Might be needed since # child objects rely on the configs uid . self . _config = config @property def passed ( self ) -> bool : return self . _passed @passed . setter def passed ( self , new_value : bool ) -> None : self . _passed = new_value @property def result ( self ) -> Any : return self . _result @result . setter def result ( self , new_value : Any ) -> None : self . _result = new_value def __enter__ ( self ) : try : self . run () except BaseException as ex : self . clean_up () raise ex return self . result def __exit__ ( self , exc_type , exc_val , exc_tb ) : # we could optionally call self . report () on exiting . self . clean_up () @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config ) @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"BaseTest"},{"location":"reference/taskcat/testing/base_test.html#ancestors-in-mro","text":"taskcat.testing._abstract_test.Test abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/taskcat/testing/base_test.html#descendants","text":"taskcat.testing._cfn_test.CFNTest taskcat.testing._lint_test.LintTest taskcat.testing._unit_test.UnitTest","title":"Descendants"},{"location":"reference/taskcat/testing/base_test.html#static-methods","text":"","title":"Static methods"},{"location":"reference/taskcat/testing/base_test.html#from_dict","text":"def from_dict ( input_config : dict , project_root : str = './' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat configuration in dictionary form. Parameters: Name Type Description Default input_config dict A Taskcat configuration in the form of a dict. None project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_dict ( cls : Type [ T ] , input_config : dict , project_root : str = \"./\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat configuration in dictionary form. Args: input_config (dict): A Taskcat configuration in the form of a dict. project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) sources = [ {\"source\": \"Manual\", \"config\": input_config}, {\"source\": \"CliArgument\", \"config\": args}, ] config = Config ( uid = uuid . uuid4 (), project_root = project_root_path , sources = sources ) return cls ( config )","title":"from_dict"},{"location":"reference/taskcat/testing/base_test.html#from_file","text":"def from_file ( project_root : str = './' , input_file : str = './.taskcat.yml' , regions : str = 'ALL' , enable_sig_v2 : bool = False ) -> ~ T Creates a Test from a Taskcat config file. Parameters: Name Type Description Default project_root str The path to the directory with your template and config file. Defaults to \"./\". \"./\" input_file str The name of the Taskcat confile file. Defaults to \"./.taskcat.yml\". \"./.taskcat.yml\" regions str A comma separated list of regions to test in. Defaults to \"ALL\". \"ALL\" enable_sig_v2 bool Enable legacy sigv2 requests for auto-created buckets. Defaults to False. False Returns: Type Description T Returns a Test instance. View Source @classmethod def from_file ( cls : Type [ T ] , project_root : str = \"./\" , input_file : str = \"./.taskcat.yml\" , regions : str = \"ALL\" , enable_sig_v2 : bool = False , ) -> T : \"\"\"Creates a Test from a Taskcat config file. Args: project_root (str, optional): The path to the directory with your template and config file. Defaults to \" . / \". input_file (str, optional): The name of the Taskcat confile file. Defaults to \" . / . taskcat . yml \". regions (str, optional): A comma separated list of regions to test in. Defaults to \" ALL \". enable_sig_v2 (bool, optional): Enable legacy sigv2 requests for auto-created buckets. Defaults to False. Returns: T: Returns a Test instance. \"\"\" # noqa : B950 project_root_path : Path = Path ( project_root ). expanduser (). resolve () input_file_path : Path = project_root_path / input_file # pylint : disable = too - many - arguments args = _build_args ( enable_sig_v2 , regions , GLOBAL_ARGS . profile ) config = Config . create ( project_root = project_root_path , project_config_path = input_file_path , args = args # TODO : detect if input file is taskcat config or CloudFormation template ) return cls ( config )","title":"from_file"},{"location":"reference/taskcat/testing/base_test.html#instance-variables","text":"config passed result","title":"Instance variables"},{"location":"reference/taskcat/testing/base_test.html#methods","text":"","title":"Methods"},{"location":"reference/taskcat/testing/base_test.html#clean_up","text":"def clean_up ( self ) -> None Clean up after the Test. View Source @abstractmethod def clean_up ( self ) -> None : \"\"\"Clean up after the Test.\"\"\"","title":"clean_up"},{"location":"reference/taskcat/testing/base_test.html#run","text":"def run ( self ) -> None Run the Test. View Source @abstractmethod def run ( self ) -> None : \"\"\"Run the Test.\"\"\"","title":"run"}]}